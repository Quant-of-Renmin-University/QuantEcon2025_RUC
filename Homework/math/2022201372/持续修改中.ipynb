{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e622f5-e95f-49bf-bf11-4ada6dae18c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ•°æ®åŠ è½½ä¸­...\n",
      "ä¸»æ•°æ®åŠ è½½æˆåŠŸ\n",
      "   è®­ç»ƒé›†: (84133, 32)\n",
      "   æµ‹è¯•é›†: (14786, 32)\n",
      "å°åŒºæ•°æ®: (3100, 27)\n",
      "ç§Ÿé‡‘æ•°æ®: (84150, 23)\n",
      "\n",
      "é«˜çº§ç‰¹å¾å·¥ç¨‹...\n",
      "åŸºç¡€ç‰¹å¾æå–...\n",
      "   å¤„ç†å‰ - è®­ç»ƒé›†: 84133, æµ‹è¯•é›†: 14786\n",
      "   å¤„ç†å - è®­ç»ƒé›†: 84133, æµ‹è¯•é›†: 14786\n",
      "å¤–éƒ¨æ•°æ®èåˆ...\n",
      "åˆå¹¶ç§Ÿé‡‘æ•°æ®å¤±è´¥: 'åŸå¸‚'\n",
      "åˆå¹¶ç§Ÿé‡‘æ•°æ®å¤±è´¥: 'åŸå¸‚'\n",
      "   èåˆå - è®­ç»ƒé›†: 91613, æµ‹è¯•é›†: 16110\n",
      " æµ‹è¯•é›†é•¿åº¦å˜åŒ–: 14786 -> 16110\n",
      "   è¿›è¡Œå»é‡å¤„ç†...\n",
      "   å»é‡åæµ‹è¯•é›†é•¿åº¦: 14786\n",
      "   é‡æ’åºåæµ‹è¯•é›†é•¿åº¦: 14786\n",
      "é«˜çº§ç»„åˆç‰¹å¾...\n",
      "   æœ€ç»ˆ - è®­ç»ƒé›†: 91613, æµ‹è¯•é›†: 14786\n",
      "è®­ç»ƒé›†ç‰¹å¾æ•°: 123\n",
      "æµ‹è¯•é›†ç‰¹å¾æ•°: 123\n",
      "å…±åŒç‰¹å¾æ•°: 122\n",
      "æ’é™¤ç‰¹å¾æ•°: 8\n",
      "å¯ç”¨ç‰¹å¾æ•°: 118\n",
      "æœ€ç»ˆä½¿ç”¨ç‰¹å¾æ•°: 118\n",
      "\n",
      "ç‰¹å¾ç»Ÿè®¡:\n",
      "   æ€»ç‰¹å¾æ•°: 118\n",
      "   æ•°å€¼ç‰¹å¾: 78\n",
      "   åˆ†ç±»ç‰¹å¾: 40\n",
      "\n",
      "ğŸ” å¼‚å¸¸å€¼å¤„ç†...\n",
      "   åŸå§‹è®­ç»ƒæ ·æœ¬: 91613\n",
      "   æ¸…æ´—åè®­ç»ƒæ ·æœ¬: 82562\n",
      "   ç§»é™¤æ¯”ä¾‹: 9.9%\n",
      "   æµ‹è¯•é›†ä¿æŒä¸å˜: 14786 æ ·æœ¬\n",
      "\n",
      " æ•°æ®é¢„å¤„ç†...\n",
      "é«˜çº§é¢„å¤„ç†å¼€å§‹...\n",
      "è®­ç»ƒé›†å½¢çŠ¶: (66049, 118)\n",
      "æµ‹è¯•é›†å½¢çŠ¶: (16513, 118)\n",
      "æ•°å€¼ç‰¹å¾æ•°é‡: 78\n",
      "åˆ†ç±»ç‰¹å¾æ•°é‡: 40\n",
      "æˆåŠŸå¤„ç†æ•°å€¼ç‰¹å¾: 44/78\n",
      "æˆåŠŸå¤„ç†åˆ†ç±»ç‰¹å¾: 40/40\n",
      "é¢„å¤„ç†å®Œæˆ:\n",
      "  è®­ç»ƒé›†æœ€ç»ˆå½¢çŠ¶: (66049, 118)\n",
      "  æµ‹è¯•é›†æœ€ç»ˆå½¢çŠ¶: (16513, 118)\n",
      "  è®­ç»ƒé›†ç¼ºå¤±å€¼: 0\n",
      "  æµ‹è¯•é›†ç¼ºå¤±å€¼: 0\n",
      "é«˜çº§é¢„å¤„ç†å¼€å§‹...\n",
      "è®­ç»ƒé›†å½¢çŠ¶: (66049, 118)\n",
      "æµ‹è¯•é›†å½¢çŠ¶: (14786, 118)\n",
      "æ•°å€¼ç‰¹å¾æ•°é‡: 78\n",
      "åˆ†ç±»ç‰¹å¾æ•°é‡: 40\n",
      "æˆåŠŸå¤„ç†æ•°å€¼ç‰¹å¾: 44/78\n",
      "æˆåŠŸå¤„ç†åˆ†ç±»ç‰¹å¾: 40/40\n",
      "é¢„å¤„ç†å®Œæˆ:\n",
      "  è®­ç»ƒé›†æœ€ç»ˆå½¢çŠ¶: (66049, 118)\n",
      "  æµ‹è¯•é›†æœ€ç»ˆå½¢çŠ¶: (14786, 118)\n",
      "  è®­ç»ƒé›†ç¼ºå¤±å€¼: 0\n",
      "  æµ‹è¯•é›†ç¼ºå¤±å€¼: 0\n",
      "é¢„å¤„ç†åé•¿åº¦æ£€æŸ¥:\n",
      "  è®­ç»ƒé›†: 66049\n",
      "  éªŒè¯é›†: 16513\n",
      "  æµ‹è¯•é›†: 14786\n",
      "  åŸå§‹æµ‹è¯•ID: 14786\n",
      "ä¿®æ­£åæµ‹è¯•é›†é•¿åº¦: 14786\n",
      "\n",
      " è¿›è¡Œç‰¹å¾é€‰æ‹©...\n",
      "åŸå§‹ç‰¹å¾æ•°: 118\n",
      "ç§»é™¤ä½æ–¹å·®ç‰¹å¾å: 99\n",
      "ç»Ÿè®¡ç‰¹å¾é€‰æ‹©å: 60\n",
      "å‰10ä¸ªæœ€é‡è¦ç‰¹å¾:\n",
      "   feature         score\n",
      "25    åŸå¸‚_x  11829.424588\n",
      "6     åŸå¸‚_y  10431.567383\n",
      "33      ç¯çº¿   5886.250807\n",
      "7     å»ºç­‘é¢ç§¯   4615.035210\n",
      "23  ç‰©ä¸šåŠå…¬ç”µè¯   4050.487322\n",
      "29     ç‡ƒæ°”è´¹   3743.267821\n",
      "52     å¼€å‘å•†   3474.560479\n",
      "16    äº§æƒæè¿°   3199.300901\n",
      "21  é¢ç§¯_å¤§æˆ·å‹   3067.677356\n",
      "11      å«æ•°   2907.763921\n",
      "ç‰¹å¾é€‰æ‹©åé•¿åº¦æ£€æŸ¥:\n",
      "  è®­ç»ƒé›†: (66049, 60)\n",
      "  éªŒè¯é›†: (16513, 60)\n",
      "  æµ‹è¯•é›†: (14786, 60)\n",
      "\n",
      "æ¨¡å‹è®­ç»ƒå¼€å§‹...\n",
      "==================================================\n",
      "\n",
      "è®­ç»ƒæ¨¡å‹: Ridge\n",
      "Ridge:\n",
      "   æµ‹è¯•MAE: 821873\n",
      "   æµ‹è¯•R2: 0.3195\n",
      "   CV MAE: 625688 Â± 5206\n",
      "\n",
      "è®­ç»ƒæ¨¡å‹: Lasso\n",
      "Lasso:\n",
      "   æµ‹è¯•MAE: 821234\n",
      "   æµ‹è¯•R2: 0.3202\n",
      "   CV MAE: 625967 Â± 5260\n",
      "\n",
      "è®­ç»ƒæ¨¡å‹: ElasticNet\n",
      "ElasticNet:\n",
      "   æµ‹è¯•MAE: 835449\n",
      "   æµ‹è¯•R2: 0.3042\n",
      "   CV MAE: 625737 Â± 4735\n",
      "\n",
      "è®­ç»ƒæ¨¡å‹: RandomForest\n",
      "RandomForest:\n",
      "   æµ‹è¯•MAE: 527563\n",
      "   æµ‹è¯•R2: 0.6913\n",
      "   CV MAE: 151444 Â± 889\n",
      "\n",
      "è®­ç»ƒæ¨¡å‹: ExtraTrees\n",
      "ExtraTrees:\n",
      "   æµ‹è¯•MAE: 566096\n",
      "   æµ‹è¯•R2: 0.6407\n",
      "   CV MAE: 185335 Â± 871\n",
      "\n",
      "è®­ç»ƒæ¨¡å‹: GradientBoosting\n",
      "GradientBoosting:\n",
      "   æµ‹è¯•MAE: 535431\n",
      "   æµ‹è¯•R2: 0.6724\n",
      "   CV MAE: 151230 Â± 1376\n",
      "\n",
      "è®­ç»ƒæ¨¡å‹: XGBoost\n",
      "XGBoost:\n",
      "   æµ‹è¯•MAE: 458439\n",
      "   æµ‹è¯•R2: 0.7436\n",
      "   CV MAE: 139764 Â± 1378\n",
      "\n",
      "è®­ç»ƒæ¨¡å‹: LightGBM\n",
      "LightGBM:\n",
      "   æµ‹è¯•MAE: 408081\n",
      "   æµ‹è¯•R2: 0.7977\n",
      "   CV MAE: 174182 Â± 2764\n",
      "\n",
      "æ¨¡å‹æ€§èƒ½æ’è¡Œæ¦œ:\n",
      "================================================================================\n",
      "              æ¨¡å‹         æµ‹è¯•MAE     æµ‹è¯•R2        CV_MAE      CV_STD\n",
      "         XGBoost 458439.281250 0.743577 139764.100000 1378.406514\n",
      "GradientBoosting 535430.629126 0.672427 151229.841326 1375.795186\n",
      "    RandomForest 527563.454492 0.691323 151443.527376  888.609256\n",
      "        LightGBM 408080.661345 0.797723 174182.090712 2763.824365\n",
      "      ExtraTrees 566096.015792 0.640694 185334.928031  870.775145\n",
      "           Ridge 821873.405735 0.319544 625687.612079 5206.204263\n",
      "      ElasticNet 835448.761436 0.304165 625737.326833 4734.529310\n",
      "           Lasso 821233.555564 0.320236 625966.845223 5259.916266\n",
      "\n",
      "ğŸ”— åˆ›å»ºå…ƒå­¦ä¹ é›†æˆ...\n",
      "  ç”Ÿæˆ Ridge çš„å…ƒç‰¹å¾...\n",
      "  ç”Ÿæˆ Lasso çš„å…ƒç‰¹å¾...\n",
      "  ç”Ÿæˆ ElasticNet çš„å…ƒç‰¹å¾...\n",
      "  ç”Ÿæˆ RandomForest çš„å…ƒç‰¹å¾...\n",
      "  ç”Ÿæˆ ExtraTrees çš„å…ƒç‰¹å¾...\n",
      "  ç”Ÿæˆ GradientBoosting çš„å…ƒç‰¹å¾...\n",
      "  ç”Ÿæˆ XGBoost çš„å…ƒç‰¹å¾...\n",
      "  ç”Ÿæˆ LightGBM çš„å…ƒç‰¹å¾...\n",
      "å…ƒå­¦ä¹ é›†æˆå®Œæˆ:\n",
      "   ä½¿ç”¨æ¨¡å‹: Ridge, Lasso, ElasticNet, RandomForest, ExtraTrees, GradientBoosting, XGBoost, LightGBM\n",
      "   éªŒè¯MAE: 540126\n",
      "   éªŒè¯R2: 0.6622\n",
      "\n",
      "æœ€ç»ˆé¢„æµ‹ç”Ÿæˆ...\n",
      "æœ€ä½³å•ä¸€æ¨¡å‹: XGBoost\n",
      "é¢„æµ‹é•¿åº¦æ£€æŸ¥:\n",
      "  é¢„æµ‹ç»“æœ: 14786\n",
      "  åŸå§‹æµ‹è¯•ID: 14786\n",
      "ğŸ”— åº”ç”¨å…ƒå­¦ä¹ é›†æˆ...\n",
      "å…ƒç‰¹å¾é•¿åº¦æ£€æŸ¥: 14786 vs 14786\n",
      "ä½¿ç”¨é›†æˆé¢„æµ‹ (æœ€ä½³æ¨¡å‹80% + å…ƒå­¦ä¹ 20%)\n",
      "\n",
      "åˆ›å»ºæäº¤æ–‡ä»¶...\n",
      "æœ€ç»ˆé•¿åº¦æ£€æŸ¥:\n",
      "  é¢„æµ‹ç»“æœ: 14786\n",
      "  æµ‹è¯•é›†ID: 14786\n",
      "\n",
      "é¢„æµ‹å®Œæˆ!\n",
      "==================================================\n",
      "é¢„æµ‹ç»Ÿè®¡:\n",
      "   æ ·æœ¬æ•°é‡: 14786\n",
      "   ä»·æ ¼èŒƒå›´: 192377 - 6171747\n",
      "   å¹³å‡ä»·æ ¼: 2335562\n",
      "   ä¸­ä½ä»·æ ¼: 1876648\n",
      "   æ ‡å‡†å·®: 1464797\n",
      "\n",
      "æ–‡ä»¶ä¿å­˜: high_performance_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, ExtraTreesRegressor\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler, QuantileTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# å°è¯•å¯¼å…¥é«˜æ€§èƒ½æ¨¡å‹\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGB = True\n",
    "except:\n",
    "    HAS_XGB = False\n",
    "    print(\"âš ï¸  XGBoostæœªå®‰è£…ï¼Œå°†è·³è¿‡XGBoostæ¨¡å‹\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGB = True\n",
    "except:\n",
    "    HAS_LGB = False\n",
    "    print(\"âš ï¸  LightGBMæœªå®‰è£…ï¼Œå°†è·³è¿‡LightGBMæ¨¡å‹\")\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def advanced_feature_engineering(df, is_training=True, training_stats=None):\n",
    "    \"\"\"é«˜çº§ç‰¹å¾å·¥ç¨‹ - æ·»åŠ è®­ç»ƒ/æµ‹è¯•é›†æ ‡è¯†\"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    # å¦‚æœæ˜¯è®­ç»ƒé›†ï¼Œè®°å½•ç»Ÿè®¡ä¿¡æ¯ï¼›å¦‚æœæ˜¯æµ‹è¯•é›†ï¼Œä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡ä¿¡æ¯\n",
    "    if training_stats is None:\n",
    "        training_stats = {}\n",
    "    \n",
    "    # é€šç”¨é¢ç§¯æå–å‡½æ•°\n",
    "    def extract_area(area_str):\n",
    "        if pd.isna(area_str):\n",
    "            return np.nan\n",
    "        try:\n",
    "            numbers = re.findall(r'\\d+\\.?\\d*', str(area_str))\n",
    "            if len(numbers) >= 2:\n",
    "                return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "            elif len(numbers) == 1:\n",
    "                return float(numbers[0])\n",
    "        except:\n",
    "            pass\n",
    "        return np.nan\n",
    "    \n",
    "    # 1. å»ºç­‘é¢ç§¯å¤„ç†\n",
    "    if 'å»ºç­‘é¢ç§¯' in data.columns:\n",
    "        data['å»ºç­‘é¢ç§¯'] = data['å»ºç­‘é¢ç§¯'].apply(extract_area)\n",
    "        # æ›´ä¿å®ˆçš„å¼‚å¸¸å€¼å¤„ç†\n",
    "        data.loc[(data['å»ºç­‘é¢ç§¯'] > 800) | (data['å»ºç­‘é¢ç§¯'] < 15), 'å»ºç­‘é¢ç§¯'] = np.nan\n",
    "    \n",
    "    # 2. å¥—å†…é¢ç§¯\n",
    "    if 'å¥—å†…é¢ç§¯' in data.columns:\n",
    "        data['å¥—å†…é¢ç§¯'] = data['å¥—å†…é¢ç§¯'].apply(extract_area)\n",
    "        data.loc[(data['å¥—å†…é¢ç§¯'] > 800) | (data['å¥—å†…é¢ç§¯'] < 15), 'å¥—å†…é¢ç§¯'] = np.nan\n",
    "        \n",
    "        # è®¡ç®—å¾—æˆ¿ç‡\n",
    "        if 'å»ºç­‘é¢ç§¯' in data.columns:\n",
    "            data['å¾—æˆ¿ç‡'] = data['å¥—å†…é¢ç§¯'] / data['å»ºç­‘é¢ç§¯']\n",
    "            data['å¾—æˆ¿ç‡'] = data['å¾—æˆ¿ç‡'].clip(0.5, 1.0)  # åˆç†èŒƒå›´\n",
    "    \n",
    "    # 3. æˆ·å‹ä¿¡æ¯\n",
    "    if 'æˆ¿å±‹æˆ·å‹' in data.columns:\n",
    "        def extract_room_info(room_str):\n",
    "            if pd.isna(room_str):\n",
    "                return 2, 1, 1\n",
    "            \n",
    "            room_str = str(room_str)\n",
    "            \n",
    "            room_match = re.search(r'(\\d+)å®¤', room_str)\n",
    "            rooms = int(room_match.group(1)) if room_match else 2\n",
    "            \n",
    "            hall_match = re.search(r'(\\d+)å…', room_str)\n",
    "            halls = int(hall_match.group(1)) if hall_match else 1\n",
    "            \n",
    "            bath_match = re.search(r'(\\d+)å«', room_str)\n",
    "            baths = int(bath_match.group(1)) if bath_match else 1\n",
    "            \n",
    "            rooms = max(1, min(rooms, 8))\n",
    "            halls = max(0, min(halls, 4))\n",
    "            baths = max(1, min(baths, 4))\n",
    "            \n",
    "            return rooms, halls, baths\n",
    "        \n",
    "        room_info = data['æˆ¿å±‹æˆ·å‹'].apply(extract_room_info)\n",
    "        data['å®¤æ•°'] = [info[0] for info in room_info]\n",
    "        data['å…æ•°'] = [info[1] for info in room_info]\n",
    "        data['å«æ•°'] = [info[2] for info in room_info]\n",
    "        data['æ€»æˆ¿é—´æ•°'] = data['å®¤æ•°'] + data['å…æ•°'] + data['å«æ•°']\n",
    "        \n",
    "        # æˆ·å‹ç±»å‹ç¼–ç \n",
    "        data['æˆ·å‹ç±»å‹'] = data['å®¤æ•°'].astype(str) + 'å®¤' + data['å…æ•°'].astype(str) + 'å…'\n",
    "        \n",
    "        # æˆ¿é—´é…æ¯”ç‰¹å¾\n",
    "        data['å®¤å…æ¯”'] = data['å®¤æ•°'] / (data['å…æ•°'] + 1)\n",
    "        data['å®¤å«æ¯”'] = data['å®¤æ•°'] / data['å«æ•°']\n",
    "    \n",
    "    # 4. æ¥¼å±‚ä¿¡æ¯\n",
    "    if 'æ‰€åœ¨æ¥¼å±‚' in data.columns:\n",
    "        def extract_floor_info(floor_str):\n",
    "            if pd.isna(floor_str):\n",
    "                return 5, 20, 0.25\n",
    "            \n",
    "            floor_str = str(floor_str)\n",
    "            patterns = [\n",
    "                r'ç¬¬?(\\d+)å±‚.*å…±(\\d+)å±‚',\n",
    "                r'(\\d+)/(\\d+)',\n",
    "                r'(\\d+)å±‚.*(\\d+)å±‚'\n",
    "            ]\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, floor_str)\n",
    "                if match:\n",
    "                    try:\n",
    "                        current = int(match.group(1))\n",
    "                        total = int(match.group(2))\n",
    "                        if 1 <= current <= total <= 100:\n",
    "                            ratio = current / total\n",
    "                            return current, total, ratio\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            return 5, 20, 0.25\n",
    "        \n",
    "        floor_info = data['æ‰€åœ¨æ¥¼å±‚'].apply(extract_floor_info)\n",
    "        data['å½“å‰æ¥¼å±‚'] = [info[0] for info in floor_info]\n",
    "        data['æ€»æ¥¼å±‚'] = [info[1] for info in floor_info]\n",
    "        data['æ¥¼å±‚æ¯”ä¾‹'] = [info[2] for info in floor_info]\n",
    "        \n",
    "        # æ¥¼å±‚åˆ†ç±»ï¼ˆæ›´ç»†è‡´ï¼‰\n",
    "        data['æ¥¼å±‚ç±»å‹_åº•å±‚'] = (data['æ¥¼å±‚æ¯”ä¾‹'] <= 0.2).astype(int)\n",
    "        data['æ¥¼å±‚ç±»å‹_ä½å±‚'] = ((data['æ¥¼å±‚æ¯”ä¾‹'] > 0.2) & (data['æ¥¼å±‚æ¯”ä¾‹'] <= 0.4)).astype(int)\n",
    "        data['æ¥¼å±‚ç±»å‹_ä¸­å±‚'] = ((data['æ¥¼å±‚æ¯”ä¾‹'] > 0.4) & (data['æ¥¼å±‚æ¯”ä¾‹'] <= 0.7)).astype(int)\n",
    "        data['æ¥¼å±‚ç±»å‹_é«˜å±‚'] = ((data['æ¥¼å±‚æ¯”ä¾‹'] > 0.7) & (data['æ¥¼å±‚æ¯”ä¾‹'] < 0.9)).astype(int)\n",
    "        data['æ¥¼å±‚ç±»å‹_é¡¶å±‚'] = (data['æ¥¼å±‚æ¯”ä¾‹'] >= 0.9).astype(int)\n",
    "        \n",
    "        # æ¥¼å±‚ä»·å€¼è¯„åˆ†ï¼ˆä¸€èˆ¬ä¸­é«˜å±‚æœ€è´µï¼‰\n",
    "        data['æ¥¼å±‚ä»·å€¼'] = np.where(\n",
    "            (data['æ¥¼å±‚æ¯”ä¾‹'] >= 0.3) & (data['æ¥¼å±‚æ¯”ä¾‹'] <= 0.8), 1, 0\n",
    "        )\n",
    "    \n",
    "    # 5. æœå‘ç‰¹å¾ï¼ˆæ›´å…¨é¢ï¼‰\n",
    "    if 'æˆ¿å±‹æœå‘' in data.columns:\n",
    "        data['æœå—'] = data['æˆ¿å±‹æœå‘'].str.contains('å—', na=False).astype(int)\n",
    "        data['æœåŒ—'] = data['æˆ¿å±‹æœå‘'].str.contains('åŒ—', na=False).astype(int)\n",
    "        data['æœä¸œ'] = data['æˆ¿å±‹æœå‘'].str.contains('ä¸œ', na=False).astype(int)\n",
    "        data['æœè¥¿'] = data['æˆ¿å±‹æœå‘'].str.contains('è¥¿', na=False).astype(int)\n",
    "        data['å—åŒ—é€šé€'] = ((data['æœå—'] == 1) & (data['æœåŒ—'] == 1)).astype(int)\n",
    "        data['ä¸œè¥¿é€šé€'] = ((data['æœä¸œ'] == 1) & (data['æœè¥¿'] == 1)).astype(int)\n",
    "        \n",
    "        # æœå‘è¯„åˆ†ï¼ˆå—>ä¸œ>åŒ—>è¥¿ï¼‰\n",
    "        orientation_score = 0\n",
    "        orientation_score += data['æœå—'] * 4\n",
    "        orientation_score += data['æœä¸œ'] * 3\n",
    "        orientation_score += data['æœåŒ—'] * 2\n",
    "        orientation_score += data['æœè¥¿'] * 1\n",
    "        data['æœå‘è¯„åˆ†'] = orientation_score\n",
    "        \n",
    "        # é€šé€æ€§è¯„åˆ†\n",
    "        data['é€šé€æ€§'] = data['å—åŒ—é€šé€'] * 2 + data['ä¸œè¥¿é€šé€'] * 1\n",
    "    \n",
    "    # 6. è£…ä¿®æƒ…å†µ\n",
    "    if 'è£…ä¿®æƒ…å†µ' in data.columns:\n",
    "        decoration_map = {\n",
    "            'è±ªåè£…ä¿®': 5, 'ç²¾è£…ä¿®': 4, 'ä¸­è£…ä¿®': 3, \n",
    "            'ç®€è£…ä¿®': 2, 'æ¯›å¯æˆ¿': 1, 'å…¶ä»–': 2\n",
    "        }\n",
    "        data['è£…ä¿®è¯„åˆ†'] = data['è£…ä¿®æƒ…å†µ'].fillna('å…¶ä»–').map(decoration_map).fillna(2)\n",
    "    \n",
    "    # 7. ç¯çº¿ç‰¹å¾\n",
    "    if 'ç¯çº¿' in data.columns:\n",
    "        def parse_ring(ring_str):\n",
    "            if pd.isna(ring_str):\n",
    "                return 5\n",
    "            \n",
    "            ring_str = str(ring_str).lower()\n",
    "            ring_map = {\n",
    "                'ä¸€ç¯': 1, 'å†…ç¯': 1, 'äºŒç¯': 2, 'ä¸‰ç¯': 3, \n",
    "                'å››ç¯': 4, 'äº”ç¯': 5, 'å…­ç¯': 6\n",
    "            }\n",
    "            \n",
    "            for key, value in ring_map.items():\n",
    "                if key in ring_str:\n",
    "                    return value\n",
    "            \n",
    "            numbers = re.findall(r'\\d+', ring_str)\n",
    "            if numbers:\n",
    "                return min(int(numbers[0]), 8)\n",
    "            \n",
    "            return 5\n",
    "        \n",
    "        data['ç¯çº¿æ•°å€¼'] = data['ç¯çº¿'].apply(parse_ring)\n",
    "        # ç¯çº¿ä»·å€¼ï¼ˆè¶Šé å†…è¶Šè´µï¼Œä½†éçº¿æ€§ï¼‰\n",
    "        data['ç¯çº¿ä»·å€¼'] = np.exp(-(data['ç¯çº¿æ•°å€¼'] - 1) * 0.3)\n",
    "    \n",
    "    # 8. æ—¶é—´ç‰¹å¾\n",
    "    if 'äº¤æ˜“æ—¶é—´' in data.columns:\n",
    "        data['äº¤æ˜“æ—¶é—´'] = pd.to_datetime(data['äº¤æ˜“æ—¶é—´'], errors='coerce')\n",
    "        data['äº¤æ˜“å¹´ä»½'] = data['äº¤æ˜“æ—¶é—´'].dt.year.fillna(2023)\n",
    "        data['äº¤æ˜“æœˆä»½'] = data['äº¤æ˜“æ—¶é—´'].dt.month.fillna(6)\n",
    "        data['äº¤æ˜“å­£åº¦'] = data['äº¤æ˜“æ—¶é—´'].dt.quarter.fillna(2)\n",
    "        \n",
    "        # å­£èŠ‚æ€§ç‰¹å¾ï¼ˆæˆ¿åœ°äº§æœ‰æ˜æ˜¾å­£èŠ‚æ€§ï¼‰\n",
    "        data['æ˜¯å¦æ—ºå­£'] = data['äº¤æ˜“æœˆä»½'].isin([3, 4, 5, 9, 10, 11]).astype(int)\n",
    "        data['æ˜¯å¦å¹´åº•'] = data['äº¤æ˜“æœˆä»½'].isin([11, 12]).astype(int)\n",
    "        \n",
    "        # å¹´ä»½çƒ­ç¼–ç  - ä½¿ç”¨å›ºå®šå¹´ä»½é¿å…è®­ç»ƒæµ‹è¯•é›†ç‰¹å¾ä¸ä¸€è‡´\n",
    "        common_years = [2020, 2021, 2022, 2023, 2024]\n",
    "        for year in common_years:\n",
    "            data[f'å¹´ä»½_{year}'] = (data['äº¤æ˜“å¹´ä»½'] == year).astype(int)\n",
    "        \n",
    "        # åˆ é™¤åŸå§‹æ—¶é—´åˆ—ï¼Œé¿å…ç±»å‹é—®é¢˜\n",
    "        data = data.drop(['äº¤æ˜“æ—¶é—´'], axis=1)\n",
    "    \n",
    "    # 9. æˆ¿é¾„ç‰¹å¾\n",
    "    if 'å¹´ä»½' in data.columns:\n",
    "        data['å¹´ä»½'] = pd.to_numeric(data['å¹´ä»½'], errors='coerce')\n",
    "        data['å¹´ä»½'] = data['å¹´ä»½'].fillna(data['å¹´ä»½'].median())\n",
    "        \n",
    "        if 'äº¤æ˜“å¹´ä»½' in data.columns:\n",
    "            data['æˆ¿é¾„'] = data['äº¤æ˜“å¹´ä»½'] - data['å¹´ä»½']\n",
    "            data['æˆ¿é¾„'] = data['æˆ¿é¾„'].clip(0, 100)\n",
    "            \n",
    "            # æˆ¿é¾„åˆ†æ¡£ï¼ˆéçº¿æ€§è´¬å€¼ï¼‰\n",
    "            data['æˆ¿é¾„_æ–°æˆ¿'] = (data['æˆ¿é¾„'] <= 2).astype(int)\n",
    "            data['æˆ¿é¾„_æ¬¡æ–°'] = ((data['æˆ¿é¾„'] > 2) & (data['æˆ¿é¾„'] <= 5)).astype(int)\n",
    "            data['æˆ¿é¾„_ä¸­ç­‰'] = ((data['æˆ¿é¾„'] > 5) & (data['æˆ¿é¾„'] <= 10)).astype(int)\n",
    "            data['æˆ¿é¾„_è¾ƒæ—§'] = ((data['æˆ¿é¾„'] > 10) & (data['æˆ¿é¾„'] <= 20)).astype(int)\n",
    "            data['æˆ¿é¾„_è€æˆ¿'] = (data['æˆ¿é¾„'] > 20).astype(int)\n",
    "            \n",
    "            # æˆ¿é¾„ä»·å€¼è¡°å‡\n",
    "            data['æˆ¿é¾„ä»·å€¼'] = np.exp(-data['æˆ¿é¾„'] * 0.02)\n",
    "    \n",
    "    # 10. ç”µæ¢¯ç‰¹å¾\n",
    "    if 'æœ‰æ— ç”µæ¢¯' in data.columns:\n",
    "        data['æœ‰ç”µæ¢¯'] = (data['æœ‰æ— ç”µæ¢¯'] == 'æœ‰').astype(int)\n",
    "        \n",
    "        # ç”µæ¢¯å¿…è¦æ€§ï¼ˆé«˜å±‚æ›´éœ€è¦ç”µæ¢¯ï¼‰\n",
    "        if 'æ€»æ¥¼å±‚' in data.columns:\n",
    "            data['ç”µæ¢¯å¿…è¦æ€§'] = np.where(data['æ€»æ¥¼å±‚'] > 6, 1, 0)\n",
    "            data['ç”µæ¢¯åŒ¹é…åº¦'] = data['æœ‰ç”µæ¢¯'] * data['ç”µæ¢¯å¿…è¦æ€§']\n",
    "    \n",
    "    return data, training_stats\n",
    "\n",
    "def advanced_external_merge(df, community_df, rent_df):\n",
    "    \"\"\"é«˜çº§å¤–éƒ¨æ•°æ®èåˆ\"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    def safe_extract_numeric(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        try:\n",
    "            x_str = str(x).replace('%', '').replace('ï¼Œ', '').replace(',', '')\n",
    "            numbers = re.findall(r'\\d+\\.?\\d*', x_str)\n",
    "            if numbers:\n",
    "                return float(numbers[0])\n",
    "        except:\n",
    "            pass\n",
    "        return np.nan\n",
    "    \n",
    "    # 1. å°åŒºæ•°æ®èåˆï¼ˆå¢å¼ºç‰ˆï¼‰\n",
    "    if community_df is not None and len(community_df) > 0:\n",
    "        result_df['å°åŒºåç§°_clean'] = result_df['å°åŒºåç§°'].astype(str).str.strip()\n",
    "        community_df['åç§°_clean'] = community_df['åç§°'].astype(str).str.strip()\n",
    "        \n",
    "        # å¤„ç†å°åŒºç‰¹å¾\n",
    "        if 'å®¹ ç§¯ ç‡' in community_df.columns:\n",
    "            community_df['å®¹ç§¯ç‡'] = community_df['å®¹ ç§¯ ç‡'].apply(safe_extract_numeric)\n",
    "            community_df['å®¹ç§¯ç‡'] = community_df['å®¹ç§¯ç‡'].fillna(2.5).clip(0.1, 10)\n",
    "            # å®¹ç§¯ç‡åˆ†æ¡£\n",
    "            community_df['å®¹ç§¯ç‡_ä½å¯†åº¦'] = (community_df['å®¹ç§¯ç‡'] <= 1.5).astype(int)\n",
    "            community_df['å®¹ç§¯ç‡_ä¸­å¯†åº¦'] = ((community_df['å®¹ç§¯ç‡'] > 1.5) & (community_df['å®¹ç§¯ç‡'] <= 3.0)).astype(int)\n",
    "            community_df['å®¹ç§¯ç‡_é«˜å¯†åº¦'] = (community_df['å®¹ç§¯ç‡'] > 3.0).astype(int)\n",
    "        \n",
    "        if 'ç»¿ åŒ– ç‡' in community_df.columns:\n",
    "            community_df['ç»¿åŒ–ç‡'] = community_df['ç»¿ åŒ– ç‡'].apply(safe_extract_numeric)\n",
    "            community_df['ç»¿åŒ–ç‡'] = community_df['ç»¿åŒ–ç‡'].fillna(30).clip(0, 80) / 100\n",
    "            # ç»¿åŒ–ç‡åˆ†æ¡£\n",
    "            community_df['ç»¿åŒ–ç‡_é«˜'] = (community_df['ç»¿åŒ–ç‡'] >= 0.35).astype(int)\n",
    "            community_df['ç»¿åŒ–ç‡_ä¸­'] = ((community_df['ç»¿åŒ–ç‡'] >= 0.25) & (community_df['ç»¿åŒ–ç‡'] < 0.35)).astype(int)\n",
    "            community_df['ç»¿åŒ–ç‡_ä½'] = (community_df['ç»¿åŒ–ç‡'] < 0.25).astype(int)\n",
    "        \n",
    "        if 'ç‰© ä¸š è´¹' in community_df.columns:\n",
    "            community_df['ç‰©ä¸šè´¹'] = community_df['ç‰© ä¸š è´¹'].apply(safe_extract_numeric)\n",
    "            community_df['ç‰©ä¸šè´¹'] = community_df['ç‰©ä¸šè´¹'].fillna(community_df['ç‰©ä¸šè´¹'].median())\n",
    "            # ç‰©ä¸šè´¹åˆ†æ¡£ï¼ˆé€šå¸¸ç‰©ä¸šè´¹è¶Šé«˜ï¼Œå°åŒºå“è´¨è¶Šå¥½ï¼‰\n",
    "            community_df['ç‰©ä¸šè´¹_æ¡£æ¬¡'] = pd.cut(community_df['ç‰©ä¸šè´¹'], \n",
    "                                             bins=[0, 2, 4, 6, np.inf], \n",
    "                                             labels=['ä½', 'ä¸­', 'é«˜', 'è±ªå']).astype(str)\n",
    "        \n",
    "        # åˆå¹¶å°åŒºæ•°æ®\n",
    "        try:\n",
    "            merge_cols = ['åç§°_clean', 'åŸå¸‚']\n",
    "            feature_cols = [c for c in community_df.columns if c not in ['åç§°', 'åç§°_clean']]\n",
    "            \n",
    "            merged = result_df.merge(\n",
    "                community_df[feature_cols + ['åç§°_clean']],\n",
    "                left_on='å°åŒºåç§°_clean',\n",
    "                right_on='åç§°_clean',\n",
    "                how='left'\n",
    "            )\n",
    "            result_df = merged\n",
    "        except Exception as e:\n",
    "            print(f\"åˆå¹¶å°åŒºæ•°æ®å¤±è´¥: {e}\")\n",
    "    \n",
    "    # 2. ç§Ÿé‡‘æ•°æ®èåˆï¼ˆå¢å¼ºç‰ˆï¼‰\n",
    "    if rent_df is not None and len(rent_df) > 0 and 'ä»·æ ¼' in rent_df.columns:\n",
    "        try:\n",
    "            rent_df['ä»·æ ¼'] = pd.to_numeric(rent_df['ä»·æ ¼'], errors='coerce')\n",
    "            rent_df = rent_df.dropna(subset=['ä»·æ ¼'])\n",
    "            \n",
    "            if len(rent_df) > 0:\n",
    "                # æŒ‰å°åŒºå’ŒåŸå¸‚ç»Ÿè®¡ç§Ÿé‡‘\n",
    "                if 'åŸå¸‚' in rent_df.columns:\n",
    "                    rent_stats = rent_df.groupby(['å°åŒºåç§°', 'åŸå¸‚'])['ä»·æ ¼'].agg([\n",
    "                        'mean', 'median', 'count', 'std', 'min', 'max'\n",
    "                    ]).reset_index()\n",
    "                    rent_stats.columns = ['å°åŒºåç§°', 'åŸå¸‚', 'å¹³å‡ç§Ÿé‡‘', 'ç§Ÿé‡‘ä¸­ä½æ•°', 'ç§Ÿé‡‘æ ·æœ¬æ•°', 'ç§Ÿé‡‘æ ‡å‡†å·®', 'æœ€ä½ç§Ÿé‡‘', 'æœ€é«˜ç§Ÿé‡‘']\n",
    "                    merge_keys = ['å°åŒºåç§°', 'åŸå¸‚']\n",
    "                else:\n",
    "                    rent_stats = rent_df.groupby('å°åŒºåç§°')['ä»·æ ¼'].agg([\n",
    "                        'mean', 'median', 'count', 'std', 'min', 'max'\n",
    "                    ]).reset_index()\n",
    "                    rent_stats.columns = ['å°åŒºåç§°', 'å¹³å‡ç§Ÿé‡‘', 'ç§Ÿé‡‘ä¸­ä½æ•°', 'ç§Ÿé‡‘æ ·æœ¬æ•°', 'ç§Ÿé‡‘æ ‡å‡†å·®', 'æœ€ä½ç§Ÿé‡‘', 'æœ€é«˜ç§Ÿé‡‘']\n",
    "                    merge_keys = ['å°åŒºåç§°']\n",
    "                \n",
    "                # è®¡ç®—ç§Ÿé‡‘ç‰¹å¾\n",
    "                rent_stats['ç§Ÿé‡‘å˜å¼‚ç³»æ•°'] = rent_stats['ç§Ÿé‡‘æ ‡å‡†å·®'] / (rent_stats['å¹³å‡ç§Ÿé‡‘'] + 1)\n",
    "                rent_stats['ç§Ÿé‡‘åŒºé—´'] = rent_stats['æœ€é«˜ç§Ÿé‡‘'] - rent_stats['æœ€ä½ç§Ÿé‡‘']\n",
    "                \n",
    "                # å¡«å……ç¼ºå¤±å€¼\n",
    "                for col in ['å¹³å‡ç§Ÿé‡‘', 'ç§Ÿé‡‘ä¸­ä½æ•°', 'ç§Ÿé‡‘æ ‡å‡†å·®', 'æœ€ä½ç§Ÿé‡‘', 'æœ€é«˜ç§Ÿé‡‘']:\n",
    "                    if col in rent_stats.columns:\n",
    "                        rent_stats[col] = rent_stats[col].fillna(rent_stats[col].median())\n",
    "                \n",
    "                rent_stats['ç§Ÿé‡‘æ ·æœ¬æ•°'] = rent_stats['ç§Ÿé‡‘æ ·æœ¬æ•°'].fillna(0)\n",
    "                rent_stats['ç§Ÿé‡‘å˜å¼‚ç³»æ•°'] = rent_stats['ç§Ÿé‡‘å˜å¼‚ç³»æ•°'].fillna(0)\n",
    "                \n",
    "                # åˆå¹¶ç§Ÿé‡‘æ•°æ®\n",
    "                result_df = result_df.merge(rent_stats, on=merge_keys, how='left')\n",
    "                \n",
    "                # å…¨å±€ç§Ÿé‡‘å¡«å……\n",
    "                global_rent_median = rent_stats['å¹³å‡ç§Ÿé‡‘'].median()\n",
    "                result_df['å¹³å‡ç§Ÿé‡‘'] = result_df['å¹³å‡ç§Ÿé‡‘'].fillna(global_rent_median)\n",
    "                result_df['ç§Ÿé‡‘ä¸­ä½æ•°'] = result_df['ç§Ÿé‡‘ä¸­ä½æ•°'].fillna(global_rent_median)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"åˆå¹¶ç§Ÿé‡‘æ•°æ®å¤±è´¥: {e}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def create_advanced_features(df, is_training=True, training_stats=None):\n",
    "    \"\"\"åˆ›å»ºé«˜çº§ç»„åˆç‰¹å¾ - ç¡®ä¿è®­ç»ƒæµ‹è¯•é›†ä¸€è‡´æ€§\"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    if training_stats is None:\n",
    "        training_stats = {}\n",
    "    \n",
    "    # 1. é¢ç§¯ç›¸å…³ç»„åˆç‰¹å¾\n",
    "    if 'å»ºç­‘é¢ç§¯' in data.columns:\n",
    "        if 'æ€»æˆ¿é—´æ•°' in data.columns:\n",
    "            data['æ¯æˆ¿é—´é¢ç§¯'] = data['å»ºç­‘é¢ç§¯'] / (data['æ€»æˆ¿é—´æ•°'] + 1)\n",
    "            \n",
    "        if 'å®¤æ•°' in data.columns:\n",
    "            data['æ¯å®¤é¢ç§¯'] = data['å»ºç­‘é¢ç§¯'] / (data['å®¤æ•°'] + 1)\n",
    "            \n",
    "        # é¢ç§¯åˆ†æ¡£ - ä½¿ç”¨è®­ç»ƒé›†çš„åˆ†ä½æ•°\n",
    "        if is_training:\n",
    "            area_quantiles = data['å»ºç­‘é¢ç§¯'].quantile([0.25, 0.75])\n",
    "            training_stats['area_q25'] = area_quantiles[0.25]\n",
    "            training_stats['area_q75'] = area_quantiles[0.75]\n",
    "        \n",
    "        if 'area_q25' in training_stats and 'area_q75' in training_stats:\n",
    "            data['é¢ç§¯_å°æˆ·å‹'] = (data['å»ºç­‘é¢ç§¯'] <= training_stats['area_q25']).astype(int)\n",
    "            data['é¢ç§¯_ä¸­æˆ·å‹'] = ((data['å»ºç­‘é¢ç§¯'] > training_stats['area_q25']) & \n",
    "                                (data['å»ºç­‘é¢ç§¯'] <= training_stats['area_q75'])).astype(int)\n",
    "            data['é¢ç§¯_å¤§æˆ·å‹'] = (data['å»ºç­‘é¢ç§¯'] > training_stats['area_q75']).astype(int)\n",
    "    \n",
    "    # 2. ç§Ÿå”®æ¯”ç‰¹å¾ï¼ˆæ ¸å¿ƒç‰¹å¾ï¼‰\n",
    "    if 'å¹³å‡ç§Ÿé‡‘' in data.columns and 'å»ºç­‘é¢ç§¯' in data.columns:\n",
    "        data['æœˆç§Ÿå”®æ¯”'] = data['å¹³å‡ç§Ÿé‡‘'] / (data['å»ºç­‘é¢ç§¯'] + 1)\n",
    "        data['å¹´ç§Ÿå”®æ¯”'] = data['æœˆç§Ÿå”®æ¯”'] * 12\n",
    "        \n",
    "        # ç§Ÿå”®æ¯”åˆ†æ¡£ - ä½¿ç”¨è®­ç»ƒé›†çš„åˆ†ä½æ•°\n",
    "        if is_training and data['å¹´ç§Ÿå”®æ¯”'].std() > 0:\n",
    "            rent_ratio_quantiles = data['å¹´ç§Ÿå”®æ¯”'].quantile([0.33, 0.67])\n",
    "            training_stats['rent_ratio_q33'] = rent_ratio_quantiles[0.33]\n",
    "            training_stats['rent_ratio_q67'] = rent_ratio_quantiles[0.67]\n",
    "        \n",
    "        if 'rent_ratio_q33' in training_stats and 'rent_ratio_q67' in training_stats:\n",
    "            data['ç§Ÿå”®æ¯”_ä½'] = (data['å¹´ç§Ÿå”®æ¯”'] <= training_stats['rent_ratio_q33']).astype(int)\n",
    "            data['ç§Ÿå”®æ¯”_ä¸­'] = ((data['å¹´ç§Ÿå”®æ¯”'] > training_stats['rent_ratio_q33']) & \n",
    "                              (data['å¹´ç§Ÿå”®æ¯”'] <= training_stats['rent_ratio_q67'])).astype(int)\n",
    "            data['ç§Ÿå”®æ¯”_é«˜'] = (data['å¹´ç§Ÿå”®æ¯”'] > training_stats['rent_ratio_q67']).astype(int)\n",
    "    \n",
    "    # 3. ä½ç½®ä»·å€¼ç»¼åˆè¯„åˆ†\n",
    "    location_score = 0\n",
    "    if 'ç¯çº¿ä»·å€¼' in data.columns:\n",
    "        location_score += data['ç¯çº¿ä»·å€¼'] * 0.4\n",
    "    if 'æœå‘è¯„åˆ†' in data.columns:\n",
    "        location_score += data['æœå‘è¯„åˆ†'] / 10 * 0.2  # å½’ä¸€åŒ–\n",
    "    if 'æ¥¼å±‚ä»·å€¼' in data.columns:\n",
    "        location_score += data['æ¥¼å±‚ä»·å€¼'] * 0.2\n",
    "    if 'é€šé€æ€§' in data.columns:\n",
    "        location_score += data['é€šé€æ€§'] / 2 * 0.2  # å½’ä¸€åŒ–\n",
    "    \n",
    "    data['ä½ç½®ä»·å€¼ç»¼åˆ'] = location_score\n",
    "    \n",
    "    # 4. æˆ¿å±‹å“è´¨è¯„åˆ†\n",
    "    quality_score = 0\n",
    "    if 'è£…ä¿®è¯„åˆ†' in data.columns:\n",
    "        quality_score += data['è£…ä¿®è¯„åˆ†'] / 5 * 0.3  # å½’ä¸€åŒ–\n",
    "    if 'æˆ¿é¾„ä»·å€¼' in data.columns:\n",
    "        quality_score += data['æˆ¿é¾„ä»·å€¼'] * 0.3\n",
    "    if 'æœ‰ç”µæ¢¯' in data.columns:\n",
    "        quality_score += data['æœ‰ç”µæ¢¯'] * 0.2\n",
    "    if 'å¾—æˆ¿ç‡' in data.columns:\n",
    "        quality_score += (data['å¾—æˆ¿ç‡'] - 0.7) * 0.2  # å¾—æˆ¿ç‡è¶Šé«˜è¶Šå¥½\n",
    "    \n",
    "    data['æˆ¿å±‹å“è´¨ç»¼åˆ'] = quality_score\n",
    "    \n",
    "    # 5. å°åŒºç¯å¢ƒè¯„åˆ†\n",
    "    community_score = 0\n",
    "    if 'ç»¿åŒ–ç‡' in data.columns:\n",
    "        community_score += data['ç»¿åŒ–ç‡'] * 0.4\n",
    "    if 'å®¹ç§¯ç‡' in data.columns:\n",
    "        # å®¹ç§¯ç‡è¶Šä½è¶Šå¥½ï¼ˆå–å€’æ•°ï¼‰\n",
    "        community_score += (1 / (data['å®¹ç§¯ç‡'] + 1)) * 0.3\n",
    "    if 'ç‰©ä¸šè´¹' in data.columns:\n",
    "        # ç‰©ä¸šè´¹æ ‡å‡†åŒ–ï¼ˆé€šå¸¸é€‚ä¸­æœ€å¥½ï¼‰\n",
    "        if is_training:\n",
    "            median_fee = data['ç‰©ä¸šè´¹'].median()\n",
    "            training_stats['median_fee'] = median_fee\n",
    "        \n",
    "        if 'median_fee' in training_stats:\n",
    "            median_fee = training_stats['median_fee']\n",
    "            community_score += (1 - abs(data['ç‰©ä¸šè´¹'] - median_fee) / median_fee) * 0.3\n",
    "    \n",
    "    data['å°åŒºç¯å¢ƒç»¼åˆ'] = community_score\n",
    "    \n",
    "    # 6. æˆ·å‹åˆç†æ€§è¯„åˆ†\n",
    "    if all(col in data.columns for col in ['å®¤æ•°', 'å…æ•°', 'å«æ•°', 'å»ºç­‘é¢ç§¯']):\n",
    "        # æ ‡å‡†æˆ·å‹æ¯”ä¾‹\n",
    "        ideal_ratios = {\n",
    "            1: (40, 60), 2: (60, 90), 3: (90, 130), \n",
    "            4: (130, 180), 5: (180, 250)\n",
    "        }\n",
    "        \n",
    "        data['æˆ·å‹åˆç†æ€§'] = 0.5  # é»˜è®¤å€¼\n",
    "        \n",
    "        for rooms in range(1, 6):\n",
    "            if rooms in ideal_ratios:\n",
    "                min_area, max_area = ideal_ratios[rooms]\n",
    "                mask = data['å®¤æ•°'] == rooms\n",
    "                area_fit = ((data['å»ºç­‘é¢ç§¯'] >= min_area) & \n",
    "                           (data['å»ºç­‘é¢ç§¯'] <= max_area))\n",
    "                data.loc[mask & area_fit, 'æˆ·å‹åˆç†æ€§'] = 1.0\n",
    "                data.loc[mask & ~area_fit, 'æˆ·å‹åˆç†æ€§'] = 0.3\n",
    "    \n",
    "    # 7. å¸‚åœºçƒ­åº¦ç‰¹å¾ï¼ˆåŸºäºç§Ÿé‡‘æ ·æœ¬æ•°ï¼‰\n",
    "    if 'ç§Ÿé‡‘æ ·æœ¬æ•°' in data.columns:\n",
    "        data['å¸‚åœºçƒ­åº¦'] = np.log1p(data['ç§Ÿé‡‘æ ·æœ¬æ•°'])  # å¯¹æ•°å˜æ¢\n",
    "        \n",
    "        # çƒ­åº¦åˆ†æ¡£ - ä½¿ç”¨è®­ç»ƒé›†çš„åˆ†ä½æ•°\n",
    "        if is_training and data['å¸‚åœºçƒ­åº¦'].std() > 0:\n",
    "            heat_quantiles = data['å¸‚åœºçƒ­åº¦'].quantile([0.33, 0.67])\n",
    "            training_stats['heat_q33'] = heat_quantiles[0.33]\n",
    "            training_stats['heat_q67'] = heat_quantiles[0.67]\n",
    "        \n",
    "        if 'heat_q33' in training_stats and 'heat_q67' in training_stats:\n",
    "            data['å¸‚åœº_å†·é—¨'] = (data['å¸‚åœºçƒ­åº¦'] <= training_stats['heat_q33']).astype(int)\n",
    "            data['å¸‚åœº_ä¸€èˆ¬'] = ((data['å¸‚åœºçƒ­åº¦'] > training_stats['heat_q33']) & \n",
    "                              (data['å¸‚åœºçƒ­åº¦'] <= training_stats['heat_q67'])).astype(int)\n",
    "            data['å¸‚åœº_çƒ­é—¨'] = (data['å¸‚åœºçƒ­åº¦'] > training_stats['heat_q67']).astype(int)\n",
    "    \n",
    "    return data, training_stats\n",
    "\n",
    "def advanced_preprocessing(X_train, X_test, feature_types):\n",
    "    \"\"\"é«˜çº§é¢„å¤„ç†\"\"\"\n",
    "    X_train_processed = X_train.copy()\n",
    "    X_test_processed = X_test.copy()\n",
    "    \n",
    "    print(f\"é«˜çº§é¢„å¤„ç†å¼€å§‹...\")\n",
    "    print(f\"è®­ç»ƒé›†å½¢çŠ¶: {X_train_processed.shape}\")\n",
    "    print(f\"æµ‹è¯•é›†å½¢çŠ¶: {X_test_processed.shape}\")\n",
    "    print(f\"æ•°å€¼ç‰¹å¾æ•°é‡: {len(feature_types['numeric'])}\")\n",
    "    print(f\"åˆ†ç±»ç‰¹å¾æ•°é‡: {len(feature_types['categorical'])}\")\n",
    "    \n",
    "    # 1. æ•°å€¼ç‰¹å¾å¤„ç†\n",
    "    numeric_features = feature_types['numeric']\n",
    "    processed_numeric = 0\n",
    "    \n",
    "    for feat in numeric_features:\n",
    "        if feat in X_train_processed.columns:\n",
    "            # ä¸¥æ ¼çš„æ•°æ®ç±»å‹æ£€æŸ¥\n",
    "            dtype = X_train_processed[feat].dtype\n",
    "            \n",
    "            # è·³è¿‡éæ•°å€¼ç±»å‹\n",
    "            if dtype in ['object', 'datetime64[ns]', 'timedelta64[ns]', 'category']:\n",
    "                print(f\"  è·³è¿‡éæ•°å€¼ç‰¹å¾: {feat} (ç±»å‹: {dtype})\")\n",
    "                continue\n",
    "            \n",
    "            # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹\n",
    "            try:\n",
    "                X_train_processed[feat] = pd.to_numeric(X_train_processed[feat], errors='coerce')\n",
    "                X_test_processed[feat] = pd.to_numeric(X_test_processed[feat], errors='coerce')\n",
    "            except:\n",
    "                print(f\"  æ— æ³•è½¬æ¢ä¸ºæ•°å€¼: {feat}\")\n",
    "                continue\n",
    "                \n",
    "            # è®¡ç®—å¡«å……å€¼ï¼ˆä½¿ç”¨ä¼—æ•°æˆ–ä¸­ä½æ•°ï¼‰\n",
    "            if X_train_processed[feat].nunique() <= 10:  # ç¦»æ•£æ•°å€¼ç‰¹å¾\n",
    "                fill_value = X_train_processed[feat].mode()\n",
    "                fill_value = fill_value[0] if len(fill_value) > 0 else 0\n",
    "            else:  # è¿ç»­æ•°å€¼ç‰¹å¾\n",
    "                fill_value = X_train_processed[feat].median()\n",
    "                if pd.isna(fill_value):\n",
    "                    fill_value = 0\n",
    "            \n",
    "            X_train_processed[feat] = X_train_processed[feat].fillna(fill_value)\n",
    "            X_test_processed[feat] = X_test_processed[feat].fillna(fill_value)\n",
    "            \n",
    "            # æ¸©å’Œçš„å¼‚å¸¸å€¼å¤„ç†ï¼ˆä½¿ç”¨æ›´å¤§çš„å€æ•°ï¼‰\n",
    "            try:\n",
    "                std_val = X_train_processed[feat].std()\n",
    "                if pd.notna(std_val) and std_val > 0:\n",
    "                    Q1 = X_train_processed[feat].quantile(0.25)\n",
    "                    Q3 = X_train_processed[feat].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    \n",
    "                    if IQR > 0:\n",
    "                        lower_bound = Q1 - 5 * IQR  # æ›´å®½æ¾çš„ç•Œé™\n",
    "                        upper_bound = Q3 + 5 * IQR\n",
    "                        \n",
    "                        X_train_processed[feat] = X_train_processed[feat].clip(lower_bound, upper_bound)\n",
    "                        X_test_processed[feat] = X_test_processed[feat].clip(lower_bound, upper_bound)\n",
    "                        processed_numeric += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  å¼‚å¸¸å€¼å¤„ç†å¤±è´¥: {feat} - {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"æˆåŠŸå¤„ç†æ•°å€¼ç‰¹å¾: {processed_numeric}/{len(numeric_features)}\")\n",
    "    \n",
    "    # 2. åˆ†ç±»ç‰¹å¾å¤„ç†\n",
    "    le_dict = {}\n",
    "    categorical_features = feature_types['categorical']\n",
    "    processed_categorical = 0\n",
    "    \n",
    "    for feat in categorical_features:\n",
    "        if feat in X_train_processed.columns:\n",
    "            try:\n",
    "                # å¡«å……ç¼ºå¤±å€¼\n",
    "                mode_val = X_train_processed[feat].mode()\n",
    "                fill_val = mode_val[0] if len(mode_val) > 0 else 'å…¶ä»–'\n",
    "                \n",
    "                X_train_processed[feat] = X_train_processed[feat].fillna(fill_val).astype(str)\n",
    "                X_test_processed[feat] = X_test_processed[feat].fillna(fill_val).astype(str)\n",
    "                \n",
    "                # å¤„ç†ä½é¢‘ç±»åˆ«\n",
    "                value_counts = X_train_processed[feat].value_counts()\n",
    "                rare_categories = value_counts[value_counts < 5].index  # å°‘äº5ä¸ªæ ·æœ¬çš„ç±»åˆ«\n",
    "                \n",
    "                X_train_processed[feat] = X_train_processed[feat].replace(rare_categories, 'å…¶ä»–')\n",
    "                X_test_processed[feat] = X_test_processed[feat].replace(rare_categories, 'å…¶ä»–')\n",
    "                \n",
    "                # æ ‡ç­¾ç¼–ç \n",
    "                le = LabelEncoder()\n",
    "                le.fit(X_train_processed[feat])\n",
    "                \n",
    "                X_train_processed[feat] = le.transform(X_train_processed[feat])\n",
    "                \n",
    "                def safe_transform(x):\n",
    "                    try:\n",
    "                        return le.transform([x])[0]\n",
    "                    except:\n",
    "                        return 0\n",
    "                \n",
    "                X_test_processed[feat] = X_test_processed[feat].apply(safe_transform)\n",
    "                le_dict[feat] = le\n",
    "                processed_categorical += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  åˆ†ç±»ç‰¹å¾å¤„ç†å¤±è´¥: {feat} - {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"æˆåŠŸå¤„ç†åˆ†ç±»ç‰¹å¾: {processed_categorical}/{len(categorical_features)}\")\n",
    "    \n",
    "    # 3. æœ€ç»ˆæ•°æ®æ¸…ç†\n",
    "    X_train_processed = X_train_processed.fillna(0)\n",
    "    X_test_processed = X_test_processed.fillna(0)\n",
    "    X_train_processed = X_train_processed.replace([np.inf, -np.inf], 0)\n",
    "    X_test_processed = X_test_processed.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    print(f\"é¢„å¤„ç†å®Œæˆ:\")\n",
    "    print(f\"  è®­ç»ƒé›†æœ€ç»ˆå½¢çŠ¶: {X_train_processed.shape}\")\n",
    "    print(f\"  æµ‹è¯•é›†æœ€ç»ˆå½¢çŠ¶: {X_test_processed.shape}\")\n",
    "    print(f\"  è®­ç»ƒé›†ç¼ºå¤±å€¼: {X_train_processed.isnull().sum().sum()}\")\n",
    "    print(f\"  æµ‹è¯•é›†ç¼ºå¤±å€¼: {X_test_processed.isnull().sum().sum()}\")\n",
    "    \n",
    "    return X_train_processed, X_test_processed, le_dict\n",
    "\n",
    "def feature_selection(X_train, y_train, feature_names, n_features=50):\n",
    "    \"\"\"ç‰¹å¾é€‰æ‹©\"\"\"\n",
    "    print(f\"\\n è¿›è¡Œç‰¹å¾é€‰æ‹©...\")\n",
    "    print(f\"åŸå§‹ç‰¹å¾æ•°: {X_train.shape[1]}\")\n",
    "    \n",
    "    # 1. ç§»é™¤ä½æ–¹å·®ç‰¹å¾\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    var_selector = VarianceThreshold(threshold=0.01)\n",
    "    X_var = var_selector.fit_transform(X_train)\n",
    "    selected_features = [feature_names[i] for i in range(len(feature_names)) \n",
    "                        if var_selector.get_support()[i]]\n",
    "    \n",
    "    print(f\"ç§»é™¤ä½æ–¹å·®ç‰¹å¾å: {len(selected_features)}\")\n",
    "    \n",
    "    # 2. ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•é€‰æ‹©ç‰¹å¾\n",
    "    k_best = SelectKBest(score_func=f_regression, k=min(n_features, len(selected_features)))\n",
    "    X_selected = k_best.fit_transform(X_var, y_train)\n",
    "    \n",
    "    # è·å–æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾å\n",
    "    feature_scores = k_best.scores_\n",
    "    selected_indices = k_best.get_support(indices=True)\n",
    "    final_features = [selected_features[i] for i in selected_indices]\n",
    "    \n",
    "    print(f\"ç»Ÿè®¡ç‰¹å¾é€‰æ‹©å: {len(final_features)}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæœ€é‡è¦çš„ç‰¹å¾\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': final_features,\n",
    "        'score': [feature_scores[i] for i in selected_indices]\n",
    "    }).sort_values('score', ascending=False)\n",
    "    \n",
    "    print(f\"å‰10ä¸ªæœ€é‡è¦ç‰¹å¾:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    return var_selector, k_best, final_features\n",
    "\n",
    "def train_advanced_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"è®­ç»ƒé«˜çº§æ¨¡å‹\"\"\"\n",
    "    \n",
    "    # æ•°æ®æ ‡å‡†åŒ–\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # ä¼ ç»Ÿæ¨¡å‹ï¼ˆä¼˜åŒ–å‚æ•°ï¼‰\n",
    "    models['Ridge'] = Ridge(alpha=5.0, random_state=42)\n",
    "    models['Lasso'] = Lasso(alpha=0.01, random_state=42, max_iter=5000)\n",
    "    models['ElasticNet'] = ElasticNet(alpha=0.01, l1_ratio=0.7, random_state=42, max_iter=5000)\n",
    "    \n",
    "    # æ ‘æ¨¡å‹ï¼ˆä¼˜åŒ–å‚æ•°ï¼‰\n",
    "    models['RandomForest'] = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    models['ExtraTrees'] = ExtraTreesRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    models['GradientBoosting'] = GradientBoostingRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        max_features='sqrt',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # é«˜æ€§èƒ½æ¨¡å‹\n",
    "    if HAS_XGB:\n",
    "        models['XGBoost'] = xgb.XGBRegressor(\n",
    "            n_estimators=300,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    if HAS_LGB:\n",
    "        models['LightGBM'] = lgb.LGBMRegressor(\n",
    "            n_estimators=300,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        )\n",
    "    \n",
    "    results = []\n",
    "    trained_models = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nè®­ç»ƒæ¨¡å‹: {name}\")\n",
    "        \n",
    "        try:\n",
    "            # é€‰æ‹©è¾“å…¥æ•°æ®\n",
    "            if name in ['Ridge', 'Lasso', 'ElasticNet']:\n",
    "                X_tr, X_te = X_train_scaled, X_test_scaled\n",
    "            else:\n",
    "                X_tr, X_te = X_train, X_test\n",
    "            \n",
    "            # è®­ç»ƒæ¨¡å‹\n",
    "            model.fit(X_tr, y_train)\n",
    "            \n",
    "            # é¢„æµ‹\n",
    "            train_pred = model.predict(X_tr)\n",
    "            test_pred = model.predict(X_te)\n",
    "            \n",
    "            # è®¡ç®—æŒ‡æ ‡\n",
    "            train_mae = mean_absolute_error(y_train, train_pred)\n",
    "            test_mae = mean_absolute_error(y_test, test_pred)\n",
    "            train_r2 = r2_score(y_train, train_pred)\n",
    "            test_r2 = r2_score(y_test, test_pred)\n",
    "            \n",
    "            # äº¤å‰éªŒè¯\n",
    "            kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            cv_scores = cross_val_score(model, X_tr, y_train, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "            cv_mae = -cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "            \n",
    "            results.append({\n",
    "                'æ¨¡å‹': name,\n",
    "                'è®­ç»ƒMAE': train_mae,\n",
    "                'æµ‹è¯•MAE': test_mae,\n",
    "                'è®­ç»ƒR2': train_r2,\n",
    "                'æµ‹è¯•R2': test_r2,\n",
    "                'CV_MAE': cv_mae,\n",
    "                'CV_STD': cv_std\n",
    "            })\n",
    "            \n",
    "            trained_models[name] = (model, scaler if name in ['Ridge', 'Lasso', 'ElasticNet'] else None)\n",
    "            \n",
    "            print(f\"{name}:\")\n",
    "            print(f\"   æµ‹è¯•MAE: {test_mae:.0f}\")\n",
    "            print(f\"   æµ‹è¯•R2: {test_r2:.4f}\")\n",
    "            print(f\"   CV MAE: {cv_mae:.0f} Â± {cv_std:.0f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name} è®­ç»ƒå¤±è´¥: {str(e)}\")\n",
    "    \n",
    "    return pd.DataFrame(results), trained_models\n",
    "\n",
    "def create_meta_ensemble(trained_models, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"åˆ›å»ºå…ƒå­¦ä¹ é›†æˆæ¨¡å‹ - ä¿®å¤ç‰ˆæœ¬\"\"\"\n",
    "    print(f\"\\nğŸ”— åˆ›å»ºå…ƒå­¦ä¹ é›†æˆ...\")\n",
    "    \n",
    "    if len(trained_models) < 2:\n",
    "        print(\"  æ¨¡å‹æ•°é‡ä¸è¶³ï¼Œè·³è¿‡å…ƒå­¦ä¹ \")\n",
    "        return None, None\n",
    "    \n",
    "    # å‡†å¤‡å…ƒç‰¹å¾\n",
    "    meta_features_train = []\n",
    "    meta_features_val = []\n",
    "    model_names = []\n",
    "    \n",
    "    for name, (model, scaler) in trained_models.items():\n",
    "        try:\n",
    "            print(f\"  ç”Ÿæˆ {name} çš„å…ƒç‰¹å¾...\")\n",
    "            \n",
    "            if scaler:\n",
    "                X_tr_scaled = scaler.fit_transform(X_train)\n",
    "                X_val_scaled = scaler.transform(X_val)\n",
    "                \n",
    "                # é‡æ–°è®­ç»ƒæ¨¡å‹\n",
    "                model.fit(X_tr_scaled, y_train)\n",
    "                train_pred = model.predict(X_tr_scaled)\n",
    "                val_pred = model.predict(X_val_scaled)\n",
    "            else:\n",
    "                # é‡æ–°è®­ç»ƒæ¨¡å‹\n",
    "                model.fit(X_train, y_train)\n",
    "                train_pred = model.predict(X_train)\n",
    "                val_pred = model.predict(X_val)\n",
    "            \n",
    "            meta_features_train.append(train_pred)\n",
    "            meta_features_val.append(val_pred)\n",
    "            model_names.append(name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  æ¨¡å‹ {name} ç”Ÿæˆå…ƒç‰¹å¾å¤±è´¥: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(meta_features_train) < 2:\n",
    "        print(\"  å¯ç”¨æ¨¡å‹ä¸è¶³ï¼Œè·³è¿‡å…ƒå­¦ä¹ \")\n",
    "        return None, None\n",
    "    \n",
    "    # æ„å»ºå…ƒç‰¹å¾çŸ©é˜µ\n",
    "    meta_X_train = np.column_stack(meta_features_train)\n",
    "    meta_X_val = np.column_stack(meta_features_val)\n",
    "    \n",
    "    # è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆç®€å•çº¿æ€§å›å½’ï¼‰\n",
    "    meta_model = Ridge(alpha=1.0)\n",
    "    meta_model.fit(meta_X_train, y_train)\n",
    "    \n",
    "    # éªŒè¯å…ƒæ¨¡å‹\n",
    "    meta_pred = meta_model.predict(meta_X_val)\n",
    "    meta_mae = mean_absolute_error(y_val, meta_pred)\n",
    "    meta_r2 = r2_score(y_val, meta_pred)\n",
    "    \n",
    "    print(f\"å…ƒå­¦ä¹ é›†æˆå®Œæˆ:\")\n",
    "    print(f\"   ä½¿ç”¨æ¨¡å‹: {', '.join(model_names)}\")\n",
    "    print(f\"   éªŒè¯MAE: {meta_mae:.0f}\")\n",
    "    print(f\"   éªŒè¯R2: {meta_r2:.4f}\")\n",
    "    \n",
    "    return meta_model, model_names\n",
    "\n",
    "def main():\n",
    "    # æ•°æ®è·¯å¾„\n",
    "    train_path = \"ruc_Class25Q1_train.csv\"\n",
    "    test_path = \"ruc_Class25Q1_test.csv\"\n",
    "    community_path = \"ruc_Class25Q1_details.csv\"\n",
    "    rent_path = \"ruc_Class25Q1_rent.csv\"\n",
    "    \n",
    "    print(f\"\\næ•°æ®åŠ è½½ä¸­...\")\n",
    "    \n",
    "    # åŠ è½½æ•°æ®\n",
    "    try:\n",
    "        train_data = pd.read_csv(train_path)\n",
    "        test_data = pd.read_csv(test_path)\n",
    "        print(f\"ä¸»æ•°æ®åŠ è½½æˆåŠŸ\")\n",
    "        print(f\"   è®­ç»ƒé›†: {train_data.shape}\")\n",
    "        print(f\"   æµ‹è¯•é›†: {test_data.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ä¸»æ•°æ®åŠ è½½å¤±è´¥: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # åŠ è½½è¾…åŠ©æ•°æ®\n",
    "    community_data, rent_data = None, None\n",
    "    try:\n",
    "        community_data = pd.read_csv(community_path)\n",
    "        print(f\"å°åŒºæ•°æ®: {community_data.shape}\")\n",
    "    except:\n",
    "        print(f\"å°åŒºæ•°æ®åŠ è½½å¤±è´¥\")\n",
    "    \n",
    "    try:\n",
    "        rent_data = pd.read_csv(rent_path)\n",
    "        print(f\"ç§Ÿé‡‘æ•°æ®: {rent_data.shape}\")\n",
    "    except:\n",
    "        print(f\"ç§Ÿé‡‘æ•°æ®åŠ è½½å¤±è´¥\")\n",
    "    \n",
    "    # ä¿å­˜æµ‹è¯•é›†ID\n",
    "    original_test_ids = test_data['ID'].copy()\n",
    "    \n",
    "    print(f\"\\né«˜çº§ç‰¹å¾å·¥ç¨‹...\")\n",
    "    \n",
    "    # ç‰¹å¾å·¥ç¨‹æµæ°´çº¿ - æ·»åŠ é•¿åº¦ç›‘æ§\n",
    "    print(\"åŸºç¡€ç‰¹å¾æå–...\")\n",
    "    print(f\"   å¤„ç†å‰ - è®­ç»ƒé›†: {len(train_data)}, æµ‹è¯•é›†: {len(test_data)}\")\n",
    "    \n",
    "    train_processed, training_stats = advanced_feature_engineering(train_data, is_training=True)\n",
    "    test_processed, _ = advanced_feature_engineering(test_data, is_training=False, training_stats=training_stats)\n",
    "    \n",
    "    print(f\"   å¤„ç†å - è®­ç»ƒé›†: {len(train_processed)}, æµ‹è¯•é›†: {len(test_processed)}\")\n",
    "    \n",
    "    print(\"å¤–éƒ¨æ•°æ®èåˆ...\")\n",
    "    train_merged = advanced_external_merge(train_processed, community_data, rent_data)\n",
    "    test_merged = advanced_external_merge(test_processed, community_data, rent_data)\n",
    "    \n",
    "    print(f\"   èåˆå - è®­ç»ƒé›†: {len(train_merged)}, æµ‹è¯•é›†: {len(test_merged)}\")\n",
    "    \n",
    "    # å¦‚æœæµ‹è¯•é›†é•¿åº¦å‘ç”Ÿå˜åŒ–ï¼Œå»é‡å¤„ç†\n",
    "    if len(test_merged) != len(test_data):\n",
    "        print(f\" æµ‹è¯•é›†é•¿åº¦å˜åŒ–: {len(test_data)} -> {len(test_merged)}\")\n",
    "        print(\"   è¿›è¡Œå»é‡å¤„ç†...\")\n",
    "        \n",
    "        # æŒ‰IDå»é‡ï¼Œä¿æŒç¬¬ä¸€æ¬¡å‡ºç°çš„è®°å½•\n",
    "        if 'ID' in test_merged.columns:\n",
    "            test_merged = test_merged.drop_duplicates(subset=['ID'], keep='first')\n",
    "            print(f\"   å»é‡åæµ‹è¯•é›†é•¿åº¦: {len(test_merged)}\")\n",
    "            \n",
    "            # ç¡®ä¿IDé¡ºåºä¸åŸå§‹ä¸€è‡´\n",
    "            test_merged = test_merged.set_index('ID').reindex(original_test_ids).reset_index()\n",
    "            print(f\"   é‡æ’åºåæµ‹è¯•é›†é•¿åº¦: {len(test_merged)}\")\n",
    "    \n",
    "    print(\"é«˜çº§ç»„åˆç‰¹å¾...\")\n",
    "    train_final, feature_stats = create_advanced_features(train_merged, is_training=True)\n",
    "    test_final, _ = create_advanced_features(test_merged, is_training=False, training_stats=feature_stats)\n",
    "    \n",
    "    print(f\"   æœ€ç»ˆ - è®­ç»ƒé›†: {len(train_final)}, æµ‹è¯•é›†: {len(test_final)}\")\n",
    "    \n",
    "    # æœ€ç»ˆç¡®ä¿æµ‹è¯•é›†é•¿åº¦æ­£ç¡®\n",
    "    if len(test_final) != len(original_test_ids):\n",
    "        print(f\"æµ‹è¯•é›†é•¿åº¦ä»ç„¶ä¸åŒ¹é…: {len(test_final)} vs {len(original_test_ids)}\")\n",
    "        \n",
    "        # å¼ºåˆ¶ä¿®æ­£\n",
    "        if 'ID' in test_final.columns:\n",
    "            # æŒ‰åŸå§‹IDé¡ºåºé‡å»ºæµ‹è¯•é›†\n",
    "            test_final_indexed = test_final.set_index('ID')\n",
    "            test_final = test_final_indexed.reindex(original_test_ids).reset_index()\n",
    "            \n",
    "            # å¡«å……ç¼ºå¤±è¡Œï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰\n",
    "            test_final = test_final.ffill().bfill().fillna(0)\n",
    "            print(f\"   ä¿®æ­£åæµ‹è¯•é›†é•¿åº¦: {len(test_final)}\")\n",
    "        else:\n",
    "            # å¦‚æœæ²¡æœ‰IDåˆ—ï¼Œç›´æ¥æˆªå–\n",
    "            test_final = test_final.iloc[:len(original_test_ids)]\n",
    "            print(f\"   æˆªå–åæµ‹è¯•é›†é•¿åº¦: {len(test_final)}\")\n",
    "    \n",
    "    # ç‰¹å¾é€‰æ‹© - ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾ä¸€è‡´ï¼Œå¹¶æ’é™¤é—®é¢˜ç‰¹å¾\n",
    "    train_features = set(train_final.columns)\n",
    "    test_features = set(test_final.columns)\n",
    "    common_features = list(train_features & test_features)\n",
    "    \n",
    "    # æ’é™¤ä¸éœ€è¦çš„åˆ—\n",
    "    exclude_columns = ['ID', 'ä»·æ ¼', 'Unnamed: 0', 'äº¤æ˜“æ—¶é—´', 'å°åŒºåç§°', 'åç§°'] + \\\n",
    "                     [col for col in common_features if col.endswith('_clean')]\n",
    "    \n",
    "    all_features = [col for col in common_features \n",
    "                   if col not in exclude_columns]\n",
    "    \n",
    "    print(f\"è®­ç»ƒé›†ç‰¹å¾æ•°: {len(train_features)}\")\n",
    "    print(f\"æµ‹è¯•é›†ç‰¹å¾æ•°: {len(test_features)}\")\n",
    "    print(f\"å…±åŒç‰¹å¾æ•°: {len(common_features)}\")\n",
    "    print(f\"æ’é™¤ç‰¹å¾æ•°: {len(exclude_columns)}\")\n",
    "    print(f\"å¯ç”¨ç‰¹å¾æ•°: {len(all_features)}\")\n",
    "    \n",
    "    # åˆ†ç¦»æ•°å€¼å’Œåˆ†ç±»ç‰¹å¾\n",
    "    numeric_features = []\n",
    "    categorical_features = []\n",
    "    \n",
    "    for col in all_features:\n",
    "        if col in train_final.columns:\n",
    "            if train_final[col].dtype in ['object']:\n",
    "                categorical_features.append(col)\n",
    "            else:\n",
    "                numeric_features.append(col)\n",
    "    \n",
    "    feature_types = {\n",
    "        'numeric': numeric_features,\n",
    "        'categorical': categorical_features\n",
    "    }\n",
    "    \n",
    "    # å‡†å¤‡è®­ç»ƒæ•°æ® - å®‰å…¨çš„ç‰¹å¾é€‰æ‹©\n",
    "    available_features_train = [col for col in all_features if col in train_final.columns]\n",
    "    available_features_test = [col for col in all_features if col in test_final.columns]\n",
    "    final_features = list(set(available_features_train) & set(available_features_test))\n",
    "    \n",
    "    print(f\"æœ€ç»ˆä½¿ç”¨ç‰¹å¾æ•°: {len(final_features)}\")\n",
    "    \n",
    "    if len(final_features) == 0:\n",
    "        print(\"æ²¡æœ‰å¯ç”¨çš„å…±åŒç‰¹å¾!\")\n",
    "        return None, None\n",
    "    \n",
    "    X = train_final[final_features]\n",
    "    y = train_final['ä»·æ ¼']\n",
    "    X_submit = test_final[final_features]\n",
    "    \n",
    "    print(f\"\\nç‰¹å¾ç»Ÿè®¡:\")\n",
    "    print(f\"   æ€»ç‰¹å¾æ•°: {len(all_features)}\")\n",
    "    print(f\"   æ•°å€¼ç‰¹å¾: {len(numeric_features)}\")\n",
    "    print(f\"   åˆ†ç±»ç‰¹å¾: {len(categorical_features)}\")\n",
    "    \n",
    "    # å¼‚å¸¸å€¼å¤„ç†ï¼ˆæ›´æ¸©å’Œï¼‰- åªå¯¹è®­ç»ƒé›†è¿›è¡Œ\n",
    "    print(f\"\\nğŸ” å¼‚å¸¸å€¼å¤„ç†...\")\n",
    "    Q1 = y.quantile(0.05)  # æ›´å®½æ¾çš„ç•Œé™\n",
    "    Q3 = y.quantile(0.95)\n",
    "    mask = (y >= Q1) & (y <= Q3)\n",
    "    \n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "    \n",
    "    print(f\"   åŸå§‹è®­ç»ƒæ ·æœ¬: {len(X)}\")\n",
    "    print(f\"   æ¸…æ´—åè®­ç»ƒæ ·æœ¬: {len(X_clean)}\")\n",
    "    print(f\"   ç§»é™¤æ¯”ä¾‹: {(1 - len(X_clean)/len(X))*100:.1f}%\")\n",
    "    print(f\"   æµ‹è¯•é›†ä¿æŒä¸å˜: {len(X_submit)} æ ·æœ¬\")\n",
    "    \n",
    "    # æ£€æŸ¥æµ‹è¯•é›†é•¿åº¦æ˜¯å¦åŒ¹é…\n",
    "    if len(X_submit) != len(original_test_ids):\n",
    "        print(f\"æµ‹è¯•é›†é•¿åº¦ä¸åŒ¹é…: {len(X_submit)} vs {len(original_test_ids)}\")\n",
    "        print(\"   å°è¯•ä¿®æ­£æµ‹è¯•é›†é•¿åº¦...\")\n",
    "        \n",
    "        if len(X_submit) > len(original_test_ids):\n",
    "            # å¦‚æœæµ‹è¯•é›†å˜é•¿äº†ï¼Œæˆªå–åˆ°åŸå§‹é•¿åº¦\n",
    "            X_submit = X_submit.iloc[:len(original_test_ids)]\n",
    "            print(f\"   æˆªå–åæµ‹è¯•é›†é•¿åº¦: {len(X_submit)}\")\n",
    "        else:\n",
    "            # å¦‚æœæµ‹è¯•é›†å˜çŸ­äº†ï¼Œç”¨æœ€åä¸€è¡Œå¡«å……\n",
    "            last_row = X_submit.iloc[-1:] if len(X_submit) > 0 else pd.DataFrame()\n",
    "            while len(X_submit) < len(original_test_ids):\n",
    "                X_submit = pd.concat([X_submit, last_row], ignore_index=True)\n",
    "            print(f\"   å¡«å……åæµ‹è¯•é›†é•¿åº¦: {len(X_submit)}\")\n",
    "    \n",
    "    # æœ€ç»ˆç¡®ä¿é•¿åº¦åŒ¹é…\n",
    "    assert len(X_submit) == len(original_test_ids), f\"ä¿®æ­£åæµ‹è¯•é›†é•¿åº¦ä»ä¸åŒ¹é…: {len(X_submit)} vs {len(original_test_ids)}\"\n",
    "    \n",
    "    # æ•°æ®åˆ†å‰²\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_clean, y_clean, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n æ•°æ®é¢„å¤„ç†...\")\n",
    "    X_train_processed, X_val_processed, le_dict = advanced_preprocessing(X_train, X_val, feature_types)\n",
    "    X_train_processed, X_submit_processed, _ = advanced_preprocessing(X_train_processed, X_submit, feature_types)\n",
    "    \n",
    "    # é•¿åº¦æ£€æŸ¥\n",
    "    print(f\"é¢„å¤„ç†åé•¿åº¦æ£€æŸ¥:\")\n",
    "    print(f\"  è®­ç»ƒé›†: {len(X_train_processed)}\")\n",
    "    print(f\"  éªŒè¯é›†: {len(X_val_processed)}\")  \n",
    "    print(f\"  æµ‹è¯•é›†: {len(X_submit_processed)}\")\n",
    "    print(f\"  åŸå§‹æµ‹è¯•ID: {len(original_test_ids)}\")       \n",
    "    print(f\"ä¿®æ­£åæµ‹è¯•é›†é•¿åº¦: {len(X_submit_processed)}\")\n",
    "    \n",
    "    # ç‰¹å¾é€‰æ‹©\n",
    "    var_selector, k_best, selected_features = feature_selection(\n",
    "        X_train_processed, y_train, X_train_processed.columns, n_features=60\n",
    "    )\n",
    "    \n",
    "    # åº”ç”¨ç‰¹å¾é€‰æ‹©\n",
    "    X_train_selected = k_best.transform(var_selector.transform(X_train_processed))\n",
    "    X_val_selected = k_best.transform(var_selector.transform(X_val_processed))\n",
    "    X_submit_selected = k_best.transform(var_selector.transform(X_submit_processed))\n",
    "    \n",
    "    # ç‰¹å¾é€‰æ‹©åé•¿åº¦æ£€æŸ¥\n",
    "    print(f\"ç‰¹å¾é€‰æ‹©åé•¿åº¦æ£€æŸ¥:\")\n",
    "    print(f\"  è®­ç»ƒé›†: {X_train_selected.shape}\")\n",
    "    print(f\"  éªŒè¯é›†: {X_val_selected.shape}\")\n",
    "    print(f\"  æµ‹è¯•é›†: {X_submit_selected.shape}\")\n",
    "    \n",
    "    # ç¡®ä¿æµ‹è¯•é›†è¡Œæ•°æ­£ç¡®\n",
    "    assert X_submit_selected.shape[0] == len(original_test_ids), f\"ç‰¹å¾é€‰æ‹©åé•¿åº¦ä¸åŒ¹é…: {X_submit_selected.shape[0]} vs {len(original_test_ids)}\"\n",
    "    \n",
    "    print(f\"\\næ¨¡å‹è®­ç»ƒå¼€å§‹...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # è®­ç»ƒé«˜çº§æ¨¡å‹\n",
    "    results_df, trained_models = train_advanced_models(\n",
    "        X_train_selected, X_val_selected, y_train, y_val\n",
    "    )\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    if len(results_df) > 0:\n",
    "        print(f\"\\næ¨¡å‹æ€§èƒ½æ’è¡Œæ¦œ:\")\n",
    "        print(\"=\" * 80)\n",
    "        results_sorted = results_df.sort_values('CV_MAE')\n",
    "        print(results_sorted[['æ¨¡å‹', 'æµ‹è¯•MAE', 'æµ‹è¯•R2', 'CV_MAE', 'CV_STD']].to_string(index=False))\n",
    "        \n",
    "        # åˆ›å»ºå…ƒå­¦ä¹ é›†æˆ\n",
    "        meta_model, meta_model_names = create_meta_ensemble(\n",
    "            trained_models, X_train_selected, y_train, X_val_selected, y_val\n",
    "        )\n",
    "        \n",
    "        print(f\"\\næœ€ç»ˆé¢„æµ‹ç”Ÿæˆ...\")\n",
    "        \n",
    "        # é€‰æ‹©æœ€ä½³æ¨¡å‹\n",
    "        best_model_name = results_sorted.iloc[0]['æ¨¡å‹']\n",
    "        best_model, best_scaler = trained_models[best_model_name]\n",
    "        \n",
    "        print(f\"æœ€ä½³å•ä¸€æ¨¡å‹: {best_model_name}\")\n",
    "        \n",
    "        # é‡æ–°è®­ç»ƒæœ€ä½³æ¨¡å‹\n",
    "        X_full_train = np.vstack([X_train_selected, X_val_selected])\n",
    "        y_full_train = np.hstack([y_train, y_val])\n",
    "        \n",
    "        if best_scaler:\n",
    "            X_full_scaled = best_scaler.fit_transform(X_full_train)\n",
    "            X_submit_scaled = best_scaler.transform(X_submit_selected)\n",
    "            best_model.fit(X_full_scaled, y_full_train)\n",
    "            best_predictions = best_model.predict(X_submit_scaled)\n",
    "        else:\n",
    "            best_model.fit(X_full_train, y_full_train)\n",
    "            best_predictions = best_model.predict(X_submit_selected)\n",
    "        \n",
    "        # é¢„æµ‹é•¿åº¦æ£€æŸ¥\n",
    "        print(f\"é¢„æµ‹é•¿åº¦æ£€æŸ¥:\")\n",
    "        print(f\"  é¢„æµ‹ç»“æœ: {len(best_predictions)}\")\n",
    "        print(f\"  åŸå§‹æµ‹è¯•ID: {len(original_test_ids)}\")\n",
    "        \n",
    "        # ç¡®ä¿é¢„æµ‹é•¿åº¦æ­£ç¡®\n",
    "        if len(best_predictions) != len(original_test_ids):\n",
    "            print(f\"é¢„æµ‹é•¿åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œä¿®æ­£...\")\n",
    "            if len(best_predictions) > len(original_test_ids):\n",
    "                best_predictions = best_predictions[:len(original_test_ids)]\n",
    "            else:\n",
    "                print(f\" é¢„æµ‹ç»“æœè¿‡çŸ­: {len(best_predictions)} < {len(original_test_ids)}\")\n",
    "                return None, None\n",
    "        \n",
    "        final_predictions = best_predictions\n",
    "        \n",
    "        # å¦‚æœæœ‰å…ƒå­¦ä¹ æ¨¡å‹ï¼Œè¿›è¡Œé›†æˆ\n",
    "        if meta_model and meta_model_names:\n",
    "            print(f\"ğŸ”— åº”ç”¨å…ƒå­¦ä¹ é›†æˆ...\")\n",
    "            # ç”Ÿæˆå…ƒç‰¹å¾\n",
    "            meta_features_submit = []\n",
    "            for name in meta_model_names:\n",
    "                model, scaler = trained_models[name]\n",
    "                \n",
    "                # é‡æ–°è®­ç»ƒæ¯ä¸ªæ¨¡å‹\n",
    "                if scaler:\n",
    "                    X_full_scaled = scaler.fit_transform(X_full_train)\n",
    "                    X_submit_scaled = scaler.transform(X_submit_selected)\n",
    "                    model.fit(X_full_scaled, y_full_train)\n",
    "                    pred = model.predict(X_submit_scaled)\n",
    "                else:\n",
    "                    model.fit(X_full_train, y_full_train)\n",
    "                    pred = model.predict(X_submit_selected)\n",
    "                    \n",
    "                meta_features_submit.append(pred)\n",
    "            \n",
    "            if len(meta_features_submit) > 0:\n",
    "                meta_X_submit = np.column_stack(meta_features_submit)\n",
    "                \n",
    "                # æ£€æŸ¥å…ƒç‰¹å¾é•¿åº¦\n",
    "                print(f\"å…ƒç‰¹å¾é•¿åº¦æ£€æŸ¥: {meta_X_submit.shape[0]} vs {len(original_test_ids)}\")\n",
    "                \n",
    "                # é‡æ–°è®­ç»ƒå…ƒæ¨¡å‹\n",
    "                # ç”Ÿæˆè®­ç»ƒæ—¶çš„å…ƒç‰¹å¾\n",
    "                meta_features_train = []\n",
    "                for name in meta_model_names:\n",
    "                    model, scaler = trained_models[name]\n",
    "                    if scaler:\n",
    "                        X_scaled = scaler.fit_transform(X_full_train)\n",
    "                        model.fit(X_scaled, y_full_train)\n",
    "                        pred = model.predict(X_scaled)\n",
    "                    else:\n",
    "                        model.fit(X_full_train, y_full_train)\n",
    "                        pred = model.predict(X_full_train)\n",
    "                    meta_features_train.append(pred)\n",
    "                \n",
    "                meta_X_train = np.column_stack(meta_features_train)\n",
    "                meta_model.fit(meta_X_train, y_full_train)\n",
    "                ensemble_predictions = meta_model.predict(meta_X_submit)\n",
    "                \n",
    "                # é•¿åº¦åŒ¹é…æ£€æŸ¥\n",
    "                if len(ensemble_predictions) == len(best_predictions):\n",
    "                    # æ··åˆé¢„æµ‹ï¼ˆç»™æœ€ä½³æ¨¡å‹æ›´é«˜æƒé‡ï¼‰\n",
    "                    final_predictions = 0.8 * best_predictions + 0.2 * ensemble_predictions\n",
    "                    print(f\"ä½¿ç”¨é›†æˆé¢„æµ‹ (æœ€ä½³æ¨¡å‹80% + å…ƒå­¦ä¹ 20%)\")\n",
    "                else:\n",
    "                    print(f\"é›†æˆé¢„æµ‹é•¿åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨æœ€ä½³å•ä¸€æ¨¡å‹\")\n",
    "            else:\n",
    "                print(f\"å…ƒå­¦ä¹ ç‰¹å¾ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æœ€ä½³å•ä¸€æ¨¡å‹\")\n",
    "        else:\n",
    "            print(f\"ä½¿ç”¨æœ€ä½³å•ä¸€æ¨¡å‹é¢„æµ‹\")\n",
    "        \n",
    "        # åˆ›å»ºæäº¤æ–‡ä»¶\n",
    "        print(f\"\\nåˆ›å»ºæäº¤æ–‡ä»¶...\")\n",
    "        print(f\"æœ€ç»ˆé•¿åº¦æ£€æŸ¥:\")\n",
    "        print(f\"  é¢„æµ‹ç»“æœ: {len(final_predictions)}\")\n",
    "        print(f\"  æµ‹è¯•é›†ID: {len(original_test_ids)}\")\n",
    "        \n",
    "        # æœ€ç»ˆé•¿åº¦ç¡®ä¿\n",
    "        if len(final_predictions) != len(original_test_ids):\n",
    "            print(f\"æœ€ç»ˆé•¿åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œä¿®æ­£...\")\n",
    "            min_length = min(len(final_predictions), len(original_test_ids))\n",
    "            final_predictions = final_predictions[:min_length]\n",
    "            test_ids_to_use = original_test_ids.iloc[:min_length]\n",
    "        else:\n",
    "            test_ids_to_use = original_test_ids\n",
    "        \n",
    "        submission = pd.DataFrame({\n",
    "            'ID': test_ids_to_use,\n",
    "            'ä»·æ ¼': final_predictions\n",
    "        })\n",
    "        \n",
    "        submission.to_csv('high_performance_submission.csv', index=False)\n",
    "        \n",
    "        print(f\"\\né¢„æµ‹å®Œæˆ!\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"é¢„æµ‹ç»Ÿè®¡:\")\n",
    "        print(f\"   æ ·æœ¬æ•°é‡: {len(submission)}\")\n",
    "        print(f\"   ä»·æ ¼èŒƒå›´: {submission['ä»·æ ¼'].min():.0f} - {submission['ä»·æ ¼'].max():.0f}\")\n",
    "        print(f\"   å¹³å‡ä»·æ ¼: {submission['ä»·æ ¼'].mean():.0f}\")\n",
    "        print(f\"   ä¸­ä½ä»·æ ¼: {submission['ä»·æ ¼'].median():.0f}\")\n",
    "        print(f\"   æ ‡å‡†å·®: {submission['ä»·æ ¼'].std():.0f}\")\n",
    "        print(f\"\\næ–‡ä»¶ä¿å­˜: high_performance_submission.csv\")\n",
    "        \n",
    "        return submission, results_df\n",
    "    \n",
    "    else:\n",
    "        print(\"æ²¡æœ‰æˆåŠŸè®­ç»ƒçš„æ¨¡å‹\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    submission, results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a42c0-2530-420c-97a5-f36b66baf1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
