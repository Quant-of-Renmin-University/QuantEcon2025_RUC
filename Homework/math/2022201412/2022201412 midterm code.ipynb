{"cells":[{"cell_type":"markdown","metadata":{"id":"BAEE51CAC7804597B5D0E06FB209A34B","trusted":true,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":true,"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"67ebe8c65302e998c320c3fd"},"source":"## æœŸä¸­å±•ç¤º"},{"cell_type":"markdown","metadata":{"id":"7A4BD9D911BB44C1A4FD653748B0C454","notebookId":"67ebe8c65302e998c320c3fd","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 1.OLSå›žå½’ï¼Œå¹¶åŠ ä¸Šäº†çš®å°”é€Šç³»æ•°æ£€éªŒä¸Žå…±çº¿æ€§æ£€éªŒï¼Œä¾¿äºŽå³æ—¶è°ƒæ•´å˜é‡ã€‚"},{"cell_type":"code","metadata":{"id":"4BF2B493A8D54A9B87D52E1BDA06CB09","notebookId":"67ebe8c65302e998c320c3fd","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\n# ========== 1. è¯»å–æ•°æ® ==========\ntrain_df = pd.read_csv('/home/mw/input/quant4533/ruc_Class25Q1_train.csv')\ntest_df = pd.read_csv('/home/mw/input/quant4533/ruc_Class25Q1_test.csv')\noriginal_test = test_df[['ID']].copy()\n\n# ========== 2. ç‰¹å¾å·¥ç¨‹å‡½æ•° ==========\ndef clean_area(df):\n    for col in ['å»ºç­‘é¢ç§¯', 'å¥—å†…é¢ç§¯']:\n        df[col] = df[col].astype(str).str.replace('ãŽ¡', '', regex=False)\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n    df['æœ‰æ•ˆé¢ç§¯'] = df['å¥—å†…é¢ç§¯'].fillna(df['å»ºç­‘é¢ç§¯'])\n    df['æœ‰æ•ˆé¢ç§¯'] = df['æœ‰æ•ˆé¢ç§¯'].fillna(df['æœ‰æ•ˆé¢ç§¯'].median())\n    return df\n# é‡æ–°è®¡ç®—æˆ¿å±‹ç”¨é€”å‡ä»·æ˜ å°„ï¼ˆç¡®ä¿å·²ç”Ÿæˆï¼‰\nç”¨é€”å‡ä»· = train_df.groupby('æˆ¿å±‹ç”¨é€”')['ä»·æ ¼'].mean()\ntrain_df['æˆ¿å±‹ç”¨é€”å‡ä»·'] = train_df['æˆ¿å±‹ç”¨é€”'].map(ç”¨é€”å‡ä»·).fillna(train_df['ä»·æ ¼'].mean())\n\n# ç¡®ä¿åŸŽå¸‚ç¼–ç ä¸Žæˆ¿å±‹å¹´é™ç­‰çº§å­—æ®µå·²å­˜åœ¨\nif 'åŸŽå¸‚ç¼–ç ' not in train_df.columns:\n    train_df['åŸŽå¸‚ç¼–ç '] = train_df['åŸŽå¸‚'].astype('category').cat.codes\nif 'æˆ¿å±‹å¹´é™ç­‰çº§' not in train_df.columns:\n    train_df['æˆ¿å±‹å¹´é™ç­‰çº§'] = train_df['æˆ¿å±‹å¹´é™'].map(lambda x: 3 if 'æ»¡äº”' in str(x) else (2 if 'æ»¡ä¸¤' in str(x) else 1))\n\n# åˆ›å»ºåŸŽå¸‚å¹´é™äº¤äº’é¡¹\ntrain_df['åŸŽå¸‚å¹´é™äº¤äº’'] = train_df['åŸŽå¸‚ç¼–ç '] * train_df['æˆ¿å±‹å¹´é™ç­‰çº§']\ndef extract_floor(df):\n    df['å½“å‰æ¥¼å±‚_raw'] = df['æ‰€åœ¨æ¥¼å±‚'].str.extract(r'ç¬¬?(\\d+)[å±‚æ¥¼]')[0].astype(float)\n    df['æ€»æ¥¼å±‚'] = df['æ‰€åœ¨æ¥¼å±‚'].str.extract(r'å…±?(\\d+)[å±‚æ¥¼]')[0].astype(float)\n    df.loc[df['å½“å‰æ¥¼å±‚_raw'].isna(), 'å½“å‰æ¥¼å±‚_raw'] = df['æ‰€åœ¨æ¥¼å±‚'].str.extract(r'(åº•|ä½Ž|ä¸­|é«˜|é¡¶)')[0].map({\n        'åº•': 1, 'ä½Ž': 2, 'ä¸­': 3, 'é«˜': 4, 'é¡¶': 5\n    })\n    df['å½“å‰æ¥¼å±‚_raw'] = df['å½“å‰æ¥¼å±‚_raw'].fillna(1)\n    df['æ€»æ¥¼å±‚'] = df['æ€»æ¥¼å±‚'].fillna(df['æ€»æ¥¼å±‚'].median())\n    df['å½“å‰æ¥¼å±‚'] = (df['å½“å‰æ¥¼å±‚_raw'] / df['æ€»æ¥¼å±‚']).clip(0, 1)\n    df.drop(columns=['å½“å‰æ¥¼å±‚_raw'], inplace=True)\n    return df\n\ndef extract_layout(df):\n    df['å®¤'] = df['æˆ¿å±‹æˆ·åž‹'].str.extract(r'(\\d+)å®¤').fillna(0).astype(int)\n    df['åŽ…'] = df['æˆ¿å±‹æˆ·åž‹'].str.extract(r'(\\d+)åŽ…').fillna(0).astype(int)\n    df['åŽ¨'] = df['æˆ¿å±‹æˆ·åž‹'].str.extract(r'(\\d+)åŽ¨').fillna(0).astype(int)\n    df['å«'] = df['æˆ¿å±‹æˆ·åž‹'].str.extract(r'(\\d+)å«').fillna(0).astype(int)\n    df['æ€»æˆ¿é—´æ•°'] = df[['å®¤', 'åŽ…', 'åŽ¨', 'å«']].sum(axis=1)\n    return df\n\ndef enrich_features(df):\n    df['æ˜¯å¦æ»¡äº”'] = df['æˆ¿å±‹å¹´é™'].apply(lambda x: 1 if 'æ»¡äº”' in str(x) else 0)\n    df['æœå‘_å«å—'] = df['æˆ¿å±‹æœå‘'].apply(lambda x: 1 if 'å—' in str(x) else 0)\n    df['é…å¤‡ç”µæ¢¯'] = df['é…å¤‡ç”µæ¢¯'].map({'æœ‰': 1, 'æ— ': 0}).fillna(0)\n    df['åŸŽå¸‚ç¼–ç '] = df['åŸŽå¸‚'].astype('category').cat.codes\n    df['åŒºåŸŸç¼–ç '] = df['åŒºåŸŸ'].astype('category').cat.codes\n    df['æˆ¿å±‹å¹´é™ç­‰çº§'] = df['æˆ¿å±‹å¹´é™'].map(lambda x: 3 if 'æ»¡äº”' in str(x) else (2 if 'æ»¡ä¸¤' in str(x) else 1))\n    ring_map = {'ä¸€çŽ¯å†…': 1.0, 'ä¸€è‡³äºŒçŽ¯': 1.5, 'äºŒçŽ¯å†…': 2.0, 'äºŒè‡³ä¸‰çŽ¯': 2.5,\n                'ä¸‰è‡³å››çŽ¯': 3.5, 'å››è‡³äº”çŽ¯': 4.5, 'ä¸‰çŽ¯å¤–': 5.0, 'äº”è‡³å…­çŽ¯': 5.5,\n                'å››çŽ¯å¤–': 6.0, 'å…­çŽ¯å¤–': 6.5, 'å†…çŽ¯å†…': 1.0, 'å†…çŽ¯è‡³ä¸­çŽ¯': 1.8,\n                'ä¸­çŽ¯è‡³å¤–çŽ¯': 4.2, 'å†…çŽ¯è‡³å¤–çŽ¯': 3.0, 'å¤–çŽ¯å¤–': 7.0}\n    df['çŽ¯çº¿æ•°å€¼'] = df['çŽ¯çº¿'].map(ring_map).fillna(4.0)\n    df['çŽ¯çº¿é¢ç§¯'] = df['çŽ¯çº¿æ•°å€¼'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['æ˜¯å¦ç²¾è£…'] = (df['è£…ä¿®æƒ…å†µ'].fillna('å…¶ä»–') == 'ç²¾è£…').astype(int)\n    df['ç²¾è£…é¢ç§¯'] = df['æ˜¯å¦ç²¾è£…'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['äº§æƒ_éžå…±æœ‰'] = (df['äº§æƒæ‰€å±ž'].fillna('å…¶ä»–') == 'éžå…±æœ‰').astype(int)\n    df['äº§æƒ_å…±æœ‰'] = (df['äº§æƒæ‰€å±ž'].fillna('å…¶ä»–') == 'å…±æœ‰').astype(int)\n    df['äº§æƒ_éžå…±æœ‰_é¢ç§¯'] = df['äº§æƒ_éžå…±æœ‰'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['äº§æƒ_å…±æœ‰_é¢ç§¯'] = df['äº§æƒ_å…±æœ‰'] * df['æœ‰æ•ˆé¢ç§¯']\n    return df\n\ndef cluster_price(df, ref_df=None):\n    df['lon'] = pd.to_numeric(df['lon'], errors='coerce') + np.random.normal(0, 0.01, size=len(df))\n    df['lat'] = pd.to_numeric(df['lat'], errors='coerce') + np.random.normal(0, 0.01, size=len(df))\n    if ref_df is None:\n        kmeans = KMeans(n_clusters=10, random_state=42)\n        df['èšç±»æ ‡ç­¾'] = kmeans.fit_predict(df[['lon', 'lat']])\n        df['å•ä»·'] = df['ä»·æ ¼'] / df['æœ‰æ•ˆé¢ç§¯']\n        df['èšç±»å‡å•ä»·'] = df.groupby('èšç±»æ ‡ç­¾')['å•ä»·'].transform('mean')\n        df['èšç±»ä¼°ä»·'] = df['èšç±»å‡å•ä»·'] * df['æœ‰æ•ˆé¢ç§¯']\n        return df, kmeans, df.groupby('èšç±»æ ‡ç­¾')['å•ä»·'].mean()\n    else:\n        df['èšç±»æ ‡ç­¾'] = ref_df[0].predict(df[['lon', 'lat']])\n        df['èšç±»å‡å•ä»·'] = df['èšç±»æ ‡ç­¾'].map(ref_df[1])\n        df['èšç±»ä¼°ä»·'] = df['èšç±»å‡å•ä»·'] * df['æœ‰æ•ˆé¢ç§¯']\n        return df\n\n# ========== 3. æ¸…æ´—ä¸Žç‰¹å¾å·¥ç¨‹ ==========\nfor df in [train_df, test_df]:\n    df = clean_area(df)\n    df = extract_floor(df)\n    df = extract_layout(df)\n    df = enrich_features(df)\ntest_df['æˆ¿å±‹ç”¨é€”å‡ä»·'] = test_df['æˆ¿å±‹ç”¨é€”'].map(ç”¨é€”å‡ä»·).fillna(train_df['ä»·æ ¼'].mean())\ntest_df['åŸŽå¸‚å¹´é™äº¤äº’'] = test_df['åŸŽå¸‚ç¼–ç '] * test_df['æˆ¿å±‹å¹´é™ç­‰çº§']\ntrain_df, kmeans_model, price_map = cluster_price(train_df)\ntest_df = cluster_price(test_df, ref_df=(kmeans_model, price_map))\n\nfor df in [train_df, test_df]:\n    df['æˆ¿é—´Ã—èšç±»å‡å•ä»·'] = df['æ€»æˆ¿é—´æ•°'] * df['èšç±»å‡å•ä»·']\n    df['ç”µæ¢¯Ã—é¢ç§¯'] = df['é…å¤‡ç”µæ¢¯'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['ç²¾è£…Ã—é¢ç§¯Ã—å‡ä»·'] = df['æ˜¯å¦ç²¾è£…'] * df['æœ‰æ•ˆé¢ç§¯'] * df['èšç±»å‡å•ä»·']\n    df['éžå…±æœ‰Ã—ç”µæ¢¯Ã—é¢ç§¯'] = df['äº§æƒ_éžå…±æœ‰'] * df['é…å¤‡ç”µæ¢¯'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['æˆ¿æ•°Ã—ç”µæ¢¯Ã—çŽ¯çº¿'] = df['æ€»æˆ¿é—´æ•°'] * df['é…å¤‡ç”µæ¢¯'] * df['çŽ¯çº¿æ•°å€¼']\n\n# ========== 4. åŽ»é™¤æžç«¯å€¼ ==========\nq1 = train_df['ä»·æ ¼'].quantile(0.01)\nq99 = train_df['ä»·æ ¼'].quantile(0.99)\ntrain_df = train_df[(train_df['ä»·æ ¼'] >= q1) & (train_df['ä»·æ ¼'] <= q99)].copy()\ndef clip_outliers(df, column, lower_quantile=0.01, upper_quantile=0.99):\n    lower = df[column].quantile(lower_quantile)\n    upper = df[column].quantile(upper_quantile)\n    df[column] = df[column].clip(lower, upper)\n    return df\n\nfor col in ['æœ‰æ•ˆé¢ç§¯', 'æ€»æ¥¼å±‚', 'èšç±»ä¼°ä»·']:\n    train_df = clip_outliers(train_df, col)\n\n\nfrom sklearn.model_selection import cross_val_score, KFold\n\n# ========== 5. è®­ç»ƒæ¨¡åž‹ ==========\nfeatures = [\n    'åŸŽå¸‚ç¼–ç ','åŒºåŸŸç¼–ç ','æˆ¿å±‹å¹´é™ç­‰çº§','æ˜¯å¦æ»¡äº”','æœå‘_å«å—','æˆ¿å±‹ç”¨é€”å‡ä»·',\n    'å®¤','åŽ…','åŽ¨','å«','æ€»æ¥¼å±‚','æœ‰æ•ˆé¢ç§¯','é…å¤‡ç”µæ¢¯',\n    'çŽ¯çº¿æ•°å€¼','lon','åŸŽå¸‚ç¼–ç ','åŸŽå¸‚å¹´é™äº¤äº’',\n    'ç²¾è£…é¢ç§¯', 'åŸŽå¸‚', 'èšç±»å‡å•ä»·', 'çŽ¯çº¿é¢ç§¯',\n    'äº§æƒ_éžå…±æœ‰_é¢ç§¯', 'äº§æƒ_å…±æœ‰_é¢ç§¯', 'æ€»æˆ¿é—´æ•°', 'æˆ¿é—´Ã—èšç±»å‡å•ä»·',\n    'ç”µæ¢¯Ã—é¢ç§¯', 'ç²¾è£…Ã—é¢ç§¯Ã—å‡ä»·', 'éžå…±æœ‰Ã—ç”µæ¢¯Ã—é¢ç§¯', 'æˆ¿æ•°Ã—ç”µæ¢¯Ã—çŽ¯çº¿'\n]\n\n\nX = train_df[all_features].dropna()\né¢ç§¯_series = train_df.loc[X.index, 'æœ‰æ•ˆé¢ç§¯']\ny = train_df.loc[X.index, 'ä»·æ ¼'] / é¢ç§¯_series  # å•ä½æˆ¿ä»·ä½œä¸ºç›®æ ‡\n\nprint(f\"âœ… å‰”é™¤å¼‚å¸¸å€¼åŽç”¨äºŽè®­ç»ƒçš„æ•°æ®æ•°é‡: {len(X)}\")\n\nX_train, X_val, y_train, y_val, area_train, area_val = train_test_split(\n    X, y, é¢ç§¯_series, test_size=0.2, random_state=42\n)\n\nfrom sklearn.preprocessing import MinMaxScaler  # æ›¿æ¢å¯¼å…¥\n\n# ç”Ÿæˆæµ‹è¯•é›†ç‰¹å¾\ntest_X = test_df[all_features].copy()\n\n# ä½¿ç”¨æžå·®æ ‡å‡†åŒ–ï¼ˆMinMaxScalerï¼‰\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_scaled = scaler.fit_transform(X)\ntest_X_scaled = scaler.transform(test_X)\n\n# æ¨¡åž‹è®­ç»ƒ\nmodel = LinearRegression()\nmodel.fit(X_train_scaled, y_train)\n\n# ========== 6. è¯„ä¼° ==========\n# éªŒè¯é›†é¢„æµ‹\ny_val_pred_unit = model.predict(X_val_scaled)\ny_val_pred_price = y_val_pred_unit * area_val\ny_val_true_price = y_val * area_val\n\nrmse_val = mean_squared_error(y_val_true_price, y_val_pred_price, squared=False)\nmae_val = mean_absolute_error(y_val_true_price, y_val_pred_price)\nprint(f\"âœ… éªŒè¯é›† RMSEï¼ˆè¿˜åŽŸåŽï¼‰: {rmse_val:,.2f}\")\nprint(f\"âœ… éªŒè¯é›† MAE ï¼ˆè¿˜åŽŸåŽï¼‰: {mae_val:,.2f}\")\n\n# è®­ç»ƒé›†é¢„æµ‹\ny_train_pred_unit = model.predict(X_train_scaled)\ny_train_pred_price = y_train_pred_unit * area_train\ny_train_true_price = y_train * area_train\n\nrmse_train = mean_squared_error(y_train_true_price, y_train_pred_price, squared=False)\nmae_train = mean_absolute_error(y_train_true_price, y_train_pred_price)\nprint(f\"âœ… è®­ç»ƒé›† RMSEï¼ˆè¿˜åŽŸåŽï¼‰: {rmse_train:,.2f}\")\nprint(f\"âœ… è®­ç»ƒé›† MAE ï¼ˆè¿˜åŽŸåŽï¼‰: {mae_train:,.2f}\")\n\n# ========== 7. 6-Fold äº¤å‰éªŒè¯ï¼ˆRMSE & MAEï¼Œè¿˜åŽŸåŽï¼‰ ==========\ndef cross_val_rmse_mae(X, y_unit, area_series, n_splits=6):\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    rmse_list, mae_list = [], []\n\n    for i, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]\n        y_train_cv, y_val_cv = y_unit.iloc[train_idx], y_unit.iloc[val_idx]\n        area_train_cv, area_val_cv = area_series.iloc[train_idx], area_series.iloc[val_idx]\n\n        scaler_cv = MinMaxScaler()\n        X_train_scaled = scaler_cv.fit_transform(X_train_cv)\n        X_val_scaled = scaler_cv.transform(X_val_cv)\n\n        model_cv = LinearRegression()\n        model_cv.fit(X_train_scaled, y_train_cv)\n        y_pred_unit = model_cv.predict(X_val_scaled)\n\n        # è¿˜åŽŸä¸ºæ€»ä»·\n        y_pred_price = y_pred_unit * area_val_cv\n        y_true_price = y_val_cv * area_val_cv\n\n        rmse = mean_squared_error(y_true_price, y_pred_price, squared=False)\n        mae = mean_absolute_error(y_true_price, y_pred_price)\n\n        print(f\"Fold {i}: RMSE = {rmse:,.2f}, MAE = {mae:,.2f}\")\n        rmse_list.append(rmse)\n        mae_list.append(mae)\n\n    print(f\"\\nâœ… 6æŠ˜CV å¹³å‡ RMSEï¼ˆè¿˜åŽŸä»·ï¼‰: {np.mean(rmse_list):,.2f}\")\n    print(f\"âœ… 6æŠ˜CV å¹³å‡ MAE ï¼ˆè¿˜åŽŸä»·ï¼‰: {np.mean(mae_list):,.2f}\")\n\n# è°ƒç”¨å‡½æ•°\ncross_val_rmse_mae(X, y, é¢ç§¯_series)\n\n# ========== 8. å›žå½’æ–¹ç¨‹ ==========\nprint(\"âœ… å›žå½’æ–¹ç¨‹:\")\nprint(\"Intercept:\", model.intercept_)\nfor name, coef in zip(X.columns, model.coef_):\n    print(f\"{name}: {coef:.4f}\")\n\n# ========== 9. é¢„æµ‹æµ‹è¯•é›† ==========\ntest_X = test_df[all_features].copy()\ntest_X_scaled = scaler.transform(test_X)\ntest_unit_price_pred = model.predict(test_X_scaled)\ntest_area = test_df['å»ºç­‘é¢ç§¯']\ntest_price_pred = test_unit_price_pred * test_area\n\n# ========== 10. ç”Ÿæˆæäº¤æ–‡ä»¶ ==========\nsubmission = test_df[['ID']].copy()\nsubmission['Price'] = np.round(test_price_pred).astype(int)\nsubmission.to_csv('submission.csv', index=False)\nprint(\"âœ… é¢„æµ‹ç»“æžœå·²ä¿å­˜ä¸º submission.csv\")\n# ========== 11. åˆ†æžå•ä½æˆ¿ä»·ä¸Žè‡ªå˜é‡çš„ç›¸å…³æ€§ + å…±çº¿æ€§ ==========\n\n# é‡æ–°è®¡ç®—å•ä½æˆ¿ä»·\ntrain_df['å•ä½æˆ¿ä»·'] = train_df['ä»·æ ¼'] / train_df['æœ‰æ•ˆé¢ç§¯']\n\n# å¯ç”¨ç‰¹å¾ = å½“å‰ç”¨äºŽè®­ç»ƒçš„ all_features\nfeatures_to_analyze = all_features.copy()\n\n# åªä¿ç•™åœ¨ train_df ä¸­å­˜åœ¨ä¸”ä¸ºæ•°å€¼åž‹çš„åˆ—\nfeatures_to_analyze = [f for f in features_to_analyze if f in train_df.columns and np.issubdtype(train_df[f].dtype, np.number)]\n\n# æ·»åŠ å•ä½æˆ¿ä»·ç”¨äºŽç›¸å…³æ€§åˆ†æž\ncorrelation_df = train_df[features_to_analyze + ['å•ä½æˆ¿ä»·']].copy().dropna()\n\n# è®¡ç®—å•ä½æˆ¿ä»·ä¸Žæ¯ä¸ªå˜é‡çš„çš®å°”é€Šç›¸å…³ç³»æ•°\ncorrelations = correlation_df.corr()['å•ä½æˆ¿ä»·'].drop('å•ä½æˆ¿ä»·').sort_values(ascending=False)\n\nprint(\"\\nðŸ“Š ä¸Žå•ä½æˆ¿ä»·çš„çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆå‰20ä¸ªï¼‰:\")\nprint(correlations.head(20))\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport pandas as pd\n\n\nX_vif = train_df[all_features].dropna().copy()\n\n# æœ‰äº›æ¨¡åž‹ï¼ˆå¦‚ LinearRegressionï¼‰éœ€è¦å‰”é™¤éžæ•°å€¼åˆ—ï¼ˆå¦‚ 'åŸŽå¸‚' ï¼‰\nX_vif = X_vif.select_dtypes(include=[np.number])\n\n# è®¡ç®— VIF\nvif_df = pd.DataFrame()\nvif_df[\"Feature\"] = X_vif.columns\nvif_df[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n\n# æŽ’åºè¾“å‡º\nvif_df = vif_df.sort_values(by=\"VIF\", ascending=False)\n\n# æ˜¾ç¤ºé«˜ VIF ç‰¹å¾ï¼ˆ> 10 çš„å¸¸è¢«è§†ä¸ºé«˜åº¦å…±çº¿ï¼‰\nprint(\"\\nðŸ“Š VIF æ£€æŸ¥ç»“æžœï¼ˆå‰å‡ é¡¹ï¼‰:\")\nprint(vif_df.head(15))\n\nhigh_vif = vif_df[vif_df[\"VIF\"] > 10]\nif not high_vif.empty:\n    print(\"\\nâš ï¸ ä»¥ä¸‹å˜é‡å­˜åœ¨é«˜å…±çº¿æ€§ï¼ˆVIF > 10ï¼‰:\")\n    print(high_vif)\nelse:\n    print(\"âœ… æœªå‘çŽ° VIF > 10 çš„å˜é‡ï¼Œå…±çº¿æ€§å¤„äºŽå¯æŽ¥å—èŒƒå›´\")","outputs":[{"output_type":"stream","name":"stdout","text":"âœ… å‰”é™¤å¼‚å¸¸å€¼åŽç”¨äºŽè®­ç»ƒçš„æ•°æ®æ•°é‡: 82386\nâœ… éªŒè¯é›† RMSEï¼ˆè¿˜åŽŸåŽï¼‰: 1,016,894.67\nâœ… éªŒè¯é›† MAE ï¼ˆè¿˜åŽŸåŽï¼‰: 574,432.51\nâœ… è®­ç»ƒé›† RMSEï¼ˆè¿˜åŽŸåŽï¼‰: 1,053,513.27\nâœ… è®­ç»ƒé›† MAE ï¼ˆè¿˜åŽŸåŽï¼‰: 592,190.03\nFold 1: RMSE = 1,013,263.62, MAE = 572,010.91\nFold 2: RMSE = 1,064,575.10, MAE = 590,765.82\nFold 3: RMSE = 1,046,965.05, MAE = 586,461.15\nFold 4: RMSE = 1,019,420.36, MAE = 584,046.09\nFold 5: RMSE = 1,060,740.82, MAE = 597,158.51\nFold 6: RMSE = 1,071,619.50, MAE = 602,055.35\n\nâœ… 6æŠ˜CV å¹³å‡ RMSEï¼ˆè¿˜åŽŸä»·ï¼‰: 1,046,097.41\nâœ… 6æŠ˜CV å¹³å‡ MAE ï¼ˆè¿˜åŽŸä»·ï¼‰: 588,749.64\nâœ… å›žå½’æ–¹ç¨‹:\nIntercept: -8436.147789056227\nåŸŽå¸‚ç¼–ç : 2906.9185\nåŒºåŸŸç¼–ç : 4802.8151\næˆ¿å±‹å¹´é™ç­‰çº§: 2431.4137\næ˜¯å¦æ»¡äº”: 351.0997\nåŽ…: -11911.9841\nå½“å‰æ¥¼å±‚: 0.0000\næ€»æ¥¼å±‚: 5138.1640\næœ‰æ•ˆé¢ç§¯: -15333.1903\nlon: 1635.6306\nlat: 919.3524\nåŸŽå¸‚å¹´é™äº¤äº’: -5541.2640\nèšç±»å‡å•ä»·: 62529.0886\næ€»æˆ¿é—´æ•°: 80968.7581\næˆ¿é—´Ã—èšç±»å‡å•ä»·: -34085.2304\nç²¾è£…Ã—é¢ç§¯Ã—å‡ä»·: 16354.3818\nâœ… é¢„æµ‹ç»“æžœå·²ä¿å­˜ä¸º submission.csv\n\nðŸ“Š ä¸Žå•ä½æˆ¿ä»·çš„çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆå‰20ä¸ªï¼‰:\nèšç±»å‡å•ä»·       0.830390\næˆ¿é—´Ã—èšç±»å‡å•ä»·    0.734685\næˆ¿å±‹å¹´é™ç­‰çº§      0.436869\nç²¾è£…Ã—é¢ç§¯Ã—å‡ä»·    0.406465\næ˜¯å¦æ»¡äº”        0.386126\nlon         0.213123\nlat         0.190246\nåŒºåŸŸç¼–ç        -0.087104\næœ‰æ•ˆé¢ç§¯       -0.110837\næ€»æˆ¿é—´æ•°       -0.119394\nåŽ…          -0.147350\næ€»æ¥¼å±‚        -0.175401\nåŸŽå¸‚å¹´é™äº¤äº’     -0.322644\nåŸŽå¸‚ç¼–ç        -0.495057\nå½“å‰æ¥¼å±‚             NaN\nName: å•ä½æˆ¿ä»·, dtype: float64\n\nðŸ“Š VIF æ£€æŸ¥ç»“æžœï¼ˆå‰å‡ é¡¹ï¼‰:\n     Feature         VIF\n5       å½“å‰æ¥¼å±‚  687.503733\n0       åŸŽå¸‚ç¼–ç    13.051472\n10    åŸŽå¸‚å¹´é™äº¤äº’   12.480405\n11     èšç±»å‡å•ä»·   12.358467\n13  æˆ¿é—´Ã—èšç±»å‡å•ä»·   12.122451\n2     æˆ¿å±‹å¹´é™ç­‰çº§   11.474725\n12      æ€»æˆ¿é—´æ•°    7.217499\n3       æ˜¯å¦æ»¡äº”    4.883184\n9        lat    4.076695\n8        lon    3.506205\n7       æœ‰æ•ˆé¢ç§¯    2.906048\n4          åŽ…    2.783771\n14  ç²¾è£…Ã—é¢ç§¯Ã—å‡ä»·    1.478662\n6        æ€»æ¥¼å±‚    1.271205\n1       åŒºåŸŸç¼–ç     1.173938\n\nâš ï¸ ä»¥ä¸‹å˜é‡å­˜åœ¨é«˜å…±çº¿æ€§ï¼ˆVIF > 10ï¼‰:\n     Feature         VIF\n5       å½“å‰æ¥¼å±‚  687.503733\n0       åŸŽå¸‚ç¼–ç    13.051472\n10    åŸŽå¸‚å¹´é™äº¤äº’   12.480405\n11     èšç±»å‡å•ä»·   12.358467\n13  æˆ¿é—´Ã—èšç±»å‡å•ä»·   12.122451\n2     æˆ¿å±‹å¹´é™ç­‰çº§   11.474725\n"}],"execution_count":8},{"cell_type":"markdown","metadata":{"id":"309B012A84FC4EFF8851D90429A86677","notebookId":"67ebe8c65302e998c320c3fd","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 2.LASSO"},{"cell_type":"code","metadata":{"id":"6CE828C3AF454711A6CF168B49339160","notebookId":"67ebe8c65302e998c320c3fd","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"#lasso\nimport pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import cross_val_score, KFold\n# ========== 1. è¯»å–æ•°æ® ==========\ntrain_df = pd.read_csv('/home/mw/input/quant4533/ruc_Class25Q1_train.csv')\ntest_df = pd.read_csv('/home/mw/input/quant4533/ruc_Class25Q1_test.csv')\noriginal_test = test_df[['ID']].copy()\n\n# ========== 2. ç‰¹å¾å·¥ç¨‹å‡½æ•° ==========\ndef clean_area(df):\n    for col in ['å»ºç­‘é¢ç§¯', 'å¥—å†…é¢ç§¯']:\n        df[col] = df[col].astype(str).str.replace('ãŽ¡', '', regex=False)\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n    df['æœ‰æ•ˆé¢ç§¯'] = df['å¥—å†…é¢ç§¯'].fillna(df['å»ºç­‘é¢ç§¯'])\n    df['æœ‰æ•ˆé¢ç§¯'] = df['æœ‰æ•ˆé¢ç§¯'].fillna(df['æœ‰æ•ˆé¢ç§¯'].median())\n    return df\n# é‡æ–°è®¡ç®—æˆ¿å±‹ç”¨é€”å‡ä»·æ˜ å°„ï¼ˆç¡®ä¿å·²ç”Ÿæˆï¼‰\nç”¨é€”å‡ä»· = train_df.groupby('æˆ¿å±‹ç”¨é€”')['ä»·æ ¼'].mean()\ntrain_df['æˆ¿å±‹ç”¨é€”å‡ä»·'] = train_df['æˆ¿å±‹ç”¨é€”'].map(ç”¨é€”å‡ä»·).fillna(train_df['ä»·æ ¼'].mean())\n\n# ç¡®ä¿åŸŽå¸‚ç¼–ç ä¸Žæˆ¿å±‹å¹´é™ç­‰çº§å­—æ®µå·²å­˜åœ¨\nif 'åŸŽå¸‚ç¼–ç ' not in train_df.columns:\n    train_df['åŸŽå¸‚ç¼–ç '] = train_df['åŸŽå¸‚'].astype('category').cat.codes\nif 'æˆ¿å±‹å¹´é™ç­‰çº§' not in train_df.columns:\n    train_df['æˆ¿å±‹å¹´é™ç­‰çº§'] = train_df['æˆ¿å±‹å¹´é™'].map(lambda x: 3 if 'æ»¡äº”' in str(x) else (2 if 'æ»¡ä¸¤' in str(x) else 1))\n\n# åˆ›å»ºåŸŽå¸‚å¹´é™äº¤äº’é¡¹\ntrain_df['åŸŽå¸‚å¹´é™äº¤äº’'] = train_df['åŸŽå¸‚ç¼–ç '] * train_df['æˆ¿å±‹å¹´é™ç­‰çº§']\ndef extract_floor(df):\n    df['å½“å‰æ¥¼å±‚_raw'] = df['æ‰€åœ¨æ¥¼å±‚'].str.extract(r'ç¬¬?(\\d+)[å±‚æ¥¼]')[0].astype(float)\n    df['æ€»æ¥¼å±‚'] = df['æ‰€åœ¨æ¥¼å±‚'].str.extract(r'å…±?(\\d+)[å±‚æ¥¼]')[0].astype(float)\n    df.loc[df['å½“å‰æ¥¼å±‚_raw'].isna(), 'å½“å‰æ¥¼å±‚_raw'] = df['æ‰€åœ¨æ¥¼å±‚'].str.extract(r'(åº•|ä½Ž|ä¸­|é«˜|é¡¶)')[0].map({\n        'åº•': 1, 'ä½Ž': 2, 'ä¸­': 3, 'é«˜': 4, 'é¡¶': 5\n    })\n    df['å½“å‰æ¥¼å±‚_raw'] = df['å½“å‰æ¥¼å±‚_raw'].fillna(1)\n    df['æ€»æ¥¼å±‚'] = df['æ€»æ¥¼å±‚'].fillna(df['æ€»æ¥¼å±‚'].median())\n    df['å½“å‰æ¥¼å±‚'] = (df['å½“å‰æ¥¼å±‚_raw'] / df['æ€»æ¥¼å±‚']).clip(0, 1)\n    df.drop(columns=['å½“å‰æ¥¼å±‚_raw'], inplace=True)\n    return df\n\ndef extract_layout(df):\n    df['å®¤'] = df['æˆ¿å±‹æˆ·åž‹'].str.extract(r'(\\d+)å®¤').fillna(0).astype(int)\n    df['åŽ…'] = df['æˆ¿å±‹æˆ·åž‹'].str.extract(r'(\\d+)åŽ…').fillna(0).astype(int)\n    df['åŽ¨'] = df['æˆ¿å±‹æˆ·åž‹'].str.extract(r'(\\d+)åŽ¨').fillna(0).astype(int)\n    df['å«'] = df['æˆ¿å±‹æˆ·åž‹'].str.extract(r'(\\d+)å«').fillna(0).astype(int)\n    df['æ€»æˆ¿é—´æ•°'] = df[['å®¤', 'åŽ…', 'åŽ¨', 'å«']].sum(axis=1)\n    return df\n\ndef enrich_features(df):\n    df['æ˜¯å¦æ»¡äº”'] = df['æˆ¿å±‹å¹´é™'].apply(lambda x: 1 if 'æ»¡äº”' in str(x) else 0)\n    df['æœå‘_å«å—'] = df['æˆ¿å±‹æœå‘'].apply(lambda x: 1 if 'å—' in str(x) else 0)\n    df['é…å¤‡ç”µæ¢¯'] = df['é…å¤‡ç”µæ¢¯'].map({'æœ‰': 1, 'æ— ': 0}).fillna(0)\n    df['åŸŽå¸‚ç¼–ç '] = df['åŸŽå¸‚'].astype('category').cat.codes\n    df['åŒºåŸŸç¼–ç '] = df['åŒºåŸŸ'].astype('category').cat.codes\n    df['æˆ¿å±‹å¹´é™ç­‰çº§'] = df['æˆ¿å±‹å¹´é™'].map(lambda x: 3 if 'æ»¡äº”' in str(x) else (2 if 'æ»¡ä¸¤' in str(x) else 1))\n    ring_map = {'ä¸€çŽ¯å†…': 1.0, 'ä¸€è‡³äºŒçŽ¯': 1.5, 'äºŒçŽ¯å†…': 2.0, 'äºŒè‡³ä¸‰çŽ¯': 2.5,\n                'ä¸‰è‡³å››çŽ¯': 3.5, 'å››è‡³äº”çŽ¯': 4.5, 'ä¸‰çŽ¯å¤–': 5.0, 'äº”è‡³å…­çŽ¯': 5.5,\n                'å››çŽ¯å¤–': 6.0, 'å…­çŽ¯å¤–': 6.5, 'å†…çŽ¯å†…': 1.0, 'å†…çŽ¯è‡³ä¸­çŽ¯': 1.8,\n                'ä¸­çŽ¯è‡³å¤–çŽ¯': 4.2, 'å†…çŽ¯è‡³å¤–çŽ¯': 3.0, 'å¤–çŽ¯å¤–': 7.0}\n    df['çŽ¯çº¿æ•°å€¼'] = df['çŽ¯çº¿'].map(ring_map).fillna(4.0)\n    df['çŽ¯çº¿é¢ç§¯'] = df['çŽ¯çº¿æ•°å€¼'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['æ˜¯å¦ç²¾è£…'] = (df['è£…ä¿®æƒ…å†µ'].fillna('å…¶ä»–') == 'ç²¾è£…').astype(int)\n    df['ç²¾è£…é¢ç§¯'] = df['æ˜¯å¦ç²¾è£…'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['äº§æƒ_éžå…±æœ‰'] = (df['äº§æƒæ‰€å±ž'].fillna('å…¶ä»–') == 'éžå…±æœ‰').astype(int)\n    df['äº§æƒ_å…±æœ‰'] = (df['äº§æƒæ‰€å±ž'].fillna('å…¶ä»–') == 'å…±æœ‰').astype(int)\n    df['äº§æƒ_éžå…±æœ‰_é¢ç§¯'] = df['äº§æƒ_éžå…±æœ‰'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['äº§æƒ_å…±æœ‰_é¢ç§¯'] = df['äº§æƒ_å…±æœ‰'] * df['æœ‰æ•ˆé¢ç§¯']\n    return df\n\ndef cluster_price(df, ref_df=None):\n    df['lon'] = pd.to_numeric(df['lon'], errors='coerce') + np.random.normal(0, 0.01, size=len(df))\n    df['lat'] = pd.to_numeric(df['lat'], errors='coerce') + np.random.normal(0, 0.01, size=len(df))\n    if ref_df is None:\n        kmeans = KMeans(n_clusters=10, random_state=42)\n        df['èšç±»æ ‡ç­¾'] = kmeans.fit_predict(df[['lon', 'lat']])\n        df['å•ä»·'] = df['ä»·æ ¼'] / df['æœ‰æ•ˆé¢ç§¯']\n        df['èšç±»å‡å•ä»·'] = df.groupby('èšç±»æ ‡ç­¾')['å•ä»·'].transform('mean')\n        df['èšç±»ä¼°ä»·'] = df['èšç±»å‡å•ä»·'] * df['æœ‰æ•ˆé¢ç§¯']\n        return df, kmeans, df.groupby('èšç±»æ ‡ç­¾')['å•ä»·'].mean()\n    else:\n        df['èšç±»æ ‡ç­¾'] = ref_df[0].predict(df[['lon', 'lat']])\n        df['èšç±»å‡å•ä»·'] = df['èšç±»æ ‡ç­¾'].map(ref_df[1])\n        df['èšç±»ä¼°ä»·'] = df['èšç±»å‡å•ä»·'] * df['æœ‰æ•ˆé¢ç§¯']\n        return df\n\n# ========== 3. æ¸…æ´—ä¸Žç‰¹å¾å·¥ç¨‹ ==========\nfor df in [train_df, test_df]:\n    df = clean_area(df)\n    df = extract_floor(df)\n    df = extract_layout(df)\n    df = enrich_features(df)\ntest_df['æˆ¿å±‹ç”¨é€”å‡ä»·'] = test_df['æˆ¿å±‹ç”¨é€”'].map(ç”¨é€”å‡ä»·).fillna(train_df['ä»·æ ¼'].mean())\ntest_df['åŸŽå¸‚å¹´é™äº¤äº’'] = test_df['åŸŽå¸‚ç¼–ç '] * test_df['æˆ¿å±‹å¹´é™ç­‰çº§']\ntrain_df, kmeans_model, price_map = cluster_price(train_df)\ntest_df = cluster_price(test_df, ref_df=(kmeans_model, price_map))\n\nfor df in [train_df, test_df]:\n    df['æˆ¿é—´Ã—èšç±»å‡å•ä»·'] = df['æ€»æˆ¿é—´æ•°'] * df['èšç±»å‡å•ä»·']\n    df['ç”µæ¢¯Ã—é¢ç§¯'] = df['é…å¤‡ç”µæ¢¯'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['ç²¾è£…Ã—é¢ç§¯Ã—å‡ä»·'] = df['æ˜¯å¦ç²¾è£…'] * df['æœ‰æ•ˆé¢ç§¯'] * df['èšç±»å‡å•ä»·']\n    df['éžå…±æœ‰Ã—ç”µæ¢¯Ã—é¢ç§¯'] = df['äº§æƒ_éžå…±æœ‰'] * df['é…å¤‡ç”µæ¢¯'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['æˆ¿æ•°Ã—ç”µæ¢¯Ã—çŽ¯çº¿'] = df['æ€»æˆ¿é—´æ•°'] * df['é…å¤‡ç”µæ¢¯'] * df['çŽ¯çº¿æ•°å€¼']\n\n# ========== 4. åŽ»é™¤æžç«¯å€¼ ==========\nq1 = train_df['ä»·æ ¼'].quantile(0.01)\nq99 = train_df['ä»·æ ¼'].quantile(0.99)\ntrain_df = train_df[(train_df['ä»·æ ¼'] >= q1) & (train_df['ä»·æ ¼'] <= q99)].copy()\ndef clip_outliers(df, column, lower_quantile=0.01, upper_quantile=0.99):\n    lower = df[column].quantile(lower_quantile)\n    upper = df[column].quantile(upper_quantile)\n    df[column] = df[column].clip(lower, upper)\n    return df\n\nfor col in ['æœ‰æ•ˆé¢ç§¯', 'æ€»æ¥¼å±‚', 'èšç±»ä¼°ä»·']:\n    train_df = clip_outliers(train_df, col)\n\n\n\n\n# ========== 5. è®­ç»ƒæ¨¡åž‹ ==========\n\nfrom sklearn.linear_model import Lasso  # ä½¿ç”¨Lasso\n\nall_features = [\n    'åŸŽå¸‚ç¼–ç ','åŒºåŸŸç¼–ç ','æˆ¿å±‹å¹´é™ç­‰çº§','æ˜¯å¦æ»¡äº”','æœå‘_å«å—','æˆ¿å±‹ç”¨é€”å‡ä»·',\n    'å®¤','åŽ…','åŽ¨','å«','æ€»æ¥¼å±‚','æœ‰æ•ˆé¢ç§¯','é…å¤‡ç”µæ¢¯',\n    'çŽ¯çº¿æ•°å€¼','lon','lat','åŸŽå¸‚ç¼–ç ','åŸŽå¸‚å¹´é™äº¤äº’',\n    'ç²¾è£…é¢ç§¯', 'åŸŽå¸‚', 'èšç±»å‡å•ä»·', 'çŽ¯çº¿é¢ç§¯',\n    'äº§æƒ_éžå…±æœ‰_é¢ç§¯', 'äº§æƒ_å…±æœ‰_é¢ç§¯', 'æ€»æˆ¿é—´æ•°', 'æˆ¿é—´Ã—èšç±»å‡å•ä»·',\n    'ç”µæ¢¯Ã—é¢ç§¯', 'ç²¾è£…Ã—é¢ç§¯Ã—å‡ä»·', 'éžå…±æœ‰Ã—ç”µæ¢¯Ã—é¢ç§¯', 'æˆ¿æ•°Ã—ç”µæ¢¯Ã—çŽ¯çº¿'\n]\n\nX = train_df[all_features].dropna()\né¢ç§¯_series = train_df.loc[X.index, 'æœ‰æ•ˆé¢ç§¯']\ny = train_df.loc[X.index, 'ä»·æ ¼'] / é¢ç§¯_series  # å•ä½æˆ¿ä»·ä½œä¸ºç›®æ ‡\n\nprint(f\"âœ… å‰”é™¤å¼‚å¸¸å€¼åŽç”¨äºŽè®­ç»ƒçš„æ•°æ®æ•°é‡: {len(X)}\")\n\nX_train, X_val, y_train, y_val, area_train, area_val = train_test_split(\n    X, y, é¢ç§¯_series, test_size=0.2, random_state=111\n)\n\n# ç”Ÿæˆæµ‹è¯•é›†ç‰¹å¾\ntest_X = test_df[all_features].copy()\n\n# ä½¿ç”¨æžå·®æ ‡å‡†åŒ–ï¼ˆMinMaxScalerï¼‰\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_scaled = scaler.fit_transform(X)\ntest_X_scaled = scaler.transform(test_X)\n\n# æ¨¡åž‹è®­ç»ƒ\nmodel = Lasso(alpha=0.1)\nmodel.fit(X_train_scaled, y_train)\n\n# ========== 6. è¯„ä¼° ==========\ny_val_pred_unit = model.predict(X_val_scaled)\ny_val_pred_price = y_val_pred_unit * area_val\ny_val_true_price = y_val * area_val\n\nrmse_val = mean_squared_error(y_val_true_price, y_val_pred_price, squared=False)\nmae_val = mean_absolute_error(y_val_true_price, y_val_pred_price)\nprint(f\"âœ… éªŒè¯é›† RMSEï¼ˆè¿˜åŽŸåŽï¼‰: {rmse_val:,.2f}\")\nprint(f\"âœ… éªŒè¯é›† MAE ï¼ˆè¿˜åŽŸåŽï¼‰: {mae_val:,.2f}\")\n\ny_train_pred_unit = model.predict(X_train_scaled)\ny_train_pred_price = y_train_pred_unit * area_train\ny_train_true_price = y_train * area_train\n\nrmse_train = mean_squared_error(y_train_true_price, y_train_pred_price, squared=False)\nmae_train = mean_absolute_error(y_train_true_price, y_train_pred_price)\nprint(f\"âœ… è®­ç»ƒé›† RMSEï¼ˆè¿˜åŽŸåŽï¼‰: {rmse_train:,.2f}\")\nprint(f\"âœ… è®­ç»ƒé›† MAE ï¼ˆè¿˜åŽŸåŽï¼‰: {mae_train:,.2f}\")\n\n# ========== 7. 6-Fold äº¤å‰éªŒè¯ï¼ˆRMSE & MAEï¼Œè¿˜åŽŸåŽï¼‰ ==========\ndef cross_val_rmse_mae(X, y_unit, area_series, n_splits=6):\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    rmse_list, mae_list = [], []\n\n    for i, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]\n        y_train_cv, y_val_cv = y_unit.iloc[train_idx], y_unit.iloc[val_idx]\n        area_train_cv, area_val_cv = area_series.iloc[train_idx], area_series.iloc[val_idx]\n\n        scaler_cv = MinMaxScaler()\n        X_train_scaled = scaler_cv.fit_transform(X_train_cv)\n        X_val_scaled = scaler_cv.transform(X_val_cv)\n\n        model_cv = Lasso(alpha=1)\n        model_cv.fit(X_train_scaled, y_train_cv)\n        y_pred_unit = model_cv.predict(X_val_scaled)\n\n        y_pred_price = y_pred_unit * area_val_cv\n        y_true_price = y_val_cv * area_val_cv\n\n        rmse = mean_squared_error(y_true_price, y_pred_price, squared=False)\n        mae = mean_absolute_error(y_true_price, y_pred_price)\n\n        print(f\"Fold {i}: RMSE = {rmse:,.2f}, MAE = {mae:,.2f}\")\n        rmse_list.append(rmse)\n        mae_list.append(mae)\n\n    print(f\"\\nâœ… 6æŠ˜CV å¹³å‡ RMSEï¼ˆè¿˜åŽŸä»·ï¼‰: {np.mean(rmse_list):,.2f}\")\n    print(f\"âœ… 6æŠ˜CV å¹³å‡ MAE ï¼ˆè¿˜åŽŸä»·ï¼‰: {np.mean(mae_list):,.2f}\")\n\ncross_val_rmse_mae(X, y, é¢ç§¯_series)\n\n# ========== 8. å›žå½’æ–¹ç¨‹ ==========\nprint(\"âœ… å›žå½’æ–¹ç¨‹:\")\nprint(\"Intercept:\", model.intercept_)\nfor name, coef in zip(X.columns, model.coef_):\n    print(f\"{name}: {coef:.4f}\")\n\n# ========== 9. é¢„æµ‹æµ‹è¯•é›† ==========\ntest_unit_price_pred = model.predict(test_X_scaled)\ntest_area = test_df['æœ‰æ•ˆé¢ç§¯']\ntest_price_pred = test_unit_price_pred * test_area\n\n# ========== 10. ç”Ÿæˆæäº¤æ–‡ä»¶ ==========\nsubmission = test_df[['ID']].copy()\nsubmission['Price'] = np.round(test_price_pred).astype(int)\nsubmission.to_csv('submission.csv', index=False)\nprint(\"âœ… é¢„æµ‹ç»“æžœå·²ä¿å­˜ä¸º submission.csv\")","outputs":[{"output_type":"stream","name":"stdout","text":"âœ… å‰”é™¤å¼‚å¸¸å€¼åŽç”¨äºŽè®­ç»ƒçš„æ•°æ®æ•°é‡: 82459\n"},{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.186e+12, tolerance: 2.887e+09\n  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"},{"output_type":"stream","name":"stdout","text":"âœ… éªŒè¯é›† RMSEï¼ˆè¿˜åŽŸåŽï¼‰: 990,639.54\nâœ… éªŒè¯é›† MAE ï¼ˆè¿˜åŽŸåŽï¼‰: 581,383.28\nâœ… è®­ç»ƒé›† RMSEï¼ˆè¿˜åŽŸåŽï¼‰: 994,865.18\nâœ… è®­ç»ƒé›† MAE ï¼ˆè¿˜åŽŸåŽï¼‰: 589,683.78\n"},{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.061e+09, tolerance: 3.025e+09\n  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"},{"output_type":"stream","name":"stdout","text":"Fold 1: RMSE = 1,001,941.78, MAE = 590,340.06\n"},{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.039e+09, tolerance: 3.029e+09\n  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"},{"output_type":"stream","name":"stdout","text":"Fold 2: RMSE = 1,019,331.93, MAE = 592,284.02\nFold 3: RMSE = 994,168.83, MAE = 582,061.15\nFold 4: RMSE = 1,027,850.71, MAE = 594,914.60\n"},{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.150e+09, tolerance: 2.994e+09\n  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"},{"output_type":"stream","name":"stdout","text":"Fold 5: RMSE = 991,092.79, MAE = 588,495.74\n"},{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.016e+09, tolerance: 3.008e+09\n  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"},{"output_type":"stream","name":"stdout","text":"Fold 6: RMSE = 1,015,009.22, MAE = 601,801.70\n\nâœ… 6æŠ˜CV å¹³å‡ RMSEï¼ˆè¿˜åŽŸä»·ï¼‰: 1,008,232.54\nâœ… 6æŠ˜CV å¹³å‡ MAE ï¼ˆè¿˜åŽŸä»·ï¼‰: 591,649.54\nâœ… å›žå½’æ–¹ç¨‹:\nIntercept: -2586.77929633213\nåŸŽå¸‚ç¼–ç : 199.6758\nåŒºåŸŸç¼–ç : 5721.7846\næˆ¿å±‹å¹´é™ç­‰çº§: 4489.0617\næ˜¯å¦æ»¡äº”: 101.2579\næœå‘_å«å—: 56.5939\næˆ¿å±‹ç”¨é€”å‡ä»·: 14643.7825\nå®¤: -4102.4708\nåŽ…: -7251.9568\nåŽ¨: 22093.5688\nå«: 12522.8733\næ€»æ¥¼å±‚: 1196.1769\næœ‰æ•ˆé¢ç§¯: -17320.7768\né…å¤‡ç”µæ¢¯: -1592.0934\nçŽ¯çº¿æ•°å€¼: -22169.7842\nlon: 2582.1745\nlat: 2452.4973\nåŸŽå¸‚ç¼–ç : 0.0000\nåŸŽå¸‚å¹´é™äº¤äº’: -9561.2792\nç²¾è£…é¢ç§¯: 4396.6165\nåŸŽå¸‚: 4059.1120\nèšç±»å‡å•ä»·: 60579.3215\nçŽ¯çº¿é¢ç§¯: -192227.4407\näº§æƒ_éžå…±æœ‰_é¢ç§¯: 636513.0680\näº§æƒ_å…±æœ‰_é¢ç§¯: 44801.5532\næ€»æˆ¿é—´æ•°: 34510.0876\næˆ¿é—´Ã—èšç±»å‡å•ä»·: -22551.4101\nç”µæ¢¯Ã—é¢ç§¯: -350044.6444\nç²¾è£…Ã—é¢ç§¯Ã—å‡ä»·: 10386.3671\néžå…±æœ‰Ã—ç”µæ¢¯Ã—é¢ç§¯: -63821.6495\næˆ¿æ•°Ã—ç”µæ¢¯Ã—çŽ¯çº¿: 35135.6556\nâœ… é¢„æµ‹ç»“æžœå·²ä¿å­˜ä¸º submission.csv\n"}],"execution_count":9},{"cell_type":"markdown","metadata":{"id":"104724CB23284F4CAC713E935F401838","notebookId":"67ebe8c65302e998c320c3fd","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 3. RIDGE(æœ€ä½³ï¼‰"},{"cell_type":"code","metadata":{"id":"E614E208EBDB4BBA84749DAD63B0C740","notebookId":"67ebe8c65302e998c320c3fd","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"#ridge\n\nimport pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\n# ========== 1. è¯»å–æ•°æ® ==========\ntrain_df = pd.read_csv('/home/mw/input/quant4533/ruc_Class25Q1_train.csv')\ntest_df = pd.read_csv('/home/mw/input/quant4533/ruc_Class25Q1_test.csv')\noriginal_test = test_df[['ID']].copy()\n\n# ========== 2. ç‰¹å¾å·¥ç¨‹å‡½æ•° ==========\ndef clean_area(df):\n    for col in ['å»ºç­‘é¢ç§¯', 'å¥—å†…é¢ç§¯']:\n        df[col] = df[col].astype(str).str.replace('ãŽ¡', '', regex=False)\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n    df['æœ‰æ•ˆé¢ç§¯'] = df['å¥—å†…é¢ç§¯'].fillna(df['å»ºç­‘é¢ç§¯'])\n    df['æœ‰æ•ˆé¢ç§¯'] = df['æœ‰æ•ˆé¢ç§¯'].fillna(df['æœ‰æ•ˆé¢ç§¯'].median())\n    return df\n# é‡æ–°è®¡ç®—æˆ¿å±‹ç”¨é€”å‡ä»·æ˜ å°„ï¼ˆç¡®ä¿å·²ç”Ÿæˆï¼‰\nç”¨é€”å‡ä»· = train_df.groupby('æˆ¿å±‹ç”¨é€”')['ä»·æ ¼'].mean()\ntrain_df['æˆ¿å±‹ç”¨é€”å‡ä»·'] = train_df['æˆ¿å±‹ç”¨é€”'].map(ç”¨é€”å‡ä»·).fillna(train_df['ä»·æ ¼'].mean())\n\n# ç¡®ä¿åŸŽå¸‚ç¼–ç ä¸Žæˆ¿å±‹å¹´é™ç­‰çº§å­—æ®µå·²å­˜åœ¨\nif 'åŸŽå¸‚ç¼–ç ' not in train_df.columns:\n    train_df['åŸŽå¸‚ç¼–ç '] = train_df['åŸŽå¸‚'].astype('category').cat.codes\nif 'æˆ¿å±‹å¹´é™ç­‰çº§' not in train_df.columns:\n    train_df['æˆ¿å±‹å¹´é™ç­‰çº§'] = train_df['æˆ¿å±‹å¹´é™'].map(lambda x: 3 if 'æ»¡äº”' in str(x) else (2 if 'æ»¡ä¸¤' in str(x) else 1))\n\n# åˆ›å»ºåŸŽå¸‚å¹´é™äº¤äº’é¡¹\ntrain_df['åŸŽå¸‚å¹´é™äº¤äº’'] = train_df['åŸŽå¸‚ç¼–ç '] * train_df['æˆ¿å±‹å¹´é™ç­‰çº§']\ndef extract_floor(df):\n    df['å½“å‰æ¥¼å±‚_raw'] = df['æ‰€åœ¨æ¥¼å±‚'].str.extract(r'ç¬¬?(\\d+)[å±‚æ¥¼]')[0].astype(float)\n    df['æ€»æ¥¼å±‚'] = df['æ‰€åœ¨æ¥¼å±‚'].str.extract(r'å…±?(\\d+)[å±‚æ¥¼]')[0].astype(float)\n    df.loc[df['å½“å‰æ¥¼å±‚_raw'].isna(), 'å½“å‰æ¥¼å±‚_raw'] = df['æ‰€åœ¨æ¥¼å±‚'].str.extract(r'(åº•|ä½Ž|ä¸­|é«˜|é¡¶)')[0].map({\n        'åº•': 1, 'ä½Ž': 2, 'ä¸­': 3, 'é«˜': 4, 'é¡¶': 5\n    })\n    df['å½“å‰æ¥¼å±‚_raw'] = df['å½“å‰æ¥¼å±‚_raw'].fillna(1)\n    df['æ€»æ¥¼å±‚'] = df['æ€»æ¥¼å±‚'].fillna(df['æ€»æ¥¼å±‚'].median())\n    df['å½“å‰æ¥¼å±‚'] = (df['å½“å‰æ¥¼å±‚_raw'] / df['æ€»æ¥¼å±‚']).clip(0, 1)\n    df.drop(columns=['å½“å‰æ¥¼å±‚_raw'], inplace=True)\n    return df\n\ndef extract_layout(df):\n    df['å®¤'] = df['æˆ¿å±‹æˆ·åž‹'].str.extract(r'(\\d+)å®¤').fillna(0).astype(int)\n    df['åŽ…'] = df['æˆ¿å±‹æˆ·åž‹'].str.extract(r'(\\d+)åŽ…').fillna(0).astype(int)\n    df['åŽ¨'] = df['æˆ¿å±‹æˆ·åž‹'].str.extract(r'(\\d+)åŽ¨').fillna(0).astype(int)\n    df['å«'] = df['æˆ¿å±‹æˆ·åž‹'].str.extract(r'(\\d+)å«').fillna(0).astype(int)\n    df['æ€»æˆ¿é—´æ•°'] = df[['å®¤', 'åŽ…', 'åŽ¨', 'å«']].sum(axis=1)\n    return df\n\ndef enrich_features(df):\n    df['æ˜¯å¦æ»¡äº”'] = df['æˆ¿å±‹å¹´é™'].apply(lambda x: 1 if 'æ»¡äº”' in str(x) else 0)\n    df['æœå‘_å«å—'] = df['æˆ¿å±‹æœå‘'].apply(lambda x: 1 if 'å—' in str(x) else 0)\n    df['é…å¤‡ç”µæ¢¯'] = df['é…å¤‡ç”µæ¢¯'].map({'æœ‰': 1, 'æ— ': 0}).fillna(0)\n    df['åŸŽå¸‚ç¼–ç '] = df['åŸŽå¸‚'].astype('category').cat.codes\n    df['åŒºåŸŸç¼–ç '] = df['åŒºåŸŸ'].astype('category').cat.codes\n    df['æˆ¿å±‹å¹´é™ç­‰çº§'] = df['æˆ¿å±‹å¹´é™'].map(lambda x: 3 if 'æ»¡äº”' in str(x) else (2 if 'æ»¡ä¸¤' in str(x) else 1))\n    ring_map = {'ä¸€çŽ¯å†…': 1.0, 'ä¸€è‡³äºŒçŽ¯': 1.5, 'äºŒçŽ¯å†…': 2.0, 'äºŒè‡³ä¸‰çŽ¯': 2.5,\n                'ä¸‰è‡³å››çŽ¯': 3.5, 'å››è‡³äº”çŽ¯': 4.5, 'ä¸‰çŽ¯å¤–': 5.0, 'äº”è‡³å…­çŽ¯': 5.5,\n                'å››çŽ¯å¤–': 6.0, 'å…­çŽ¯å¤–': 6.5, 'å†…çŽ¯å†…': 1.0, 'å†…çŽ¯è‡³ä¸­çŽ¯': 1.8,\n                'ä¸­çŽ¯è‡³å¤–çŽ¯': 4.2, 'å†…çŽ¯è‡³å¤–çŽ¯': 3.0, 'å¤–çŽ¯å¤–': 7.0}\n    df['çŽ¯çº¿æ•°å€¼'] = df['çŽ¯çº¿'].map(ring_map).fillna(4.0)\n    df['çŽ¯çº¿é¢ç§¯'] = df['çŽ¯çº¿æ•°å€¼'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['æ˜¯å¦ç²¾è£…'] = (df['è£…ä¿®æƒ…å†µ'].fillna('å…¶ä»–') == 'ç²¾è£…').astype(int)\n    df['ç²¾è£…é¢ç§¯'] = df['æ˜¯å¦ç²¾è£…'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['äº§æƒ_éžå…±æœ‰'] = (df['äº§æƒæ‰€å±ž'].fillna('å…¶ä»–') == 'éžå…±æœ‰').astype(int)\n    df['äº§æƒ_å…±æœ‰'] = (df['äº§æƒæ‰€å±ž'].fillna('å…¶ä»–') == 'å…±æœ‰').astype(int)\n    df['äº§æƒ_éžå…±æœ‰_é¢ç§¯'] = df['äº§æƒ_éžå…±æœ‰'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['äº§æƒ_å…±æœ‰_é¢ç§¯'] = df['äº§æƒ_å…±æœ‰'] * df['æœ‰æ•ˆé¢ç§¯']\n    return df\n\ndef cluster_price(df, ref_df=None):\n    df['lon'] = pd.to_numeric(df['lon'], errors='coerce') + np.random.normal(0, 0.01, size=len(df))\n    df['lat'] = pd.to_numeric(df['lat'], errors='coerce') + np.random.normal(0, 0.01, size=len(df))\n    if ref_df is None:\n        kmeans = KMeans(n_clusters=10, random_state=42)\n        df['èšç±»æ ‡ç­¾'] = kmeans.fit_predict(df[['lon', 'lat']])\n        df['å•ä»·'] = df['ä»·æ ¼'] / df['æœ‰æ•ˆé¢ç§¯']\n        df['èšç±»å‡å•ä»·'] = df.groupby('èšç±»æ ‡ç­¾')['å•ä»·'].transform('mean')\n        df['èšç±»ä¼°ä»·'] = df['èšç±»å‡å•ä»·'] * df['æœ‰æ•ˆé¢ç§¯']\n        return df, kmeans, df.groupby('èšç±»æ ‡ç­¾')['å•ä»·'].mean()\n    else:\n        df['èšç±»æ ‡ç­¾'] = ref_df[0].predict(df[['lon', 'lat']])\n        df['èšç±»å‡å•ä»·'] = df['èšç±»æ ‡ç­¾'].map(ref_df[1])\n        df['èšç±»ä¼°ä»·'] = df['èšç±»å‡å•ä»·'] * df['æœ‰æ•ˆé¢ç§¯']\n        return df\n\n# ========== 3. æ¸…æ´—ä¸Žç‰¹å¾å·¥ç¨‹ ==========\nfor df in [train_df, test_df]:\n    df = clean_area(df)\n    df = extract_floor(df)\n    df = extract_layout(df)\n    df = enrich_features(df)\ntest_df['æˆ¿å±‹ç”¨é€”å‡ä»·'] = test_df['æˆ¿å±‹ç”¨é€”'].map(ç”¨é€”å‡ä»·).fillna(train_df['ä»·æ ¼'].mean())\ntest_df['åŸŽå¸‚å¹´é™äº¤äº’'] = test_df['åŸŽå¸‚ç¼–ç '] * test_df['æˆ¿å±‹å¹´é™ç­‰çº§']\ntrain_df, kmeans_model, price_map = cluster_price(train_df)\ntest_df = cluster_price(test_df, ref_df=(kmeans_model, price_map))\n\nfor df in [train_df, test_df]:\n    df['æˆ¿é—´Ã—èšç±»å‡å•ä»·'] = df['æ€»æˆ¿é—´æ•°'] * df['èšç±»å‡å•ä»·']\n    df['ç”µæ¢¯Ã—é¢ç§¯'] = df['é…å¤‡ç”µæ¢¯'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['ç²¾è£…Ã—é¢ç§¯Ã—å‡ä»·'] = df['æ˜¯å¦ç²¾è£…'] * df['æœ‰æ•ˆé¢ç§¯'] * df['èšç±»å‡å•ä»·']\n    df['éžå…±æœ‰Ã—ç”µæ¢¯Ã—é¢ç§¯'] = df['äº§æƒ_éžå…±æœ‰'] * df['é…å¤‡ç”µæ¢¯'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['æˆ¿æ•°Ã—ç”µæ¢¯Ã—çŽ¯çº¿'] = df['æ€»æˆ¿é—´æ•°'] * df['é…å¤‡ç”µæ¢¯'] * df['çŽ¯çº¿æ•°å€¼']\n\n# ========== 4. åŽ»é™¤æžç«¯å€¼ ==========\nq1 = train_df['ä»·æ ¼'].quantile(0.01)\nq99 = train_df['ä»·æ ¼'].quantile(0.99)\ntrain_df = train_df[(train_df['ä»·æ ¼'] >= q1) & (train_df['ä»·æ ¼'] <= q99)].copy()\ndef clip_outliers(df, column, lower_quantile=0.01, upper_quantile=0.99):\n    lower = df[column].quantile(lower_quantile)\n    upper = df[column].quantile(upper_quantile)\n    df[column] = df[column].clip(lower, upper)\n    return df\n\nfor col in ['æœ‰æ•ˆé¢ç§¯', 'æ€»æ¥¼å±‚', 'èšç±»ä¼°ä»·']:\n    train_df = clip_outliers(train_df, col)\n\n\nfrom sklearn.linear_model import Ridge  # ä½¿ç”¨ Ridge å›žå½’\n\n# ========== 5. è®­ç»ƒæ¨¡åž‹ ==========\nall_features = [\n    'åŸŽå¸‚ç¼–ç ','åŒºåŸŸç¼–ç ','æˆ¿å±‹å¹´é™ç­‰çº§','æ˜¯å¦æ»¡äº”','æœå‘_å«å—','æˆ¿å±‹ç”¨é€”å‡ä»·',\n    'å®¤','åŽ…','åŽ¨','å«','å½“å‰æ¥¼å±‚','æ€»æ¥¼å±‚','é…å¤‡ç”µæ¢¯',\n    'çŽ¯çº¿æ•°å€¼','lon','lat','åŸŽå¸‚ç¼–ç ','æœ‰æ•ˆé¢ç§¯','åŸŽå¸‚å¹´é™äº¤äº’',\n    'ç²¾è£…é¢ç§¯', 'åŸŽå¸‚', 'èšç±»å‡å•ä»·', 'çŽ¯çº¿é¢ç§¯',\n    'äº§æƒ_éžå…±æœ‰_é¢ç§¯', 'äº§æƒ_å…±æœ‰_é¢ç§¯', 'æ€»æˆ¿é—´æ•°', 'æˆ¿é—´Ã—èšç±»å‡å•ä»·',\n    'ç”µæ¢¯Ã—é¢ç§¯', 'ç²¾è£…Ã—é¢ç§¯Ã—å‡ä»·', 'éžå…±æœ‰Ã—ç”µæ¢¯Ã—é¢ç§¯', 'æˆ¿æ•°Ã—ç”µæ¢¯Ã—çŽ¯çº¿'\n]\n\nX = train_df[all_features].dropna()\né¢ç§¯_series = train_df.loc[X.index, 'æœ‰æ•ˆé¢ç§¯']\ny = train_df.loc[X.index, 'ä»·æ ¼'] / é¢ç§¯_series\n\nprint(f\"âœ… å‰”é™¤å¼‚å¸¸å€¼åŽç”¨äºŽè®­ç»ƒçš„æ•°æ®æ•°é‡: {len(X)}\")\n\nX_train, X_val, y_train, y_val, area_train, area_val = train_test_split(\n    X, y, é¢ç§¯_series, test_size=0.2, random_state=42\n)\n\ntest_X = test_df[all_features].copy()\n\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_scaled = scaler.fit_transform(X)\ntest_X_scaled = scaler.transform(test_X)\n\n# æ¨¡åž‹è®­ç»ƒï¼ˆRidgeï¼‰\nmodel = Ridge(alpha=0.1)\nmodel.fit(X_train_scaled, y_train)\n\n# ========== 6. è¯„ä¼° ==========\ny_val_pred_unit = model.predict(X_val_scaled)\ny_val_pred_price = y_val_pred_unit * area_val\ny_val_true_price = y_val * area_val\n\nrmse_val = mean_squared_error(y_val_true_price, y_val_pred_price, squared=False)\nmae_val = mean_absolute_error(y_val_true_price, y_val_pred_price)\nprint(f\"âœ… éªŒè¯é›† RMSEï¼ˆè¿˜åŽŸåŽï¼‰: {rmse_val:,.2f}\")\nprint(f\"âœ… éªŒè¯é›† MAE ï¼ˆè¿˜åŽŸåŽï¼‰: {mae_val:,.2f}\")\n\ny_train_pred_unit = model.predict(X_train_scaled)\ny_train_pred_price = y_train_pred_unit * area_train\ny_train_true_price = y_train * area_train\n\nrmse_train = mean_squared_error(y_train_true_price, y_train_pred_price, squared=False)\nmae_train = mean_absolute_error(y_train_true_price, y_train_pred_price)\nprint(f\"âœ… è®­ç»ƒé›† RMSEï¼ˆè¿˜åŽŸåŽï¼‰: {rmse_train:,.2f}\")\nprint(f\"âœ… è®­ç»ƒé›† MAE ï¼ˆè¿˜åŽŸåŽï¼‰: {mae_train:,.2f}\")\n\n# ========== 7. 6-Fold äº¤å‰éªŒè¯ï¼ˆRMSE & MAEï¼Œè¿˜åŽŸåŽï¼‰ ==========\ndef cross_val_rmse_mae(X, y_unit, area_series, n_splits=6):\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    rmse_list, mae_list = [], []\n\n    for i, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]\n        y_train_cv, y_val_cv = y_unit.iloc[train_idx], y_unit.iloc[val_idx]\n        area_train_cv, area_val_cv = area_series.iloc[train_idx], area_series.iloc[val_idx]\n\n        scaler_cv = MinMaxScaler()\n        X_train_scaled = scaler_cv.fit_transform(X_train_cv)\n        X_val_scaled = scaler_cv.transform(X_val_cv)\n\n        model_cv = Ridge(alpha=0.1)\n        model_cv.fit(X_train_scaled, y_train_cv)\n        y_pred_unit = model_cv.predict(X_val_scaled)\n\n        y_pred_price = y_pred_unit * area_val_cv\n        y_true_price = y_val_cv * area_val_cv\n\n        rmse = mean_squared_error(y_true_price, y_pred_price, squared=False)\n        mae = mean_absolute_error(y_true_price, y_pred_price)\n\n        print(f\"Fold {i}: RMSE = {rmse:,.2f}, MAE = {mae:,.2f}\")\n        rmse_list.append(rmse)\n        mae_list.append(mae)\n\n    print(f\"\\nâœ… 6æŠ˜CV å¹³å‡ RMSEï¼ˆè¿˜åŽŸä»·ï¼‰: {np.mean(rmse_list):,.2f}\")\n    print(f\"âœ… 6æŠ˜CV å¹³å‡ MAE ï¼ˆè¿˜åŽŸä»·ï¼‰: {np.mean(mae_list):,.2f}\")\n\ncross_val_rmse_mae(X, y, é¢ç§¯_series)\n\n# ========== 8. å›žå½’æ–¹ç¨‹ ==========\nprint(\"âœ… å›žå½’æ–¹ç¨‹:\")\nprint(\"Intercept:\", model.intercept_)\nfor name, coef in zip(X.columns, model.coef_):\n    print(f\"{name}: {coef:.4f}\")\n\n# ========== 9. é¢„æµ‹æµ‹è¯•é›† ==========\ntest_unit_price_pred = model.predict(test_X_scaled)\ntest_area = test_df['æœ‰æ•ˆé¢ç§¯']\ntest_price_pred = test_unit_price_pred * test_area\n\n# ========== 10. ç”Ÿæˆæäº¤æ–‡ä»¶ ==========\nsubmission = test_df[['ID']].copy()\nsubmission['Price'] = np.round(test_price_pred).astype(int)\nsubmission.to_csv('submission.csv', index=False)\nprint(\"âœ… é¢„æµ‹ç»“æžœå·²ä¿å­˜ä¸º submission.csv\")","outputs":[{"output_type":"stream","name":"stdout","text":"âœ… å‰”é™¤å¼‚å¸¸å€¼åŽç”¨äºŽè®­ç»ƒçš„æ•°æ®æ•°é‡: 82386\nâœ… éªŒè¯é›† RMSEï¼ˆè¿˜åŽŸåŽï¼‰: 977,203.08\nâœ… éªŒè¯é›† MAE ï¼ˆè¿˜åŽŸåŽï¼‰: 592,062.87\nâœ… è®­ç»ƒé›† RMSEï¼ˆè¿˜åŽŸåŽï¼‰: 1,000,861.78\nâœ… è®­ç»ƒé›† MAE ï¼ˆè¿˜åŽŸåŽï¼‰: 602,237.81\nFold 1: RMSE = 972,690.76, MAE = 588,763.84\nFold 2: RMSE = 1,007,689.72, MAE = 602,810.92\nFold 3: RMSE = 1,004,834.55, MAE = 600,401.14\nFold 4: RMSE = 967,938.53, MAE = 589,722.97\nFold 5: RMSE = 1,005,951.88, MAE = 605,310.61\nFold 6: RMSE = 1,027,290.66, MAE = 609,677.09\n\nâœ… 6æŠ˜CV å¹³å‡ RMSEï¼ˆè¿˜åŽŸä»·ï¼‰: 997,732.68\nâœ… 6æŠ˜CV å¹³å‡ MAE ï¼ˆè¿˜åŽŸä»·ï¼‰: 599,447.76\nâœ… å›žå½’æ–¹ç¨‹:\nIntercept: -541.9388602440522\nåŸŽå¸‚ç¼–ç : 1252.1358\nåŒºåŸŸç¼–ç : 5620.7528\næˆ¿å±‹å¹´é™ç­‰çº§: 3428.5726\næ˜¯å¦æ»¡äº”: 250.0679\næœå‘_å«å—: -44.8971\næˆ¿å±‹ç”¨é€”å‡ä»·: 21778.9007\nå®¤: 4296.1051\nåŽ…: -1812.3877\nåŽ¨: 29363.6455\nå«: 17928.3534\nå½“å‰æ¥¼å±‚: 0.0000\næ€»æ¥¼å±‚: 1295.7406\né…å¤‡ç”µæ¢¯: -3009.9434\nçŽ¯çº¿æ•°å€¼: -30592.7743\nlon: 2219.2049\nlat: 3511.3794\nåŸŽå¸‚ç¼–ç : 1252.1358\næœ‰æ•ˆé¢ç§¯: -14931.7787\nåŸŽå¸‚å¹´é™äº¤äº’: -7743.8468\nç²¾è£…é¢ç§¯: 3780.3611\nåŸŽå¸‚: 1252.1358\nèšç±»å‡å•ä»·: 61628.7780\nçŽ¯çº¿é¢ç§¯: 82751.3605\näº§æƒ_éžå…±æœ‰_é¢ç§¯: 267046.4623\näº§æƒ_å…±æœ‰_é¢ç§¯: 81908.7827\næ€»æˆ¿é—´æ•°: 14902.8773\næˆ¿é—´Ã—èšç±»å‡å•ä»·: -26460.5921\nç”µæ¢¯Ã—é¢ç§¯: -229315.2515\nç²¾è£…Ã—é¢ç§¯Ã—å‡ä»·: 10317.7724\néžå…±æœ‰Ã—ç”µæ¢¯Ã—é¢ç§¯: -111711.4245\næˆ¿æ•°Ã—ç”µæ¢¯Ã—çŽ¯çº¿: 39854.6722\nâœ… é¢„æµ‹ç»“æžœå·²ä¿å­˜ä¸º submission.csv\n"}],"execution_count":13},{"cell_type":"markdown","metadata":{"id":"0817D674DBC742A58DBFDD8D59AB5512","notebookId":"67ebe8c65302e998c320c3fd","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 4.Elastic Net"},{"cell_type":"code","metadata":{"id":"F87D62E330AC4268AED1FFC7BEBF70FC","notebookId":"67ebe8c65302e998c320c3fd","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"#elastic net\n\nimport pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\n# ========== 1. è¯»å–æ•°æ® ==========\ntrain_df = pd.read_csv('/home/mw/input/quant4533/ruc_Class25Q1_train.csv')\ntest_df = pd.read_csv('/home/mw/input/quant4533/ruc_Class25Q1_test.csv')\noriginal_test = test_df[['ID']].copy()\n\n# ========== 2. ç‰¹å¾å·¥ç¨‹å‡½æ•° ==========\ndef clean_area(df):\n    for col in ['å»ºç­‘é¢ç§¯', 'å¥—å†…é¢ç§¯']:\n        df[col] = df[col].astype(str).str.replace('ãŽ¡', '', regex=False)\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n    df['æœ‰æ•ˆé¢ç§¯'] = df['å¥—å†…é¢ç§¯'].fillna(df['å»ºç­‘é¢ç§¯'])\n    df['æœ‰æ•ˆé¢ç§¯'] = df['æœ‰æ•ˆé¢ç§¯'].fillna(df['æœ‰æ•ˆé¢ç§¯'].median())\n    return df\n# é‡æ–°è®¡ç®—æˆ¿å±‹ç”¨é€”å‡ä»·æ˜ å°„ï¼ˆç¡®ä¿å·²ç”Ÿæˆï¼‰\nç”¨é€”å‡ä»· = train_df.groupby('æˆ¿å±‹ç”¨é€”')['ä»·æ ¼'].mean()\ntrain_df['æˆ¿å±‹ç”¨é€”å‡ä»·'] = train_df['æˆ¿å±‹ç”¨é€”'].map(ç”¨é€”å‡ä»·).fillna(train_df['ä»·æ ¼'].mean())\n\n# ç¡®ä¿åŸŽå¸‚ç¼–ç ä¸Žæˆ¿å±‹å¹´é™ç­‰çº§å­—æ®µå·²å­˜åœ¨\nif 'åŸŽå¸‚ç¼–ç ' not in train_df.columns:\n    train_df['åŸŽå¸‚ç¼–ç '] = train_df['åŸŽå¸‚'].astype('category').cat.codes\nif 'æˆ¿å±‹å¹´é™ç­‰çº§' not in train_df.columns:\n    train_df['æˆ¿å±‹å¹´é™ç­‰çº§'] = train_df['æˆ¿å±‹å¹´é™'].map(lambda x: 3 if 'æ»¡äº”' in str(x) else (2 if 'æ»¡ä¸¤' in str(x) else 1))\n\n# åˆ›å»ºåŸŽå¸‚å¹´é™äº¤äº’é¡¹\ntrain_df['åŸŽå¸‚å¹´é™äº¤äº’'] = train_df['åŸŽå¸‚ç¼–ç '] * train_df['æˆ¿å±‹å¹´é™ç­‰çº§']\ndef extract_floor(df):\n    df['å½“å‰æ¥¼å±‚_raw'] = df['æ‰€åœ¨æ¥¼å±‚'].str.extract(r'ç¬¬?(\\d+)[å±‚æ¥¼]')[0].astype(float)\n    df['æ€»æ¥¼å±‚'] = df['æ‰€åœ¨æ¥¼å±‚'].str.extract(r'å…±?(\\d+)[å±‚æ¥¼]')[0].astype(float)\n    df.loc[df['å½“å‰æ¥¼å±‚_raw'].isna(), 'å½“å‰æ¥¼å±‚_raw'] = df['æ‰€åœ¨æ¥¼å±‚'].str.extract(r'(åº•|ä½Ž|ä¸­|é«˜|é¡¶)')[0].map({\n        'åº•': 1, 'ä½Ž': 2, 'ä¸­': 3, 'é«˜': 4, 'é¡¶': 5\n    })\n    df['å½“å‰æ¥¼å±‚_raw'] = df['å½“å‰æ¥¼å±‚_raw'].fillna(1)\n    df['æ€»æ¥¼å±‚'] = df['æ€»æ¥¼å±‚'].fillna(df['æ€»æ¥¼å±‚'].median())\n    df['å½“å‰æ¥¼å±‚'] = (df['å½“å‰æ¥¼å±‚_raw'] / df['æ€»æ¥¼å±‚']).clip(0, 1)\n    df.drop(columns=['å½“å‰æ¥¼å±‚_raw'], inplace=True)\n    return df\n\ndef extract_layout(df):\n    df['å®¤'] = df['æˆ¿å±‹æˆ·åž‹'].str.extract(r'(\\d+)å®¤').fillna(0).astype(int)\n    df['åŽ…'] = df['æˆ¿å±‹æˆ·åž‹'].str.extract(r'(\\d+)åŽ…').fillna(0).astype(int)\n    df['åŽ¨'] = df['æˆ¿å±‹æˆ·åž‹'].str.extract(r'(\\d+)åŽ¨').fillna(0).astype(int)\n    df['å«'] = df['æˆ¿å±‹æˆ·åž‹'].str.extract(r'(\\d+)å«').fillna(0).astype(int)\n    df['æ€»æˆ¿é—´æ•°'] = df[['å®¤', 'åŽ…', 'åŽ¨', 'å«']].sum(axis=1)\n    return df\n\ndef enrich_features(df):\n    df['æ˜¯å¦æ»¡äº”'] = df['æˆ¿å±‹å¹´é™'].apply(lambda x: 1 if 'æ»¡äº”' in str(x) else 0)\n    df['æœå‘_å«å—'] = df['æˆ¿å±‹æœå‘'].apply(lambda x: 1 if 'å—' in str(x) else 0)\n    df['é…å¤‡ç”µæ¢¯'] = df['é…å¤‡ç”µæ¢¯'].map({'æœ‰': 1, 'æ— ': 0}).fillna(0)\n    df['åŸŽå¸‚ç¼–ç '] = df['åŸŽå¸‚'].astype('category').cat.codes\n    df['åŒºåŸŸç¼–ç '] = df['åŒºåŸŸ'].astype('category').cat.codes\n    df['æˆ¿å±‹å¹´é™ç­‰çº§'] = df['æˆ¿å±‹å¹´é™'].map(lambda x: 3 if 'æ»¡äº”' in str(x) else (2 if 'æ»¡ä¸¤' in str(x) else 1))\n    ring_map = {'ä¸€çŽ¯å†…': 1.0, 'ä¸€è‡³äºŒçŽ¯': 1.5, 'äºŒçŽ¯å†…': 2.0, 'äºŒè‡³ä¸‰çŽ¯': 2.5,\n                'ä¸‰è‡³å››çŽ¯': 3.5, 'å››è‡³äº”çŽ¯': 4.5, 'ä¸‰çŽ¯å¤–': 5.0, 'äº”è‡³å…­çŽ¯': 5.5,\n                'å››çŽ¯å¤–': 6.0, 'å…­çŽ¯å¤–': 6.5, 'å†…çŽ¯å†…': 1.0, 'å†…çŽ¯è‡³ä¸­çŽ¯': 1.8,\n                'ä¸­çŽ¯è‡³å¤–çŽ¯': 4.2, 'å†…çŽ¯è‡³å¤–çŽ¯': 3.0, 'å¤–çŽ¯å¤–': 7.0}\n    df['çŽ¯çº¿æ•°å€¼'] = df['çŽ¯çº¿'].map(ring_map).fillna(4.0)\n    df['çŽ¯çº¿é¢ç§¯'] = df['çŽ¯çº¿æ•°å€¼'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['æ˜¯å¦ç²¾è£…'] = (df['è£…ä¿®æƒ…å†µ'].fillna('å…¶ä»–') == 'ç²¾è£…').astype(int)\n    df['ç²¾è£…é¢ç§¯'] = df['æ˜¯å¦ç²¾è£…'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['äº§æƒ_éžå…±æœ‰'] = (df['äº§æƒæ‰€å±ž'].fillna('å…¶ä»–') == 'éžå…±æœ‰').astype(int)\n    df['äº§æƒ_å…±æœ‰'] = (df['äº§æƒæ‰€å±ž'].fillna('å…¶ä»–') == 'å…±æœ‰').astype(int)\n    df['äº§æƒ_éžå…±æœ‰_é¢ç§¯'] = df['äº§æƒ_éžå…±æœ‰'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['äº§æƒ_å…±æœ‰_é¢ç§¯'] = df['äº§æƒ_å…±æœ‰'] * df['æœ‰æ•ˆé¢ç§¯']\n    return df\n\ndef cluster_price(df, ref_df=None):\n    df['lon'] = pd.to_numeric(df['lon'], errors='coerce') + np.random.normal(0, 0.01, size=len(df))\n    df['lat'] = pd.to_numeric(df['lat'], errors='coerce') + np.random.normal(0, 0.01, size=len(df))\n    if ref_df is None:\n        kmeans = KMeans(n_clusters=10, random_state=42)\n        df['èšç±»æ ‡ç­¾'] = kmeans.fit_predict(df[['lon', 'lat']])\n        df['å•ä»·'] = df['ä»·æ ¼'] / df['æœ‰æ•ˆé¢ç§¯']\n        df['èšç±»å‡å•ä»·'] = df.groupby('èšç±»æ ‡ç­¾')['å•ä»·'].transform('mean')\n        df['èšç±»ä¼°ä»·'] = df['èšç±»å‡å•ä»·'] * df['æœ‰æ•ˆé¢ç§¯']\n        return df, kmeans, df.groupby('èšç±»æ ‡ç­¾')['å•ä»·'].mean()\n    else:\n        df['èšç±»æ ‡ç­¾'] = ref_df[0].predict(df[['lon', 'lat']])\n        df['èšç±»å‡å•ä»·'] = df['èšç±»æ ‡ç­¾'].map(ref_df[1])\n        df['èšç±»ä¼°ä»·'] = df['èšç±»å‡å•ä»·'] * df['æœ‰æ•ˆé¢ç§¯']\n        return df\n\n# ========== 3. æ¸…æ´—ä¸Žç‰¹å¾å·¥ç¨‹ ==========\nfor df in [train_df, test_df]:\n    df = clean_area(df)\n    df = extract_floor(df)\n    df = extract_layout(df)\n    df = enrich_features(df)\ntest_df['æˆ¿å±‹ç”¨é€”å‡ä»·'] = test_df['æˆ¿å±‹ç”¨é€”'].map(ç”¨é€”å‡ä»·).fillna(train_df['ä»·æ ¼'].mean())\ntest_df['åŸŽå¸‚å¹´é™äº¤äº’'] = test_df['åŸŽå¸‚ç¼–ç '] * test_df['æˆ¿å±‹å¹´é™ç­‰çº§']\ntrain_df, kmeans_model, price_map = cluster_price(train_df)\ntest_df = cluster_price(test_df, ref_df=(kmeans_model, price_map))\n\nfor df in [train_df, test_df]:\n    df['æˆ¿é—´Ã—èšç±»å‡å•ä»·'] = df['æ€»æˆ¿é—´æ•°'] * df['èšç±»å‡å•ä»·']\n    df['ç”µæ¢¯Ã—é¢ç§¯'] = df['é…å¤‡ç”µæ¢¯'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['ç²¾è£…Ã—é¢ç§¯Ã—å‡ä»·'] = df['æ˜¯å¦ç²¾è£…'] * df['æœ‰æ•ˆé¢ç§¯'] * df['èšç±»å‡å•ä»·']\n    df['éžå…±æœ‰Ã—ç”µæ¢¯Ã—é¢ç§¯'] = df['äº§æƒ_éžå…±æœ‰'] * df['é…å¤‡ç”µæ¢¯'] * df['æœ‰æ•ˆé¢ç§¯']\n    df['æˆ¿æ•°Ã—ç”µæ¢¯Ã—çŽ¯çº¿'] = df['æ€»æˆ¿é—´æ•°'] * df['é…å¤‡ç”µæ¢¯'] * df['çŽ¯çº¿æ•°å€¼']\n\n# ========== 4. åŽ»é™¤æžç«¯å€¼ ==========\nq1 = train_df['ä»·æ ¼'].quantile(0.01)\nq99 = train_df['ä»·æ ¼'].quantile(0.99)\ntrain_df = train_df[(train_df['ä»·æ ¼'] >= q1) & (train_df['ä»·æ ¼'] <= q99)].copy()\ndef clip_outliers(df, column, lower_quantile=0.01, upper_quantile=0.99):\n    lower = df[column].quantile(lower_quantile)\n    upper = df[column].quantile(upper_quantile)\n    df[column] = df[column].clip(lower, upper)\n    return df\n\nfor col in ['æœ‰æ•ˆé¢ç§¯', 'æ€»æ¥¼å±‚', 'èšç±»ä¼°ä»·']:\n    train_df = clip_outliers(train_df, col)\n\n\nfrom sklearn.linear_model import ElasticNet  # ä½¿ç”¨ ElasticNet å›žå½’\n\n# ========== 5. è®­ç»ƒæ¨¡åž‹ ==========\nall_features = [\n    'åŸŽå¸‚ç¼–ç ','åŒºåŸŸç¼–ç ','æˆ¿å±‹å¹´é™ç­‰çº§','æ˜¯å¦æ»¡äº”','æœå‘_å«å—','æˆ¿å±‹ç”¨é€”å‡ä»·',\n    'å®¤','åŽ…','åŽ¨','å«','æ€»æ¥¼å±‚','æœ‰æ•ˆé¢ç§¯','é…å¤‡ç”µæ¢¯',\n    'çŽ¯çº¿æ•°å€¼','lon','lat','åŸŽå¸‚ç¼–ç ','åŸŽå¸‚å¹´é™äº¤äº’',\n    'ç²¾è£…é¢ç§¯', 'åŸŽå¸‚', 'èšç±»å‡å•ä»·', 'çŽ¯çº¿é¢ç§¯',\n    'äº§æƒ_éžå…±æœ‰_é¢ç§¯', 'äº§æƒ_å…±æœ‰_é¢ç§¯', 'æ€»æˆ¿é—´æ•°', 'æˆ¿é—´Ã—èšç±»å‡å•ä»·',\n    'ç”µæ¢¯Ã—é¢ç§¯', 'ç²¾è£…Ã—é¢ç§¯Ã—å‡ä»·', 'éžå…±æœ‰Ã—ç”µæ¢¯Ã—é¢ç§¯', 'æˆ¿æ•°Ã—ç”µæ¢¯Ã—çŽ¯çº¿'\n]\n\nX = train_df[all_features].dropna()\né¢ç§¯_series = train_df.loc[X.index, 'æœ‰æ•ˆé¢ç§¯']\ny = train_df.loc[X.index, 'ä»·æ ¼'] / é¢ç§¯_series\n\nprint(f\"âœ… å‰”é™¤å¼‚å¸¸å€¼åŽç”¨äºŽè®­ç»ƒçš„æ•°æ®æ•°é‡: {len(X)}\")\n\nX_train, X_val, y_train, y_val, area_train, area_val = train_test_split(\n    X, y, é¢ç§¯_series, test_size=0.2, random_state=42\n)\n\ntest_X = test_df[all_features].copy()\n\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_scaled = scaler.fit_transform(X)\ntest_X_scaled = scaler.transform(test_X)\n\n# æ¨¡åž‹è®­ç»ƒï¼ˆElasticNetï¼‰\nmodel = ElasticNet(alpha=0.1, l1_ratio=0.5)\nmodel.fit(X_train_scaled, y_train)\n\n# ========== 6. è¯„ä¼° ==========\ny_val_pred_unit = model.predict(X_val_scaled)\ny_val_pred_price = y_val_pred_unit * area_val\ny_val_true_price = y_val * area_val\n\nrmse_val = mean_squared_error(y_val_true_price, y_val_pred_price, squared=False)\nmae_val = mean_absolute_error(y_val_true_price, y_val_pred_price)\nprint(f\"âœ… éªŒè¯é›† RMSEï¼ˆè¿˜åŽŸåŽï¼‰: {rmse_val:,.2f}\")\nprint(f\"âœ… éªŒè¯é›† MAE ï¼ˆè¿˜åŽŸåŽï¼‰: {mae_val:,.2f}\")\n\ny_train_pred_unit = model.predict(X_train_scaled)\ny_train_pred_price = y_train_pred_unit * area_train\ny_train_true_price = y_train * area_train\n\nrmse_train = mean_squared_error(y_train_true_price, y_train_pred_price, squared=False)\nmae_train = mean_absolute_error(y_train_true_price, y_train_pred_price)\nprint(f\"âœ… è®­ç»ƒé›† RMSEï¼ˆè¿˜åŽŸåŽï¼‰: {rmse_train:,.2f}\")\nprint(f\"âœ… è®­ç»ƒé›† MAE ï¼ˆè¿˜åŽŸåŽï¼‰: {mae_train:,.2f}\")\n\n# ========== 7. 6-Fold äº¤å‰éªŒè¯ï¼ˆRMSE & MAEï¼Œè¿˜åŽŸåŽï¼‰ ==========\ndef cross_val_rmse_mae(X, y_unit, area_series, n_splits=6):\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    rmse_list, mae_list = [], []\n\n    for i, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]\n        y_train_cv, y_val_cv = y_unit.iloc[train_idx], y_unit.iloc[val_idx]\n        area_train_cv, area_val_cv = area_series.iloc[train_idx], area_series.iloc[val_idx]\n\n        scaler_cv = MinMaxScaler()\n        X_train_scaled = scaler_cv.fit_transform(X_train_cv)\n        X_val_scaled = scaler_cv.transform(X_val_cv)\n\n        model_cv = ElasticNet(alpha=0.1, l1_ratio=0.5)\n        model_cv.fit(X_train_scaled, y_train_cv)\n        y_pred_unit = model_cv.predict(X_val_scaled)\n\n        y_pred_price = y_pred_unit * area_val_cv\n        y_true_price = y_val_cv * area_val_cv\n\n        rmse = mean_squared_error(y_true_price, y_pred_price, squared=False)\n        mae = mean_absolute_error(y_true_price, y_pred_price)\n\n        print(f\"Fold {i}: RMSE = {rmse:,.2f}, MAE = {mae:,.2f}\")\n        rmse_list.append(rmse)\n        mae_list.append(mae)\n\n    print(f\"\\nâœ… 6æŠ˜CV å¹³å‡ RMSEï¼ˆè¿˜åŽŸä»·ï¼‰: {np.mean(rmse_list):,.2f}\")\n    print(f\"âœ… 6æŠ˜CV å¹³å‡ MAE ï¼ˆè¿˜åŽŸä»·ï¼‰: {np.mean(mae_list):,.2f}\")\n\ncross_val_rmse_mae(X, y, é¢ç§¯_series)\n\n# ========== 8. å›žå½’æ–¹ç¨‹ ==========\nprint(\"âœ… å›žå½’æ–¹ç¨‹:\")\nprint(\"Intercept:\", model.intercept_)\nfor name, coef in zip(X.columns, model.coef_):\n    print(f\"{name}: {coef:.4f}\")\n\n# ========== 9. é¢„æµ‹æµ‹è¯•é›† ==========\ntest_unit_price_pred = model.predict(test_X_scaled)\ntest_area = test_df['æœ‰æ•ˆé¢ç§¯']\ntest_price_pred = test_unit_price_pred * test_area\n\n# ========== 10. ç”Ÿæˆæäº¤æ–‡ä»¶ ==========\nsubmission = test_df[['ID']].copy()\nsubmission['Price'] = np.round(test_price_pred).astype(int)\nsubmission.to_csv('submission.csv', index=False)\nprint(\"âœ… é¢„æµ‹ç»“æžœå·²ä¿å­˜ä¸º submission.csv\")","outputs":[{"output_type":"stream","name":"stdout","text":"âœ… å‰”é™¤å¼‚å¸¸å€¼åŽç”¨äºŽè®­ç»ƒçš„æ•°æ®æ•°é‡: 82459\nâœ… éªŒè¯é›† RMSEï¼ˆè¿˜åŽŸåŽï¼‰: 1,132,946.25\nâœ… éªŒè¯é›† MAE ï¼ˆè¿˜åŽŸåŽï¼‰: 705,847.13\nâœ… è®­ç»ƒé›† RMSEï¼ˆè¿˜åŽŸåŽï¼‰: 1,149,910.34\nâœ… è®­ç»ƒé›† MAE ï¼ˆè¿˜åŽŸåŽï¼‰: 707,383.14\nFold 1: RMSE = 1,132,910.09, MAE = 704,887.29\nFold 2: RMSE = 1,146,687.56, MAE = 705,034.11\nFold 3: RMSE = 1,132,568.66, MAE = 697,153.31\nFold 4: RMSE = 1,150,048.60, MAE = 708,729.46\nFold 5: RMSE = 1,166,220.53, MAE = 710,716.30\nFold 6: RMSE = 1,154,253.90, MAE = 718,450.50\n\nâœ… 6æŠ˜CV å¹³å‡ RMSEï¼ˆè¿˜åŽŸä»·ï¼‰: 1,147,114.89\nâœ… 6æŠ˜CV å¹³å‡ MAE ï¼ˆè¿˜åŽŸä»·ï¼‰: 707,495.16\nâœ… å›žå½’æ–¹ç¨‹:\nIntercept: 14225.398166488372\nåŸŽå¸‚ç¼–ç : -2972.0050\nåŒºåŸŸç¼–ç : 2571.7639\næˆ¿å±‹å¹´é™ç­‰çº§: 4712.7083\næ˜¯å¦æ»¡äº”: 2415.9786\næœå‘_å«å—: 69.7005\næˆ¿å±‹ç”¨é€”å‡ä»·: 744.2884\nå®¤: -121.4877\nåŽ…: -543.2436\nåŽ¨: 484.9370\nå«: 297.6376\næ€»æ¥¼å±‚: -404.1836\næœ‰æ•ˆé¢ç§¯: -1567.2995\né…å¤‡ç”µæ¢¯: 1174.2675\nçŽ¯çº¿æ•°å€¼: -7197.0587\nlon: 3521.8675\nlat: 2725.8243\nåŸŽå¸‚ç¼–ç : -2971.9860\nåŸŽå¸‚å¹´é™äº¤äº’: -3104.3478\nç²¾è£…é¢ç§¯: 1305.4177\nåŸŽå¸‚: -2971.9944\nèšç±»å‡å•ä»·: 26249.6510\nçŽ¯çº¿é¢ç§¯: -109.9649\näº§æƒ_éžå…±æœ‰_é¢ç§¯: -27.7646\näº§æƒ_å…±æœ‰_é¢ç§¯: 18.5970\næ€»æˆ¿é—´æ•°: 45.1782\næˆ¿é—´Ã—èšç±»å‡å•ä»·: 6917.5880\nç”µæ¢¯Ã—é¢ç§¯: 13.7250\nç²¾è£…Ã—é¢ç§¯Ã—å‡ä»·: 2748.1640\néžå…±æœ‰Ã—ç”µæ¢¯Ã—é¢ç§¯: 1.7952\næˆ¿æ•°Ã—ç”µæ¢¯Ã—çŽ¯çº¿: 176.0410\nâœ… é¢„æµ‹ç»“æžœå·²ä¿å­˜ä¸º submission.csv\n"}],"execution_count":14}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}