{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed060f0a-f92f-4894-a9ec-b6173a4e9638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®è¯»å–æˆåŠŸï¼\n"
     ]
    }
   ],
   "source": [
    "##rfç¬¬ä¸€æ­¥ï¼šæ•°æ®è¯»å–\n",
    "\n",
    "import pandas as pd\n",
    "file_path = \"C:/Users/HP/Desktop/final/train.csv\"\n",
    "\n",
    "# å°è¯•ä¸åŒç¼–ç æ ¼å¼\n",
    "try:\n",
    "    data = pd.read_csv(file_path, encoding=\"utf-8\")  # UTF-8 ç¼–ç \n",
    "except UnicodeDecodeError:\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, encoding=\"gbk\")  # GBK ç¼–ç ï¼ˆé€‚ç”¨äºä¸­æ–‡ï¼‰\n",
    "    except UnicodeDecodeError:\n",
    "        data = pd.read_csv(file_path, encoding=\"latin1\")  # Latin-1 ç¼–ç ï¼ˆé€‚ç”¨äºéƒ¨åˆ† ANSI æ–‡ä»¶ï¼‰\n",
    "print(\"æ•°æ®è¯»å–æˆåŠŸï¼\")\n",
    "\n",
    "# äº¤æ˜“æ—¶é—´å’Œå»ºç­‘é¢ç§¯çš„æ•°æ®æå–\n",
    "#data['äº¤æ˜“æ—¶é—´'] = pd.to_datetime(data['äº¤æ˜“æ—¶é—´'], format='%Y/%m/%d')  # è½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼\n",
    "#data['äº¤æ˜“æ—¶é—´'] = data['äº¤æ˜“æ—¶é—´'].dt.year  # æå–å¹´ä»½\n",
    "# å»æ‰ \"ã¡\" å¹¶è½¬æ¢ä¸º float ç±»å‹\n",
    "data['å»ºç­‘é¢ç§¯'] = data['å»ºç­‘é¢ç§¯'].str.replace('ã¡', '', regex=False).astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0291e36-c9bb-49e7-82e1-587052299d00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   åŒºåŸŸ        ä»·æ ¼    å»ºç­‘é¢ç§¯  é…å¤‡ç”µæ¢¯        äº¤æ˜“æ—¶é—´  äº¤æ˜“æƒå±  äº§æƒæ‰€å±  å‘¨è¾¹é…å¥—             å•ä»·  \\\n",
      "0  79   6564200   52.30     0   2020/8/11     1     0     1  125510.516252   \n",
      "1  43   4174000  127.44     0   2020/3/13     1     0     1   32752.667922   \n",
      "2  97  16310000  228.54     0  2020/11/21     1     0     0   71366.062834   \n",
      "3  62   2834600   43.60     1   2019/2/14     1     0     0   65013.761468   \n",
      "4  62   1954000   39.85     1   2019/4/26     1     0     0   49033.877039   \n",
      "\n",
      "      log_å•ä»·  ...  è£…ä¿®æƒ…å†µ_2  è£…ä¿®æƒ…å†µ_3  åˆ«å¢…ç±»å‹_1  åˆ«å¢…ç±»å‹_2  åˆ«å¢…ç±»å‹_3  æˆ¿å±‹ç”¨é€”_1  æˆ¿å±‹å¹´é™_1  \\\n",
      "0  11.740145  ...   False    True   False   False   False    True   False   \n",
      "1  10.396740  ...   False    True   False   False   False    True   False   \n",
      "2  11.175578  ...   False    True   False   False   False    True   False   \n",
      "3  11.082354  ...   False    True   False   False   False   False   False   \n",
      "4  10.800267  ...   False    True   False   False   False   False   False   \n",
      "\n",
      "   æˆ¿å±‹å¹´é™_2  æˆ¿å±‹å¹´é™_5  äº¤é€šå‡ºè¡Œ_1  \n",
      "0   False    True   False  \n",
      "1   False    True   False  \n",
      "2   False    True   False  \n",
      "3   False    True    True  \n",
      "4   False    True    True  \n",
      "\n",
      "[5 rows x 712 columns]\n"
     ]
    }
   ],
   "source": [
    "#ç¬¬äºŒæ­¥ï¼šæ•°æ®å¤„ç†\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 1ï¼šé€‰å–ç‰¹å¾å’Œç›®æ ‡å˜é‡\n",
    "df = data.copy()  # å‡è®¾ä½ å·²ç»å®Œæˆå‰é¢çš„å¼‚å¸¸å€¼å¤„ç†ç­‰\n",
    "df['å»ºç­‘é¢ç§¯'] = pd.to_numeric(df['å»ºç­‘é¢ç§¯'], errors='coerce')\n",
    "df['å•ä»·'] = df['ä»·æ ¼'] / df['å»ºç­‘é¢ç§¯']\n",
    "df['log_å•ä»·'] = np.log(df['å•ä»·'])\n",
    "df['log_é¢ç§¯'] = np.log(df['å»ºç­‘é¢ç§¯'] + 1)\n",
    "\n",
    "# 2ï¼šå¤„ç†åˆ†ç±»å˜é‡ï¼ˆç‹¬çƒ­ç¼–ç ï¼‰\n",
    "categorical_cols = ['åŸå¸‚', 'æ¿å—', 'åŒºåŸŸ', 'ç¯çº¿', 'æˆ¿å±‹æˆ·å‹', 'æ‰€åœ¨æ¥¼å±‚', 'æˆ¿å±‹æœå‘', 'è£…ä¿®æƒ…å†µ', 'log_é¢ç§¯', 'åˆ«å¢…ç±»å‹', 'æˆ¿å±‹ç”¨é€”', 'æˆ¿å±‹å¹´é™',   \n",
    "        'äº¤é€šå‡ºè¡Œ','å¹´ä»½']\n",
    "categorical_cols_to_encode = ['å¹´ä»½','åŸå¸‚', 'æ¿å—','ç¯çº¿', 'æˆ¿å±‹æˆ·å‹', 'æ‰€åœ¨æ¥¼å±‚', 'æˆ¿å±‹æœå‘', 'è£…ä¿®æƒ…å†µ','åˆ«å¢…ç±»å‹', 'æˆ¿å±‹ç”¨é€”', 'æˆ¿å±‹å¹´é™','äº¤é€šå‡ºè¡Œ' ]\n",
    "df = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b05651-dd52-423b-b787-c6d3a1c1b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¼‚å¸¸å€¼å¤„ç†\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# è®¡ç®—å»ºç­‘é¢ç§¯çš„ IQR å¹¶å»é™¤å¼‚å¸¸å€¼\n",
    "Q1_area = data['å»ºç­‘é¢ç§¯'].quantile(0.25)\n",
    "Q3_area = data['å»ºç­‘é¢ç§¯'].quantile(0.75)\n",
    "IQR_area = Q3_area - Q1_area\n",
    "lower_bound_area = Q1_area - 1.5 * IQR_area\n",
    "upper_bound_area = Q3_area + 1.5 * IQR_area\n",
    "\n",
    "# è®¡ç®—æˆ¿ä»·ï¼ˆå•ä½é¢ç§¯ä»·æ ¼ï¼‰çš„ IQR å¹¶å»é™¤å¼‚å¸¸å€¼\n",
    "Q1_price = data['ä»·æ ¼'].quantile(0.25)\n",
    "Q3_price = data['ä»·æ ¼'].quantile(0.75)\n",
    "IQR_price = Q3_price - Q1_price\n",
    "lower_bound_price = Q1_price - 1.5 * IQR_price\n",
    "upper_bound_price = Q3_price + 1.5 * IQR_price\n",
    "\n",
    "# è¿‡æ»¤æ•°æ®\n",
    "filtered_data = data[\n",
    "    (data['å»ºç­‘é¢ç§¯'] >= lower_bound_area) & (data['å»ºç­‘é¢ç§¯'] <= upper_bound_area) & \n",
    "    (data['ä»·æ ¼'] >= lower_bound_price) & (data['ä»·æ ¼'] <= upper_bound_price)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c799198-88d1-423b-b381-79348cab070e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       åŒºåŸŸ    log_é¢ç§¯  å¹´ä»½_2016  å¹´ä»½_2017  å¹´ä»½_2018  å¹´ä»½_2019  å¹´ä»½_2020  å¹´ä»½_2021  \\\n",
      "35981  68  4.753590    False    False    False    False    False     True   \n",
      "19903  67  4.939426    False    False    False    False    False     True   \n",
      "79323  93  4.254193    False    False    False    False    False    False   \n",
      "35652  65  4.509650    False    False    False    False    False     True   \n",
      "539    62  3.797734    False    False    False    False     True    False   \n",
      "\n",
      "       å¹´ä»½_2022   åŸå¸‚_1  ...  è£…ä¿®æƒ…å†µ_3  åˆ«å¢…ç±»å‹_1  åˆ«å¢…ç±»å‹_2  åˆ«å¢…ç±»å‹_3  æˆ¿å±‹ç”¨é€”_1  æˆ¿å±‹å¹´é™_1  \\\n",
      "35981    False  False  ...    True   False   False   False    True   False   \n",
      "19903    False   True  ...    True   False   False   False    True   False   \n",
      "79323     True  False  ...   False   False   False   False    True   False   \n",
      "35652    False  False  ...    True   False   False   False    True   False   \n",
      "539      False  False  ...    True   False   False   False   False   False   \n",
      "\n",
      "       æˆ¿å±‹å¹´é™_2  æˆ¿å±‹å¹´é™_5  äº¤é€šå‡ºè¡Œ_1  åŒºåŸŸ_encoded  \n",
      "35981   False   False    True    8.823686  \n",
      "19903    True   False    True    9.531486  \n",
      "79323    True   False    True    9.561751  \n",
      "35652    True   False    True    9.405939  \n",
      "539     False    True    True   11.111212  \n",
      "\n",
      "[5 rows x 704 columns]\n",
      "9.580087417395717\n"
     ]
    }
   ],
   "source": [
    "# ç¬¬ä¸‰æ­¥ï¼šè®­ç»ƒé›†åˆ’åˆ†\n",
    "#1ï¼šè®¾å®šç‰¹å¾Xå’Œç›®æ ‡y\n",
    "feature_cols = [col for col in df.columns if col.startswith('log_é¢ç§¯') or \n",
    "                col.startswith(tuple(categorical_cols)) or \n",
    "                col.startswith(tuple([c + \"_\" for c in categorical_cols]))]\n",
    "X = df[feature_cols]\n",
    "y = df['log_å•ä»·']\n",
    "y_region_mean = y.mean()\n",
    "\n",
    "\n",
    "# 2ï¼šè®­ç»ƒæµ‹è¯•é›†åˆ’åˆ†\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#3ï¼šç›®æ ‡ç¼–ç \n",
    "# è®¡ç®—è®­ç»ƒé›†ä¸­çš„åŒºåŸŸå‡å€¼\n",
    "region_mean_map = X_train.join(y_train).groupby('åŒºåŸŸ')['log_å•ä»·'].mean()\n",
    "# ç”¨è®­ç»ƒé›†å‡å€¼å¡«å……è®­ç»ƒé›†\n",
    "# åŒºåŸŸç›®æ ‡ç¼–ç ç»“æœå•ç‹¬ä¿å­˜\n",
    "#X_train_region_encoded = X_train['åŒºåŸŸ'].map(region_mean_map)\n",
    "# æŠŠå®ƒæ·»åŠ åˆ° X_trainï¼Œå†æ‰§è¡Œè¿™ä¸€è¡Œ\n",
    "#X_train['åŒºåŸŸ_encoded'] = X_train_region_encoded\n",
    "X_train['åŒºåŸŸ_encoded']=X_train['åŒºåŸŸ'].map(region_mean_map)\n",
    "\n",
    "# åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œç¼–ç æ—¶ï¼Œåªä½¿ç”¨è®­ç»ƒé›†ä¿¡æ¯\n",
    "X_test['åŒºåŸŸ_encoded'] = X_test['åŒºåŸŸ'].map(region_mean_map)\n",
    "\n",
    "print(X_train.head())\n",
    "print(y_region_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "dec40b37-e2ee-4cd8-ab4d-1d41a17e189c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\tools\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "243 fits failed out of a total of 486.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "125 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\tools\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\tools\\anaconda\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"D:\\tools\\anaconda\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"D:\\tools\\anaconda\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "118 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\tools\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\tools\\anaconda\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"D:\\tools\\anaconda\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"D:\\tools\\anaconda\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "D:\\tools\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [        nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan -0.14232668 -0.1452847  -0.14344638\n",
      " -0.14321828 -0.14649821 -0.14392245 -0.14261589 -0.14463865 -0.14367725\n",
      " -0.14258706 -0.14794773 -0.14567233 -0.14108194 -0.14562033 -0.14421629\n",
      " -0.14306032 -0.1447957  -0.14417866 -0.14296696 -0.14601822 -0.14460116\n",
      " -0.14296696 -0.14601822 -0.14460116 -0.14273374 -0.14591792 -0.14485602\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan -0.07015887 -0.07196833 -0.07178699\n",
      " -0.07078998 -0.0725533  -0.07213255 -0.07186097 -0.07376207 -0.07331666\n",
      " -0.07616912 -0.07664057 -0.07645491 -0.07617159 -0.07484941 -0.0751562\n",
      " -0.07523426 -0.07711966 -0.07636998 -0.07811185 -0.07934783 -0.07916215\n",
      " -0.07811185 -0.07934783 -0.07916215 -0.07836388 -0.07920573 -0.07924682\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan -0.04367028 -0.04409324 -0.04390457\n",
      " -0.04410489 -0.04471912 -0.04459978 -0.04591131 -0.04599326 -0.04581723\n",
      " -0.05275468 -0.05246715 -0.0522273  -0.05223222 -0.05290558 -0.05262665\n",
      " -0.05314111 -0.05348361 -0.05317404 -0.0610913  -0.06089777 -0.0605437\n",
      " -0.0610913  -0.06089777 -0.0605437  -0.05979153 -0.06052968 -0.06028692]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€ä½³å‚æ•°ç»„åˆï¼š {'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "#ç¬¬å››æ­¥ï¼šéšæœºæ£®æ—å‚æ•°é€‰æ‹©\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"æœ€ä½³å‚æ•°ç»„åˆï¼š\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "ff944407-b45c-4a58-8596-ac359cac11aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹è¯„ä¼°ç»“æœï¼šRMSE = 0.1569, RÂ² = 0.9583\n"
     ]
    }
   ],
   "source": [
    "# ç¬¬äº”æ­¥ï¼šè®­ç»ƒéšæœºæ£®æ—æ¨¡å‹\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=30, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# é¢„æµ‹ä¸è¯„ä¼°\n",
    "y_pred = rf.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"æ¨¡å‹è¯„ä¼°ç»“æœï¼šRMSE = {rmse:.4f}, RÂ² = {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "b59d6e72-42f0-4845-8abe-c9ea5aa3f668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç‰¹å¾é‡è¦æ€§å‰10åï¼š\n",
      "        Feature  Importance\n",
      "703  åŒºåŸŸ_encoded    0.858781\n",
      "1        log_é¢ç§¯    0.034364\n",
      "698      æˆ¿å±‹ç”¨é€”_1    0.011147\n",
      "677        ç¯çº¿_4    0.003927\n",
      "676        ç¯çº¿_3    0.003335\n",
      "0            åŒºåŸŸ    0.002837\n",
      "675        ç¯çº¿_2    0.002824\n",
      "693      è£…ä¿®æƒ…å†µ_2    0.002793\n",
      "213      æ¿å—_247    0.002365\n",
      "702      äº¤é€šå‡ºè¡Œ_1    0.001916\n"
     ]
    }
   ],
   "source": [
    "# ç¬¬å…­æ­¥ï¼šè¾“å‡ºç‰¹å¾é‡è¦æ€§\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"ç‰¹å¾é‡è¦æ€§å‰10åï¼š\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "7cf7281a-cb4b-48d7-9e27-5b0dfc1ed365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ€»ä»·é¢„æµ‹ç»“æœï¼šRMSE = 1202400.78, RÂ² = 0.8107\n"
     ]
    }
   ],
   "source": [
    "#ç¬¬ä¸ƒæ­¥.æ€»ä»·æ¨¡å‹è¯„ä¼°\n",
    "# 1ï¼šè¿˜åŸé¢„æµ‹å€¼å’Œå®é™…å€¼ï¼ˆä» log_å•ä»· -> å•ä»· -> æ€»ä»·ï¼‰\n",
    "y_pred_price = np.exp(y_pred)                      # è¿˜åŸä¸ºå•ä»·\n",
    "y_test_price = np.exp(y_test)                      # å®é™…å•ä»·\n",
    "\n",
    "area_test = np.exp(X_test['log_é¢ç§¯']) - 1          # è¿˜åŸå»ºç­‘é¢ç§¯ï¼ˆå¯¹åº” +1 çš„å˜æ¢ï¼‰\n",
    "\n",
    "y_pred_total_price = y_pred_price * area_test\n",
    "y_test_total_price = y_test_price * area_test\n",
    "\n",
    "# 2ï¼šè®¡ç®—æ€»ä»·çš„ RMSE å’Œ RÂ²\n",
    "rmse_total = np.sqrt(mean_squared_error(y_test_total_price, y_pred_total_price))\n",
    "r2_total = r2_score(y_test_total_price, y_pred_total_price)\n",
    "\n",
    "print(f\"æ€»ä»·é¢„æµ‹ç»“æœï¼šRMSE = {rmse_total:.2f}, RÂ² = {r2_total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "99252ccc-4850-4cb5-b68b-2f19e444451a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æŠ˜æ•° 1 â€”â€” æ€»ä»·é¢„æµ‹ RMSE = 659352.52, RÂ² = 0.9445\n",
      "æŠ˜æ•° 2 â€”â€” æ€»ä»·é¢„æµ‹ RMSE = 986578.66, RÂ² = 0.8532\n",
      "æŠ˜æ•° 3 â€”â€” æ€»ä»·é¢„æµ‹ RMSE = 616097.93, RÂ² = 0.9417\n",
      "æŠ˜æ•° 4 â€”â€” æ€»ä»·é¢„æµ‹ RMSE = 567175.23, RÂ² = 0.9509\n",
      "æŠ˜æ•° 5 â€”â€” æ€»ä»·é¢„æµ‹ RMSE = 514305.96, RÂ² = 0.9599\n",
      "æŠ˜æ•° 6 â€”â€” æ€»ä»·é¢„æµ‹ RMSE = 606308.57, RÂ² = 0.9505\n",
      "==================================================\n",
      "6 æŠ˜äº¤å‰éªŒè¯ æ€»ä»·é¢„æµ‹ RMSE å¹³å‡ = 658303.14ï¼Œæ ‡å‡†å·® = 168101.23\n",
      "6 æŠ˜äº¤å‰éªŒè¯ æ€»ä»·é¢„æµ‹ RÂ² å¹³å‡ = 0.9334ï¼Œæ ‡å‡†å·® = 0.0398\n"
     ]
    }
   ],
   "source": [
    "#6æŠ˜äº¤å‰éªŒè¯\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# â€”â€” 1. å‡†å¤‡ X_fullã€y_full â€”â€” \n",
    "feature_cols = [\n",
    "    col for col in df.columns\n",
    "    if col.startswith('log_é¢ç§¯')\n",
    "    or col.startswith(tuple(categorical_cols))\n",
    "    or col.startswith(tuple([c + \"_\" for c in categorical_cols]))\n",
    "]\n",
    "X_full = df[feature_cols].copy()    # è¿™é‡Œåº”ä¿è¯ X_full ä¸­åŒ…å«äº† 'åŒºåŸŸ' åˆ—\n",
    "y_full = df['log_å•ä»·'].copy()\n",
    "\n",
    "# è®¡ç®—æ•´ä½“ y çš„å‡å€¼ï¼Œä»¥ä¾¿ç»™é‚£äº›åœ¨è®­ç»ƒæŠ˜ä¸­æœªå‡ºç°çš„â€œåŒºåŸŸâ€èµ‹å€¼\n",
    "global_y_mean = y_full.mean()\n",
    "\n",
    "# â€”â€” 2. å®šä¹‰ 6 æŠ˜äº¤å‰éªŒè¯ â€”â€” \n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=42)\n",
    "\n",
    "# ç”¨æ¥ä¿å­˜æ¯ä¸€æŠ˜çš„æ€»ä»· RMSE å’Œ RÂ²\n",
    "rmse_list = []\n",
    "r2_list = []\n",
    "\n",
    "# â€”â€” 3. é€æŠ˜è®­ç»ƒä¸è¯„ä¼° â€”â€” \n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_full), start=1):\n",
    "    # 3.1 åˆ’åˆ†æœ¬æŠ˜çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "    X_train_fold = X_full.iloc[train_idx].copy()\n",
    "    X_val_fold   = X_full.iloc[val_idx].copy()\n",
    "    y_train_fold = y_full.iloc[train_idx].copy()\n",
    "    y_val_fold   = y_full.iloc[val_idx].copy()\n",
    "    \n",
    "    # 3.2 åœ¨æœ¬æŠ˜è®­ç»ƒé›†ä¸Šè®¡ç®—â€œåŒºåŸŸâ€ç›®æ ‡ç¼–ç æ˜ å°„\n",
    "    #     groupby åå¾—åˆ° Seriesï¼šindex æ˜¯åŒºåŸŸåç§°ï¼Œvalues æ˜¯è¯¥åŒºåŸŸåœ¨æœ¬æŠ˜è®­ç»ƒé›†ä¸Šçš„ log_å•ä»· å¹³å‡å€¼\n",
    "    region_mean_map = (\n",
    "        X_train_fold[['åŒºåŸŸ']]\n",
    "        .join(y_train_fold)\n",
    "        .groupby('åŒºåŸŸ')['log_å•ä»·']\n",
    "        .mean()\n",
    "    )\n",
    "    \n",
    "    # 3.3 æŠŠæœ¬æŠ˜è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­ 'åŒºåŸŸ' æ˜ å°„æˆ 'åŒºåŸŸ_encoded'\n",
    "    X_train_fold['åŒºåŸŸ_encoded'] = X_train_fold['åŒºåŸŸ'].map(region_mean_map)\n",
    "    X_val_fold  ['åŒºåŸŸ_encoded'] = X_val_fold ['åŒºåŸŸ'].map(region_mean_map)\n",
    "    \n",
    "    # å¯¹äºéªŒè¯é›†ä¸­å‡ºç°è®­ç»ƒé›†ä¸­æ²¡æœ‰çš„â€œåŒºåŸŸâ€ï¼Œç”¨è®­ç»ƒé›† y çš„æ•´ä½“å¹³å‡å€¼æ¥å¡«å……\n",
    "    X_train_fold['åŒºåŸŸ_encoded'] = X_train_fold['åŒºåŸŸ_encoded'].fillna(y_train_fold.mean())\n",
    "    X_val_fold  ['åŒºåŸŸ_encoded'] = X_val_fold ['åŒºåŸŸ_encoded'].fillna(y_train_fold.mean())\n",
    "    \n",
    "    # 3.4 å¦‚æœ feature_cols ä¸­å·²ç»åŒ…å«äº† 'åŒºåŸŸ_encoded'ï¼Œåˆ™æ— é¡»å†æ¬¡æ·»åŠ åˆ°åˆ—è¡¨ä¸­\n",
    "    #     å¦åˆ™ï¼Œéœ€è¦åœ¨ feature_cols_fold ä¸­åŠ å…¥ 'åŒºåŸŸ_encoded'\n",
    "    feature_cols_fold = list(feature_cols)\n",
    "    if 'åŒºåŸŸ_encoded' not in feature_cols_fold:\n",
    "        feature_cols_fold.append('åŒºåŸŸ_encoded')\n",
    "    \n",
    "    # 3.5 è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹\n",
    "    rf_cv = RandomForestRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=30, \n",
    "        random_state=42\n",
    "    )\n",
    "    rf_cv.fit(\n",
    "        X_train_fold[feature_cols_fold],\n",
    "        y_train_fold\n",
    "    )\n",
    "    \n",
    "    # 3.6 åœ¨éªŒè¯é›†ä¸Šé¢„æµ‹ log_å•ä»·\n",
    "    y_val_pred_log = rf_cv.predict(X_val_fold[feature_cols_fold])\n",
    "    \n",
    "    # â€”â€” 4. è®¡ç®—æ€»ä»·çš„ RMSE ä¸ RÂ² â€”â€” \n",
    "    # 4.1 å…ˆæŠŠ log_å•ä»· â†’ å•ä»·ï¼Œç„¶åå†è®¡ç®—â€œæ€»ä»·â€ï¼šå•ä»· * é¢ç§¯\n",
    "    y_val_pred_price = np.exp(y_val_pred_log)      # è¿˜åŸä¸º â€œå•ä»·â€\n",
    "    y_val_price      = np.exp(y_val_fold)          # éªŒè¯é›†çœŸå® â€œå•ä»·â€\n",
    "    \n",
    "    # å»ºç­‘é¢ç§¯ä¹Ÿæ˜¯ log è¿‡ +1 çš„ï¼šlog_é¢ç§¯ = np.log(é¢ç§¯ + 1)\n",
    "    # æ‰€ä»¥è¿˜åŸé¢ç§¯ï¼šarea = exp(log_é¢ç§¯) - 1\n",
    "    area_val = np.exp(X_val_fold['log_é¢ç§¯']) - 1\n",
    "    \n",
    "    y_val_pred_total = y_val_pred_price * area_val\n",
    "    y_val_total      = y_val_price      * area_val\n",
    "    \n",
    "    # 4.2 è®¡ç®—æœ¬æŠ˜çš„ RMSE å’Œ RÂ²\n",
    "    rmse_fold = np.sqrt(mean_squared_error(y_val_total, y_val_pred_total))\n",
    "    r2_fold   = r2_score(y_val_total, y_val_pred_total)\n",
    "    \n",
    "    print(f\"æŠ˜æ•° {fold_idx} â€”â€” æ€»ä»·é¢„æµ‹ RMSE = {rmse_fold:.2f}, RÂ² = {r2_fold:.4f}\")\n",
    "    \n",
    "    rmse_list.append(rmse_fold)\n",
    "    r2_list.append(r2_fold)\n",
    "\n",
    "# â€”â€” 5. è¾“å‡ºäº¤å‰éªŒè¯æ•´ä½“ç»“æœ â€”â€” \n",
    "mean_rmse = np.mean(rmse_list)\n",
    "std_rmse  = np.std(rmse_list, ddof=1)\n",
    "mean_r2   = np.mean(r2_list)\n",
    "std_r2    = np.std(r2_list, ddof=1)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"6 æŠ˜äº¤å‰éªŒè¯ æ€»ä»·é¢„æµ‹ RMSE å¹³å‡ = {mean_rmse:.2f}ï¼Œæ ‡å‡†å·® = {std_rmse:.2f}\")\n",
    "print(f\"6 æŠ˜äº¤å‰éªŒè¯ æ€»ä»·é¢„æµ‹ RÂ² å¹³å‡ = {mean_r2:.4f}ï¼Œæ ‡å‡†å·® = {std_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "8cb0762d-f63c-4b12-85bb-c34306b2d48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   åŒºåŸŸ    log_é¢ç§¯  å¹´ä»½_2016  å¹´ä»½_2017  å¹´ä»½_2018  å¹´ä»½_2019  å¹´ä»½_2020  å¹´ä»½_2021  \\\n",
      "0  45  5.348059        0        0        0        0        0        0   \n",
      "1  45  5.104065        0        0        0        0        0        0   \n",
      "2  43  4.643621        0        0        0        0        0        0   \n",
      "3  39  4.706462        0        0        0        0        0        0   \n",
      "4  79  4.063885        0        0        0        0        0        0   \n",
      "\n",
      "   å¹´ä»½_2022   åŸå¸‚_1  ...  è£…ä¿®æƒ…å†µ_3  åˆ«å¢…ç±»å‹_1  åˆ«å¢…ç±»å‹_2  åˆ«å¢…ç±»å‹_3  æˆ¿å±‹ç”¨é€”_1  æˆ¿å±‹å¹´é™_1  \\\n",
      "0        0  False  ...    True   False   False   False    True   False   \n",
      "1        0  False  ...    True   False   False   False    True   False   \n",
      "2        0  False  ...   False   False   False   False    True   False   \n",
      "3        0  False  ...   False   False   False   False    True   False   \n",
      "4        0  False  ...   False   False   False   False    True   False   \n",
      "\n",
      "   æˆ¿å±‹å¹´é™_2  æˆ¿å±‹å¹´é™_5  äº¤é€šå‡ºè¡Œ_1  åŒºåŸŸ_encoded  \n",
      "0    True   False   False   10.846593  \n",
      "1   False    True    True   10.846593  \n",
      "2   False    True   False   10.486310  \n",
      "3   False    True    True   10.096055  \n",
      "4   False    True    True   11.405375  \n",
      "\n",
      "[5 rows x 704 columns]\n"
     ]
    }
   ],
   "source": [
    "##é¢„æµ‹ï¼š1.æ•°æ®å¤„ç†ï¼š\n",
    "#ç¬¬å…«æ­¥ï¼šè¯»å–é¢„æµ‹æ•°æ®\n",
    "test_path = \"C:/Users/HP/Desktop/final/test_.xlsx\"\n",
    "test_df = pd.read_excel(test_path)\n",
    "\n",
    "# æ¸…æ´—å»ºç­‘é¢ç§¯å­—æ®µ\n",
    "test_df['å»ºç­‘é¢ç§¯'] = test_df['å»ºç­‘é¢ç§¯'].astype(str).str.replace('ã¡', '', regex=False).astype(float)\n",
    "# è®°å½•åŸå§‹å»ºç­‘é¢ç§¯ç”¨äºåè¿˜åŸä»·æ ¼\n",
    "test_df['åŸå§‹é¢ç§¯'] = test_df['å»ºç­‘é¢ç§¯']\n",
    "\n",
    "# æ„é€  log_é¢ç§¯ï¼ˆä¸è®­ç»ƒé›†ä¸€è‡´ï¼‰\n",
    "test_df['log_é¢ç§¯'] = np.log(test_df['å»ºç­‘é¢ç§¯'] + 1)\n",
    "\n",
    "# ä¿ç•™è®­ç»ƒä¸­ä½¿ç”¨çš„æ‰€æœ‰ç‰¹å¾åˆ—\n",
    "test_categorical_cols = [\n",
    "    'åŸå¸‚', 'æ¿å—', 'åŒºåŸŸ', 'ç¯çº¿', 'æˆ¿å±‹æˆ·å‹', 'æ‰€åœ¨æ¥¼å±‚',\n",
    "    'æˆ¿å±‹æœå‘', 'è£…ä¿®æƒ…å†µ', 'log_é¢ç§¯', 'åˆ«å¢…ç±»å‹',\n",
    "    'æˆ¿å±‹ç”¨é€”', 'æˆ¿å±‹å¹´é™', 'äº¤é€šå‡ºè¡Œ','å¹´ä»½'\n",
    "]\n",
    "test_df = test_df[test_categorical_cols]\n",
    "\n",
    "# ä¸è®­ç»ƒé›†ç›¸åŒçš„ç‹¬çƒ­ç¼–ç \n",
    "test_df = pd.get_dummies(test_df, columns=['å¹´ä»½','åŸå¸‚', 'æ¿å—','ç¯çº¿', 'æˆ¿å±‹æˆ·å‹', 'æ‰€åœ¨æ¥¼å±‚', 'æˆ¿å±‹æœå‘', 'è£…ä¿®æƒ…å†µ','åˆ«å¢…ç±»å‹', 'æˆ¿å±‹ç”¨é€”', 'æˆ¿å±‹å¹´é™','äº¤é€šå‡ºè¡Œ'], drop_first=True)\n",
    "\n",
    "# åŒºåŸŸç›®æ ‡ç¼–ç \n",
    "#test_df['åŒºåŸŸ_encoded'] = X_train_region_encoded\n",
    "# åŒºåŸŸç›®æ ‡ç¼–ç ï¼ˆç”¨è®­ç»ƒé›†æ˜ å°„ï¼‰\n",
    "#test_df['åŒºåŸŸ_encoded'] = X_train['åŒºåŸŸ'].map(region_mean_map)\n",
    "test_df['åŒºåŸŸ_encoded'] = test_df['åŒºåŸŸ'].map(region_mean_map)\n",
    "# æ›¿æ¢ NaN ä¸º y_region_mean\n",
    "#test_df['åŒºåŸŸ_encoded'] = X_train['åŒºåŸŸ_encoded'].fillna(y_region_mean)\n",
    "test_df['åŒºåŸŸ_encoded'] = test_df['åŒºåŸŸ_encoded'].fillna(y_region_mean)\n",
    "\n",
    "# è¡¥å…¨ç¼ºå¤±åˆ—\n",
    "for col in X.columns:\n",
    "    if col not in test_df.columns:\n",
    "        test_df[col] = 0\n",
    "\n",
    "# å¤šä½™åˆ—åˆ é™¤ï¼ˆå¦‚æœæµ‹è¯•é›†ç¼–ç ç”Ÿæˆäº†è®­ç»ƒé›†ä¸­æ²¡æœ‰çš„åˆ—ï¼‰\n",
    "#test_df = test_df[X.columns]  # åªä¿ç•™è®­ç»ƒé›†ç”¨åˆ°çš„åˆ—\n",
    "\n",
    "# ç¡®ä¿åˆ—é¡ºåºä¸€è‡´\n",
    "test_df = test_df[X_train.columns]\n",
    "\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "5f762600-82be-46af-9ba6-17710d58e60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: C:/Users/HP/Desktop/final/prediction_result_rf_3.csv\n"
     ]
    }
   ],
   "source": [
    "#2. è¿›è¡Œé¢„æµ‹ï¼ˆé¢„æµ‹log_å•ä»·ï¼‰\n",
    "test_rf_log_unit_price = rf.predict(test_df)\n",
    "\n",
    "# åå˜æ¢å¾—åˆ°æ€»ä»·\n",
    "test_price_rf = np.exp(test_rf_log_unit_price) * test_df['log_é¢ç§¯'].apply(lambda x: np.exp(x) - 1)\n",
    "\n",
    "# å‡†å¤‡ä¿å­˜çš„ç»“æœ\n",
    "result_df_rf = pd.DataFrame({\n",
    "    \"ID\": range(len(test_price_rf)),\n",
    "    \"Price\": test_price_rf  # ä¿ç•™åŸå§‹æµ®ç‚¹æ•°\n",
    "})\n",
    "\n",
    "# ä¿å­˜ç»“æœä¸ºCSV\n",
    "output_path = \"C:/Users/HP/Desktop/final/prediction_result_rf_3.csv\"\n",
    "result_df_rf.to_csv(output_path, index=False)\n",
    "print(f\"é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3435964-2748-4e5b-bc98-9d3235e9a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##xgboostï¼š1.å‚æ•°é€‰å–\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# å‡è®¾å·²æœ‰ X, y\n",
    "# X, y = ...\n",
    "\n",
    "# æ‹†åˆ†è®­ç»ƒ/æµ‹è¯•é›†ï¼ˆæ³¨æ„å‘½åä¸èƒ½æ··ä¹±ï¼‰\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'booster': 'gbtree',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'random_state': 42  # ä¿4è¯æ¯æ¬¡ç»“æœä¸€è‡´\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(**param)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        #early_stopping_rounds=30,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    return rmse\n",
    "\n",
    "# åˆ›å»º Study å¯¹è±¡\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# å¯åŠ¨æœç´¢\n",
    "study.optimize(objective, n_trials=50, timeout=600)  # è¿è¡Œæœ€å¤š5åˆ†é’Ÿæˆ–20è½®\n",
    "\n",
    "# æ‰“å°ç»“æœ\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c020d2-03b0-481d-81d0-16802899bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params\n",
    "model = xgb.XGBRegressor(**best_params)\n",
    "model.fit(X_train, y_train)  # ç”¨è®­ç»ƒé›†æ•°æ®é‡æ–°è®­å¾—åˆ°æ¨¡å‹è¯„ä¼°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52073d47-ce64-4548-97fa-5651baa16831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.æ¨¡å‹è¯„ä¼°\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"XGBoostæ¨¡å‹è¯„ä¼°ç»“æœï¼šRMSE = {rmse:.4f}, RÂ² = {r2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190a0cf-2fdc-4c7e-a658-5b3805d79428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.ç‰¹å¾é‡è¦æ€§\n",
    "import pandas as pd\n",
    "\n",
    "# è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆä½¿ç”¨ gain ä½œä¸ºåº¦é‡ï¼‰\n",
    "importance_dict = model.get_booster().get_score(importance_type='gain')\n",
    "\n",
    "# è½¬æ¢ä¸º DataFrame å¹¶æ’åº\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': list(importance_dict.keys()),\n",
    "    'Gain': list(importance_dict.values())\n",
    "}).sort_values(by='Gain', ascending=False)\n",
    "\n",
    "# è¾“å‡ºç»“æœ\n",
    "print(\"\\nğŸ“Š ç‰¹å¾é‡è¦æ€§ï¼ˆæŒ‰ Gain æ’åºï¼‰ï¼š\")\n",
    "print(importance_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6126450d-847e-436f-a3be-fb1fbd12bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.æ€»ä»·æ¨¡å‹è¯„ä¼°\n",
    "# ç¬¬ä¸€æ­¥ï¼šè¿˜åŸé¢„æµ‹å€¼å’Œå®é™…å€¼ï¼ˆä» log_å•ä»· -> å•ä»· -> æ€»ä»·ï¼‰\n",
    "y_pred_price = np.exp(y_pred)                      # è¿˜åŸä¸ºå•ä»·\n",
    "y_test_price = np.exp(y_test)                      # å®é™…å•ä»·\n",
    "\n",
    "area_test = np.exp(X_test['log_é¢ç§¯']) - 1          # è¿˜åŸå»ºç­‘é¢ç§¯ï¼ˆå¯¹åº” +1 çš„å˜æ¢ï¼‰\n",
    "\n",
    "y_pred_total_price = y_pred_price * area_test\n",
    "y_test_total_price = y_test_price * area_test\n",
    "\n",
    "# ç¬¬äºŒæ­¥ï¼šè®¡ç®—æ€»ä»·çš„ RMSE å’Œ RÂ²\n",
    "rmse_total = np.sqrt(mean_squared_error(y_test_total_price, y_pred_total_price))\n",
    "r2_total = r2_score(y_test_total_price, y_pred_total_price)\n",
    "\n",
    "print(f\"æ€»ä»·é¢„æµ‹ç»“æœï¼šRMSE = {rmse_total:.2f}, RÂ² = {r2_total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99042a5-8ab7-4548-988e-60d0acaccf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "##é¢„æµ‹ï¼š1.æ•°æ®å¤„ç†ï¼š\n",
    "#ç¬¬å…«æ­¥ï¼šè¯»å–é¢„æµ‹æ•°æ®\n",
    "test_path = \"C:/Users/HP/Desktop/final/test_.xlsx\"\n",
    "test_df = pd.read_excel(test_path)\n",
    "\n",
    "# æ¸…æ´—å»ºç­‘é¢ç§¯å­—æ®µ\n",
    "test_df['å»ºç­‘é¢ç§¯'] = test_df['å»ºç­‘é¢ç§¯'].astype(str).str.replace('ã¡', '', regex=False).astype(float)\n",
    "# è®°å½•åŸå§‹å»ºç­‘é¢ç§¯ç”¨äºåè¿˜åŸä»·æ ¼\n",
    "test_df['åŸå§‹é¢ç§¯'] = test_df['å»ºç­‘é¢ç§¯']\n",
    "\n",
    "# æ„é€  log_é¢ç§¯ï¼ˆä¸è®­ç»ƒé›†ä¸€è‡´ï¼‰\n",
    "test_df['log_é¢ç§¯'] = np.log(test_df['å»ºç­‘é¢ç§¯'] + 1)\n",
    "\n",
    "# ä¿ç•™è®­ç»ƒä¸­ä½¿ç”¨çš„æ‰€æœ‰ç‰¹å¾åˆ—\n",
    "test_categorical_cols = [\n",
    "    'åŸå¸‚', 'æ¿å—', 'åŒºåŸŸ', 'ç¯çº¿', 'æˆ¿å±‹æˆ·å‹', 'æ‰€åœ¨æ¥¼å±‚',\n",
    "    'æˆ¿å±‹æœå‘', 'è£…ä¿®æƒ…å†µ', 'log_é¢ç§¯', 'åˆ«å¢…ç±»å‹',\n",
    "    'æˆ¿å±‹ç”¨é€”', 'æˆ¿å±‹å¹´é™', 'äº¤é€šå‡ºè¡Œ','å¹´ä»½'\n",
    "]\n",
    "test_df = test_df[test_categorical_cols]\n",
    "\n",
    "# ä¸è®­ç»ƒé›†ç›¸åŒçš„ç‹¬çƒ­ç¼–ç \n",
    "test_df = pd.get_dummies(test_df, columns=['åŸå¸‚', 'æ¿å—'], drop_first=True)\n",
    "\n",
    "# åŒºåŸŸç›®æ ‡ç¼–ç \n",
    "#test_df['åŒºåŸŸ_encoded'] = X_train_region_encoded\n",
    "# åŒºåŸŸç›®æ ‡ç¼–ç ï¼ˆç”¨è®­ç»ƒé›†æ˜ å°„ï¼‰\n",
    "#test_df['åŒºåŸŸ_encoded'] = X_train['åŒºåŸŸ'].map(region_mean_map)\n",
    "test_df['åŒºåŸŸ_encoded'] = test_df['åŒºåŸŸ'].map(region_mean_map)\n",
    "# æ›¿æ¢ NaN ä¸º y_region_mean\n",
    "#test_df['åŒºåŸŸ_encoded'] = X_train['åŒºåŸŸ_encoded'].fillna(y_region_mean)\n",
    "test_df['åŒºåŸŸ_encoded'] = test_df['åŒºåŸŸ_encoded'].fillna(y_region_mean)\n",
    "\n",
    "# è¡¥å…¨ç¼ºå¤±åˆ—\n",
    "for col in X.columns:\n",
    "    if col not in test_df.columns:\n",
    "        test_df[col] = 0\n",
    "\n",
    "# å¤šä½™åˆ—åˆ é™¤ï¼ˆå¦‚æœæµ‹è¯•é›†ç¼–ç ç”Ÿæˆäº†è®­ç»ƒé›†ä¸­æ²¡æœ‰çš„åˆ—ï¼‰\n",
    "#test_df = test_df[X.columns]  # åªä¿ç•™è®­ç»ƒé›†ç”¨åˆ°çš„åˆ—\n",
    "\n",
    "# ç¡®ä¿åˆ—é¡ºåºä¸€è‡´\n",
    "test_df = test_df[X_train.columns]\n",
    "\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142d5c3-dfa5-4374-aa63-c9144b0dbf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. è¿›è¡Œé¢„æµ‹ï¼ˆé¢„æµ‹log_å•ä»·ï¼‰\n",
    "test_xgboost_log_unit_price = model.predict(test_df)\n",
    "\n",
    "# åå˜æ¢å¾—åˆ°æ€»ä»·\n",
    "test_price_xgboost = np.exp(test_xgboost_log_unit_price) * test_df['log_é¢ç§¯'].apply(lambda x: np.exp(x) - 1)\n",
    "\n",
    "# å‡†å¤‡ä¿å­˜çš„ç»“æœ\n",
    "result_df_xgboost = pd.DataFrame({\n",
    "    \"ID\": range(len(test_price_xgboost)),\n",
    "    \"Price\": test_price_xgboost  # ä¿ç•™åŸå§‹æµ®ç‚¹æ•°\n",
    "})\n",
    "\n",
    "# ä¿å­˜ç»“æœä¸ºCSV\n",
    "output_path = \"C:/Users/HP/Desktop/final/prediction_result_xg_3.csv\"\n",
    "result_df_xgboost.to_csv(output_path, index=False)\n",
    "print(f\"é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a8654-c486-4485-9d05-24fc840048dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# --------- 1. æ•°å€¼æ½œåœ¨ç‰¹å¾è¾“å…¥ ---------\n",
    "num_latent_input = Input(shape=(encoding_dim,), name=\"numeric_latent\")\n",
    "\n",
    "# --------- 2. æ¿å— Embedding è¾“å…¥ ---------\n",
    "#  å‡è®¾ le_block.classes_ æ•°é‡ä¸º num_blocks ä¸ªï¼ˆå³æ¿å—ç±»åˆ«æ•°ï¼‰\n",
    "num_blocks = len(le_block.classes_)\n",
    "\n",
    "block_input = Input(shape=(1,), name=\"block_id\")  # ä¼ å…¥ä¸€ä¸ªæ•´æ•° ID\n",
    "# Embedding å±‚å°†æ¿å— IDï¼ˆ0..num_blocks-1ï¼‰æ˜ å°„åˆ° 4 ç»´å‘é‡\n",
    "block_embedding = Embedding(\n",
    "    input_dim=num_blocks,\n",
    "    output_dim=4,          # 4 ç»´åµŒå…¥ï¼Œä¹Ÿå¯è°ƒ\n",
    "    #input_length=1,\n",
    "    name=\"block_embedding\"\n",
    ")(block_input)\n",
    "block_vec = Flatten(name=\"block_flatten\")(block_embedding)  # å±•å¹³æˆ (None, 4)\n",
    "\n",
    "# --------- 3. æ‰€æœ‰ one-hot è¾“å…¥ ---------\n",
    "cat_input = Input(shape=(X_cat_train.shape[1],), name=\"categorical_onehot\")\n",
    "\n",
    "\n",
    "# --------- 4. æ‹¼æ¥æ‰€æœ‰ç‰¹å¾ ---------\n",
    "concat = Concatenate(name=\"concat_all\")(\n",
    "    [num_latent_input, block_vec, cat_input]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# --------- 5. å…¨è¿æ¥å±‚åšå›å½’ ---------\n",
    "x = Dense(32, activation='relu', name=\"dense_1\")(concat)\n",
    "x = Dense(16, activation='relu', name=\"dense_2\")(x)\n",
    "output = Dense(1, activation='linear', name=\"price_output\")(x)\n",
    "\n",
    "# --------- 6. å®šä¹‰æ¨¡å‹ ---------\n",
    "model = Model(\n",
    "    inputs=[num_latent_input, block_input, cat_input],\n",
    "    outputs=output\n",
    ")\n",
    "\n",
    "# æ‰“å°æ¨¡å‹ç»“æ„ï¼Œæ£€æŸ¥æ— è¯¯\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45268fb-82a1-45a0-afc8-f397ce09b4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',            # å‡æ–¹è¯¯å·®\n",
    "    metrics=['mae']        # å¹³å‡ç»å¯¹è¯¯å·®\n",
    ")\n",
    "\n",
    "# å‡†å¤‡è®­ç»ƒæ—¶è¦ä¼ ç»™ model.fit çš„è¾“å…¥æ ¼å¼\n",
    "train_inputs = {\n",
    "    \"numeric_latent\": X_num_train_latent,      # Autoencoder Latent (n_train, 2)\n",
    "    \"block_id\": X_block_train,                 # æ¿å— ID (n_train,)\n",
    "    \"categorical_onehot\": X_cat_train          # æ‰€æœ‰ one-hot ç¼–ç ç‰¹å¾ (n_train,  ç‹¬çƒ­ç»´åº¦)\n",
    "}\n",
    "test_inputs = {\n",
    "    \"numeric_latent\": X_num_test_latent,\n",
    "    \"block_id\": X_block_test,\n",
    "    \"categorical_onehot\": X_cat_test\n",
    "}\n",
    "\n",
    "# è®­ç»ƒ\n",
    "history = model.fit(\n",
    "    train_inputs,\n",
    "    y_train,\n",
    "    validation_split=0.1,  # ç•™ä¸€éƒ¨åˆ†åšéªŒè¯\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09180fe3-d098-465b-80ff-b76735ed4ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹\n",
    "y_pred = model.predict(test_inputs)\n",
    "\n",
    "# å¦‚æœä½ çš„ y_test æ˜¯â€œlog_å•ä»·â€ï¼Œå¯ä»¥è®¡ç®— RMSEã€R2\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2  = r2_score(y_test, y_pred)\n",
    "print(f\"æµ‹è¯•é›† RMSE = {rmse:.4f}, RÂ² = {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c4157f-ca71-4173-9c65-9fc9bcc741b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- è¯»å–æµ‹è¯•é›†å¹¶å®Œæˆä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ•°æ®æ¸…æ´— ----------\n",
    "\n",
    "test_path = \"C:/Users/HP/Desktop/final/test_.xlsx\"\n",
    "df_test = pd.read_excel(test_path)\n",
    "\n",
    "# ï¼ˆ1ï¼‰å»æ‰â€œã¡â€å¹¶è½¬æ¢æˆ float\n",
    "df_test['å»ºç­‘é¢ç§¯'] = (\n",
    "    df_test['å»ºç­‘é¢ç§¯']\n",
    "    .astype(str)\n",
    "    .str.replace('ã¡', '', regex=False)\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# ï¼ˆ2ï¼‰æ„é€  log_é¢ç§¯ï¼ˆä¸è®­ç»ƒé€»è¾‘ä¸€è‡´ï¼‰\n",
    "df_test['log_é¢ç§¯'] = np.log(df_test['å»ºç­‘é¢ç§¯'] + 1)\n",
    "\n",
    "# ï¼ˆ3ï¼‰å…¶å®ƒæ•°å€¼ç‰¹å¾ï¼šä¸€å®šè¦å’Œè®­ç»ƒæ—¶ç”¨çš„ numeric_cols ä¸€è‡´\n",
    "numeric_cols = ['log_é¢ç§¯', 'ç¯çº¿', 'æˆ¿å±‹æˆ·å‹', 'æ‰€åœ¨æ¥¼å±‚']\n",
    "X_num_test = df_test[numeric_cols].values  # å½¢çŠ¶ï¼š(n_test, 4)\n",
    "\n",
    "# ï¼ˆ4ï¼‰æ¿å— IDï¼šå¿…é¡»ç”¨ è®­ç»ƒé˜¶æ®µ çš„ le_block å» transform\n",
    "#     ï¼ˆå¦‚æœæŸäº›æ¿å—åœ¨æµ‹è¯•é›†é‡Œæ²¡å‡ºç°ï¼Œtransform ä¼šæŠ¥é”™ â€”â€” ç¡®ä¿ le_block æ˜¯è®­ç»ƒæ—¶ fit åçš„é‚£ä¸ªï¼‰\n",
    "# å…ˆæŸ¥çœ‹ä¸€ä¸‹è®­ç»ƒæ—¶ le_block å­¦åˆ°çš„ç±»åˆ«åˆ—è¡¨\n",
    "seen_blocks = set(le_block.classes_)\n",
    "# å…ˆæå–åŸå§‹æ¿å—æ ‡ç­¾\n",
    "raw_blocks = df_test['æ¿å—'].astype(str).values\n",
    "\n",
    "# ï¼ˆ2ï¼‰æŠŠæµ‹è¯•é›†ä¸­è®­ç»ƒæ—¶æ²¡è§è¿‡çš„æ ‡ç­¾ï¼Œå…ˆæ›¿æ¢æˆä¸€ä¸ªâ€œæœªçŸ¥â€å ä½ç¬¦\n",
    "# è¿™é‡ŒæŠŠæ‰€æœ‰ unseen-label éƒ½è®¾ä¸º 'UNKNOWN'\n",
    "df_test['æ¿å—_clean'] = df_test['æ¿å—'].astype(str).map(\n",
    "    lambda x: x if x in seen_blocks else 'UNKNOWN'\n",
    ")\n",
    "\n",
    "# ï¼ˆ3ï¼‰åœ¨ LabelEncoder é‡Œä¸´æ—¶æŠŠ 'UNKNOWN' ä¹Ÿå½“æˆä¸€ç±»ï¼š  \n",
    "#     æ–¹æ³• Aï¼šç›´æ¥åœ¨ transform å‰æŠŠ unseen æ˜ å°„åˆ°ä¸€ä¸ªå·²æœ‰ç±»åˆ«ï¼Œæ¯”å¦‚ç¬¬ 0 ä¸ª\n",
    "#     æ–¹æ³• Bï¼šæˆ–è€…ç»™ le_block.classes_ é‡Œæ–°å¢ 'UNKNOWN'ï¼ˆä½†éœ€è¦é‡æ–° fitï¼‰\n",
    "#\n",
    "# æˆ‘ä»¬ç”¨ æ–¹æ³• Aï¼ŒæŠŠæ‰€æœ‰ unseen ç›´æ¥æ˜ å°„åˆ°å·²çŸ¥çš„åºå· 0\n",
    "# å³ä½¿ 0 ä»£è¡¨çš„åŸæ¥æ˜¯æŸä¸ªå®é™…æ¿å—ï¼Œä½†è¿™æ ·è¾ƒç®€å•\n",
    "\n",
    "# å…ˆæ‹¿åˆ°è®­ç»ƒæ—¶ le_block ç»™æ¯ä¸ªæ ‡ç­¾çš„ç¼–ç ï¼ˆå­—å…¸å½¢å¼ï¼‰\n",
    "mapping = {lbl: idx for idx, lbl in enumerate(le_block.classes_)}\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼šå¦‚æœæ ‡ç­¾æ²¡åœ¨ mapping é‡Œï¼Œå°±è¿”å› 0\n",
    "def safe_encode(x):\n",
    "    return mapping[x] if x in mapping else 0\n",
    "\n",
    "# æŠŠæµ‹è¯•é›†â€œæ¿å—_cleanâ€æ˜ å°„åˆ°æ•´æ•°\n",
    "df_test['æ¿å—_id'] = df_test['æ¿å—_clean'].map(safe_encode).astype(int)\n",
    "\n",
    "# è¿™æ · df_test['æ¿å—_id'] ä¸­æ‰€æœ‰åŸæœ¬ unseen çš„éƒ½ä¼šæ˜¯ 0\n",
    "X_block_test = df_test['æ¿å—_id'].values  # (n_test,)\n",
    "\n",
    "# ï¼ˆ5ï¼‰one-hot ç¼–ç åˆ—ï¼šå¿…é¡»åœ¨è®­ç»ƒæ—¶ç”¨åˆ°çš„ onehot_cols åˆ—åï¼Œåœ¨æµ‹è¯•é›†ä¸­ä¸€ä¸€æå–\n",
    "#     å¯¹ä¸å­˜åœ¨çš„åˆ—è¡¥ 0\n",
    "X_cat_test_list = []\n",
    "for col in onehot_cols:  # onehot_cols éœ€ä»è®­ç»ƒè„šæœ¬ä¸­ä¼ é€’è¿‡æ¥\n",
    "    if col in df_test.columns:\n",
    "        arr = df_test[col].values.reshape(-1, 1)  # å½¢çŠ¶ï¼š(n_test,1)\n",
    "    else:\n",
    "        arr = np.zeros((df_test.shape[0], 1), dtype=float)\n",
    "    X_cat_test_list.append(arr)\n",
    "\n",
    "# æŠŠæ‰€æœ‰ one-hot åˆ—æ°´å¹³æ‹¼æ¥æˆ ndarray\n",
    "X_cat_test = np.hstack(X_cat_test_list)  # å½¢çŠ¶ï¼š(n_test, len(onehot_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d6640-3dd0-4c06-9714-812cf0fbf460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒé˜¶æ®µ fit å¥½çš„ scalerã€encoder éƒ½è¦åœ¨æ­¤æ—¶ç›´æ¥å¤ç”¨\n",
    "# scaler = StandardScaler().fit(X_num_train)  \n",
    "# encoder = Model(inputs=num_input, outputs=encoded)\n",
    "\n",
    "# (1) æ ‡å‡†åŒ–\n",
    "X_num_test_scaled = scaler.transform(X_num_test)  # shape = (n_test, 4)\n",
    "\n",
    "# (2) ç”¨è®­ç»ƒå¥½çš„ encoder æå– latent\n",
    "X_num_test_latent = encoder.predict(X_num_test_scaled)  # shape = (n_test, encoding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd138a34-fc70-4bae-849f-61ac2c421a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„é€ ä¸è®­ç»ƒæ—¶åŒåçš„è¾“å…¥å­—å…¸\n",
    "test_inputs = {\n",
    "    \"numeric_latent\": X_num_test_latent,   # (n_test, encoding_dim)\n",
    "    \"block_id\": X_block_test,              # (n_test,)\n",
    "    \"categorical_onehot\": X_cat_test       # (n_test, len(onehot_cols))\n",
    "}\n",
    "\n",
    "# æ‰“å°æ£€æŸ¥ä¸€ä¸‹ä¸‰è·¯è¾“å…¥çš„ç±»å‹å’Œå½¢çŠ¶\n",
    "for key, val in test_inputs.items():\n",
    "    print(f\"{key}: type={type(val)}, shape={getattr(val, 'shape', 'æ— shape')}\")\n",
    "\n",
    "# å¦‚æœéƒ½æ˜¯ (n_test, â€¦) ä¸” n_test ç›¸åŒï¼Œå°±å¯ä»¥å®‰å…¨è°ƒç”¨ predict\n",
    "y_test_pred_log = model.predict(test_inputs)  # å½¢çŠ¶ï¼š(n_test, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81377225-97c4-47ef-a110-4c209a735f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# æ€»ä»· = å•ä»· * å»ºç­‘é¢ç§¯ = exp(log_å•ä»·) * å»ºç­‘é¢ç§¯\n",
    "df_test['é¢„æµ‹_æ€»ä»·'] = np.exp(y_test_pred_log.flatten()) * df_test['å»ºç­‘é¢ç§¯']\n",
    "\n",
    "# å¦‚æœä½ åªéœ€è¦ log_å•ä»· é¢„æµ‹ç»“æœï¼š\n",
    "#df_test['é¢„æµ‹_log_å•ä»·'] = y_test_pred_log\n",
    "\n",
    "# æœ€ç»ˆä¿å­˜åˆ° CSV æˆ– Excel\n",
    "output_df = df_test[['ID', 'é¢„æµ‹_æ€»ä»·']]  # å‡è®¾æµ‹è¯•é›†æœ‰ ID åˆ—\n",
    "output_df.to_csv(\"C:/Users/HP/Desktop/final/prediction_output.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
